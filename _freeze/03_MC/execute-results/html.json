{
  "hash": "9a3921e3696759fde5799ad978d6075e",
  "result": {
    "engine": "knitr",
    "markdown": "# Monte Carlo Method and its variations\n\n> Leading objectives:\n> \n> + understand how Monte Carlo (MC) and Markov chain Monte Carlo (MCMC) methods differ\n> + implement a Metropolis–Hastings algorithm to draw samples from the posterior\n> + use output of MCMC to obtain estimates and standard errors\n> + use efficient proposals and tuning for MCMC\n\n## Background and Motivation\n\nWhat we have seen in the last chapter up to now, is to use the `conjugate prior` to obtain closed form expressions for the posterior distribution. However, in many cases, conjugate priors are not available or not desirable. In such cases, we need to resort to numerical methods to approximate the posterior distribution. \n\n\n> **Question.**  \n> How can we perform Bayesian inference when conjugate priors are not available\n> and the posterior has no closed-form expression?\n\nThere are two broad classes of approaches:\n\n- **Simulation-based methods**:  \n  accept–reject sampling, Markov chain Monte Carlo (MCMC), particle filters, and related algorithms.\n\n- **Deterministic approximation methods**:  \n  Laplace approximations (including INLA), variational Bayes, expectation propagation, and related techniques.\n  \nWe will be focusing on the Monte Carlo (MC) methods\n\n## Monte Carlo Methods\n\n### Motivation: why Monte Carlo?\n\nIn Bayesian inference we repeatedly encounter integrals such as\n\n$$\n\\mathbb{E}[g(\\theta)\\mid y]\n=\\int g(\\theta)\\,p(\\theta\\mid y)\\,d\\theta,\n\\qquad\n\\Pr(\\theta\\in A\\mid y)=\\int_A p(\\theta\\mid y)\\,d\\theta,\n$$\n\nthat are not available in closed form. **Monte Carlo** replaces these integrals by averages of random draws.\n\n::: callout-note\nKey idea: replace an intractable integral by an empirical mean.\n:::\n\n\n## 4.1 The Monte Carlo Method\n\nIn the previous chapter, we obtained the following posterior distributions\nfor the birth rates of women without and with bachelor’s degrees:\n\n$$\n\\theta_1 \\mid \\sum_{i=1}^{111} Y_{i,1} = 217\n\\sim \\text{Gamma}(219, 112),\n$$\n\n$$\n\\theta_2 \\mid \\sum_{i=1}^{44} Y_{i,2} = 66\n\\sim \\text{Gamma}(68, 45).\n$$\n\nIt was claimed that\n\n$$\n\\Pr(\\theta_1 > \\theta_2 \\mid \\text{data}) = 0.97.\n$$\n\nHow do we compute such a probability?\n\nFrom Chapter 2, since $\\theta_1$ and $\\theta_2$ are conditionally independent given the data, we have\n\n$$\n\\Pr(\\theta_1 > \\theta_2 \\mid \\text{data})\n=\n\\int_0^\\infty \\int_0^{\\theta_1}\np(\\theta_1 \\mid \\text{data})\np(\\theta_2 \\mid \\text{data})\n\\, d\\theta_2 \\, d\\theta_1.\n$$\n\nSubstituting the gamma densities gives\n\n$$\n\\int_0^\\infty \\int_0^{\\theta_1}\n\\text{dgamma}(\\theta_1; 219, 112)\n\\,\n\\text{dgamma}(\\theta_2; 68, 45)\n\\, d\\theta_2 \\, d\\theta_1.\n$$\n\nThis integral can be evaluated numerically.\nHowever, in realistic Bayesian models, such integrals quickly become\nhigh-dimensional and analytically intractable.\n\nThis motivates **Monte Carlo methods**.\n\n### Monte Carlo Approximation\n\nSuppose we wish to compute\n\n$$\n\\mathbb{E}[g(\\theta) \\mid y]\n=\n\\int g(\\theta) \\, p(\\theta \\mid y) \\, d\\theta.\n$$\n\nIf we can generate independent samples\n\n$$\n\\theta^{(1)}, \\ldots, \\theta^{(S)}\n\\sim p(\\theta \\mid y),\n$$\n\nthen we approximate the expectation by\n\n$$\n\\frac{1}{S}\n\\sum_{s=1}^S g(\\theta^{(s)}).\n$$\n\nThis is called a **Monte Carlo approximation**.\n\nBy the Law of Large Numbers,\n\n$$\n\\frac{1}{S}\n\\sum_{s=1}^S g(\\theta^{(s)})\n\\;\\longrightarrow\\;\n\\mathbb{E}[g(\\theta) \\mid y] = \\int g(\\theta) p(\\theta \\mid y) d\\theta \n\\quad \\text{as } S \\to \\infty.\n$$\nWith the property above, we can calculate many quantities of interest about the posterior distribution. For example\n\n+ $\\bar{\\theta} \\to \\mathbb{E}[\\theta \\mid y]$,\n+ $\\displaystyle\n  \\frac{1}{S-1}\n  \\sum_{s=1}^S (\\theta^{(s)} - \\bar{\\theta})^2\n  \\to \\mathrm{Var}(\\theta \\mid y)\n  $.\n+ $\\displaystyle\n  \\frac{1}{S}\n  \\sum_{s=1}^S \\mathbf{1}\\{\\theta^{(s)} \\in A\\}\n  \\to \\Pr(\\theta \\in A \\mid y)\n  $. \n+ the empirical distribution of $\\{\\theta^{(1)}, \\dots, \\theta^{(S)}\\}$ converges to $p(\\theta \\mid y)$.\n+ the sample median converges to the posterior median $\\theta_{1/2}$.\n+ the sample $\\alpha$-quantile converges to $\\theta_\\alpha$.\n\nThus Monte Carlo sampling allows us to approximate:\n\n- posterior means,\n- posterior variances,\n- posterior probabilities,\n- credible intervals,\n- many more\n\n### Example: Estimating $\\Pr(\\theta_1 > \\theta_2)$\n\nTo approximate\n\n$$\n\\Pr(\\theta_1 > \\theta_2 \\mid \\text{data}),\n$$\n\nwe can:\n\n1. Draw $\\theta_1^{(s)} \\sim \\text{Gamma}(219, 112)$.\n2. Draw $\\theta_2^{(s)} \\sim \\text{Gamma}(68, 45)$.\n3. Compute the indicator\n   $$\n   I^{(s)} = \\mathbf{1}\\{\\theta_1^{(s)} > \\theta_2^{(s)}\\}.\n   $$\n\nThen\n\n$$\n\\Pr(\\theta_1 > \\theta_2 \\mid \\text{data})\n\\approx\n\\frac{1}{S}\n\\sum_{s=1}^S I^{(s)}.\n$$\n\nThis avoids evaluating any double integrals.\n\n---\n\n> Why Monte Carlo?\n>\n> Works in high dimensions.\n> Requires only the ability to simulate.\n> Avoids symbolic integration.\n> Scales to complex hierarchical models.\n\nThis is the foundation of modern Bayesian computation.\n\n\n\n### Figure 4.1 — Monte Carlo Approximation\n\nHistograms and kernel density estimates of Monte Carlo samples\napproximating the $\\text{Gamma}(68,45)$ posterior distribution.\nThe true density is shown in gray.\n\n---\n\n### Convergence Properties\n\nLet $\\theta^{(1)}, \\dots, \\theta^{(S)} \\sim p(\\theta \\mid y)$.\n\nAs $S \\to \\infty$:\n\n- $\\displaystyle \\frac{\\#\\{\\theta^{(s)} \\le c\\}}{S}\n  \\;\\longrightarrow\\;\n  \\Pr(\\theta \\le c \\mid y)$\n\n- The empirical distribution of\n  $\\{\\theta^{(1)},\\dots,\\theta^{(S)}\\}$\n  converges to $p(\\theta \\mid y)$\n\n- The sample median converges to the posterior median $\\theta_{1/2}$\n\n- The sample $\\alpha$-quantile converges to $\\theta_\\alpha$\n\n---\n\n**Key message:**  \nAlmost any aspect of a posterior distribution can be approximated\narbitrarily well using a sufficiently large Monte Carlo sample.\n\n---\n\n## Numerical Evaluation\n\nWe now compare Monte Carlo approximations to quantities that\ncan be computed analytically in this conjugate example.\n\nSuppose\n\n$$\nY_1,\\dots,Y_n \\mid \\theta \\sim \\text{Poisson}(\\theta),\n\\quad\n\\theta \\sim \\text{Gamma}(a,b).\n$$\n\nAfter observing $y_1,\\dots,y_n$ with\n$\\sum y_i = sy$ and sample size $n$,\nthe posterior distribution is\n\n$$\n\\theta \\mid y \\sim \\text{Gamma}(a+sy,\\; b+n).\n$$\n\n---\n\n### Example: College-Educated Group\n\nFor the birth-rate example:\n\n- $a = 2$\n- $b = 1$\n- $sy = 66$\n- $n = 44$\n\nPosterior:\n\n$$\n\\theta \\mid y \\sim \\text{Gamma}(68,45).\n$$\n\nPosterior mean:\n\n$$\n\\mathbb{E}[\\theta \\mid y]\n=\n\\frac{a+sy}{b+n}\n=\n\\frac{68}{45}\n=\n1.51.\n$$\n\n---\n\n### Monte Carlo in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(8670)\n\n## Posterior parameters\na  <- 2\nb  <- 1\nsy <- 66\nn  <- 44\n\nshape_post <- a + sy\nrate_post  <- b + n\n\n## Exact quantities\nmean_exact <- shape_post / rate_post\np_exact    <- pgamma(1.75, shape = shape_post, rate = rate_post)\nci_exact   <- qgamma(c(0.025, 0.975),\n                     shape = shape_post,\n                     rate  = rate_post)\n\n## Monte Carlo samples\ntheta_mc10   <- rgamma(10,   shape_post, rate_post)\ntheta_mc100  <- rgamma(100,  shape_post, rate_post)\ntheta_mc1000 <- rgamma(1000, shape_post, rate_post)\n\n## Function to summarize MC output\nmc_summary <- function(theta_sample) {\n  c(\n    Mean        = mean(theta_sample),\n    Prob_less   = mean(theta_sample < 1.75),\n    CI_lower    = quantile(theta_sample, 0.025),\n    CI_upper    = quantile(theta_sample, 0.975)\n  )\n}\n\n## Build comparison table\nresults <- rbind(\n  Exact   = c(Mean      = mean_exact,\n              Prob_less = p_exact,\n              CI_lower  = ci_exact[1],\n              CI_upper  = ci_exact[2]),\n\n  MC_10   = mc_summary(theta_mc10),\n  MC_100  = mc_summary(theta_mc100),\n  MC_1000 = mc_summary(theta_mc1000)\n)\n\nround(results, 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Mean Prob_less CI_lower CI_upper\nExact   1.5111    0.8998   1.1734   1.8908\nMC_10   1.5077    1.0000   1.3628   1.6228\nMC_100  1.5033    0.8500   1.1536   1.9305\nMC_1000 1.5180    0.8860   1.1810   1.8937\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSmax <- 1000\ntheta_seq <- rgamma(Smax, shape = shape_post, rate = rate_post)\ncum_mean  <- cumsum(theta_seq) / seq_along(theta_seq)\n\nplot(cum_mean, type=\"l\",\n     xlab=\"Number of draws (S)\",\n     ylab=\"Cumulative Monte Carlo mean\")\nabline(h = mean_exact, lty = 2, lwd = 2)\n```\n\n::: {.cell-output-display}\n![](03_MC_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxgrid <- seq(0.5, 2.5, length.out = 400)\ntrue_pdf <- dgamma(xgrid, shape = shape_post, rate = rate_post)\n\npar(mfrow=c(1,3))\n\nfor (S in c(10,100,1000)) {\n  x <- get(paste0(\"theta_mc\", S))\n  hist(x, prob=TRUE,\n       main=paste0(\"S = \", S),\n       xlab=expression(theta),\n       border=\"black\")\n  lines(xgrid, true_pdf, lwd=2)\n}\n```\n\n::: {.cell-output-display}\n![](03_MC_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow=c(1,1))\n```\n:::\n\n\n---\n\nThis Chapter follows closely with Chapter 3 in Hoff (2009).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}