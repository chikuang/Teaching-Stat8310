{
  "hash": "c95f681d6111257ecc4041239739a043",
  "result": {
    "engine": "knitr",
    "markdown": "# Week 2 — Conjugate Priors and Analytical Posteriors\n\n------------------------------------------------------------------------\n\n## Overview\n\nThis week focuses on **conjugate priors** — special priors that yield posteriors in the same family of distributions as the prior.\\\nStudents will learn why conjugacy simplifies Bayesian inference, how to identify conjugate pairs for common likelihoods, and how to perform analytical posterior updates without simulation.\\\nWe will also introduce the concept of prior sensitivity analysis and noninformative (objective) priors.\n\n------------------------------------------------------------------------\n\n## Learning Goals\n\nBy the end of Week 2, you should be able to:\n\n-   Define and identify conjugate priors for standard likelihood models.\\\n-   Derive analytical posteriors for Binomial, Poisson, and Normal models.\\\n-   Compute posterior summaries and predictive distributions.\\\n-   Discuss the influence of priors on posterior inference.\\\n-   Perform prior sensitivity analysis in R.\n\n------------------------------------------------------------------------\n\n## Lecture 1: The Concept of Conjugacy\n\n### 1.1 Definition\n\nA **conjugate prior** for a likelihood $p(y \\mid \\theta)$ is a prior distribution $p(\\theta)$ such that the posterior $p(\\theta \\mid y)$ belongs to the same family as the prior.\n\nFormally: $$\np(\\theta \\mid y) \\propto p(y \\mid \\theta)\\, p(\\theta)\n$$ If $p(\\theta \\mid y)$ has the same functional form as $p(\\theta)$, then $p(\\theta)$ is *conjugate* to the likelihood.\n\n### 1.2 Why Conjugacy Matters\n\n-   Provides closed-form expressions for posterior means, variances, and credible intervals.\\\n-   Facilitates sequential updating — easy to update priors as new data arrive.\\\n-   Useful for educational and analytic illustration before moving to MCMC methods.\n\n### 1.3 Examples of Conjugate Pairs\n\n| Likelihood | Conjugate Prior | Posterior Family |\n|----|----|----|\n| Binomial$(n,\\theta)$ | Beta$(\\alpha,\\beta)$ | Beta$(\\alpha+y, \\beta+n-y)$ |\n| Poisson$(\\lambda)$ | Gamma$(a,b)$ | Gamma$(a+\\sum y_i, b+n)$ |\n| Normal$(\\mu,\\sigma^2)$ (known variance) | Normal$(\\mu_0,\\tau_0^2)$ | Normal$(\\mu_1,\\tau_1^2)$ |\n| Exponential$(\\lambda)$ | Gamma$(a,b)$ | Gamma$(a+n, b+\\sum y_i)$ |\n| Normal mean/variance (unknown $\\sigma^2$) | Normal–Inverse-Gamma | Normal–Inverse-Gamma |\n\n------------------------------------------------------------------------\n\n## Lecture 2: Beta–Binomial and Gamma–Poisson Models\n\n### 2.1 Beta–Binomial Model (Review and Generalization)\n\nLet $y \\mid \\theta \\sim \\text{Binomial}(n,\\theta)$ and $\\theta \\sim \\text{Beta}(\\alpha_0,\\beta_0)$.\\\nThen the posterior is: $$\n\\theta \\mid y \\sim \\text{Beta}(\\alpha_0 + y, \\beta_0 + n - y).\n$$\n\n**Posterior Mean:** $$\nE[\\theta \\mid y] = \\frac{\\alpha_0 + y}{\\alpha_0 + \\beta_0 + n}.\n$$\n\n**Predictive Probability for a Future Success:** $$\np(\\tilde{y}=1 \\mid y) = E[\\theta \\mid y].\n$$\n\n**Interpretation:**\\\nEach observation updates the Beta prior by adding one success or failure to the corresponding shape parameter.\n\n------------------------------------------------------------------------\n\n### 2.2 Gamma–Poisson Model (Counts)\n\nSuppose we model count data as $y_i \\sim \\text{Poisson}(\\lambda)$, with prior $\\lambda \\sim \\text{Gamma}(a_0, b_0)$\\\n(where the Gamma density is parameterized as $p(\\lambda) \\propto \\lambda^{a_0-1} e^{-b_0\\lambda}$).\n\n**Posterior:** $$\n\\lambda \\mid y_1,\\ldots,y_n \\sim \\text{Gamma}\\left(a_0 + \\sum_{i=1}^n y_i,\\; b_0 + n\\right).\n$$\n\n**Posterior Mean and Variance:** $$\nE[\\lambda \\mid y] = \\frac{a_0 + \\sum y_i}{b_0 + n}, \\quad\n\\text{Var}[\\lambda \\mid y] = \\frac{a_0 + \\sum y_i}{(b_0 + n)^2}.\n$$\n\n**Posterior Predictive:** $$\np(\\tilde{y} \\mid y) = \\int \\text{Poisson}(\\tilde{y} \\mid \\lambda)\\, p(\\lambda \\mid y)\\, d\\lambda,\n$$ which follows a **Negative Binomial** distribution.\n\n**Interpretation:**\\\nThe Gamma prior acts as if we had observed $a_0-1$ pseudo-events over $b_0$ pseudo-trials.\n\n------------------------------------------------------------------------\n\n### 2.3 R Example: Gamma–Poisson Updating\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Posterior update for Gamma-Poisson model\ny <- c(3, 2, 4, 1, 0, 2, 3)\na0 <- 2; b0 <- 1   # prior Gamma(2,1)\nn <- length(y)\n\na1 <- a0 + sum(y)\nb1 <- b0 + n\n\nlambda <- seq(0, 10, length.out = 400)\nprior <- dgamma(lambda, a0, b0)\nposterior <- dgamma(lambda, a1, b1)\n\nplot(lambda, prior, type=\"l\", lwd=2, col=\"blue\", ylim=c(0, max(posterior)),\n     ylab=\"Density\", xlab=expression(lambda),\n     main=\"Gamma-Poisson Updating\")\nlines(lambda, posterior, col=\"red\", lwd=2)\nlegend(\"topright\",\n       legend=c(\"Prior Gamma(2,1)\", paste0(\"Posterior Gamma(\", a1, \",\", b1, \")\")),\n       col=c(\"blue\", \"red\"), lwd=2)\n```\n\n::: {.cell-output-display}\n![](week02_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}