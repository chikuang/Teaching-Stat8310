{
  "hash": "3778fd24a620bab0c821eb4998796be6",
  "result": {
    "engine": "knitr",
    "markdown": "# Week 10 — Bayesian Nonparametrics\n\nThis week introduces **Bayesian Nonparametric (BNP)** models, which allow infinite flexibility in representing uncertainty about functions, densities, or model structure.\\\nWe focus on two cornerstone approaches: **Dirichlet Processes** for mixture modeling and clustering, and **Gaussian Processes** for regression on functions.\n\n------------------------------------------------------------------------\n\n## Learning Goals\n\nBy the end of this week, you should be able to:\n\n-   Explain the motivation for nonparametric Bayesian models.\\\n-   Describe the Dirichlet Process and its stick-breaking and Chinese Restaurant representations.\\\n-   Explain how DP mixture models perform clustering automatically.\\\n-   Understand Gaussian Processes for function estimation.\\\n-   Implement simple DP and GP examples in R using available packages.\n\n------------------------------------------------------------------------\n\n## Lecture 1 — Dirichlet Process Models\n\n### 1.1 Motivation\n\nClassical parametric models fix the number of parameters (e.g., number of clusters).\\\n**Dirichlet Processes (DPs)** let the data decide model complexity by placing a prior on infinite mixture components.\n\n------------------------------------------------------------------------\n\n### 1.2 The Dirichlet Process\n\nA **Dirichlet Process** $\\text{DP}(\\alpha, G_0)$ is a distribution over distributions such that,\\\nfor any partition $A_1, \\ldots, A_k$ of the space, $$\n(G(A_1),\\ldots,G(A_k)) \\sim \\text{Dirichlet}(\\alpha G_0(A_1), \\ldots, \\alpha G_0(A_k)).\n$$\n\n-   $G_0$: base (prior mean) distribution.\\\n-   $\\alpha$: concentration parameter controlling clustering strength.\n\nAs $\\alpha \\to 0$: few clusters (more sharing).\\\nAs $\\alpha \\to \\infty$: many clusters (approaches $G_0$).\n\n------------------------------------------------------------------------\n\n### 1.3 Stick-Breaking Representation\n\nA constructive definition (Sethuraman, 1994): $$\nG = \\sum_{k=1}^{\\infty} \\pi_k \\delta_{\\theta_k}, \\quad\n\\theta_k \\sim G_0, \\quad\n\\pi_k = v_k \\prod_{l<k}(1-v_l), \\quad\nv_k \\sim \\text{Beta}(1,\\alpha).\n$$\n\nThe weights $\\pi_k$ form an infinite sequence summing to 1 — conceptually “breaking a stick” into random lengths.\n\n------------------------------------------------------------------------\n\n### 1.4 DP Mixture Model\n\nFor data $y_1,\\ldots,y_n$: $$\ny_i \\mid \\theta_i \\sim F(\\theta_i), \\qquad\n\\theta_i \\mid G \\sim G, \\qquad\nG \\sim \\text{DP}(\\alpha, G_0).\n$$\n\nMarginally, this induces **clustering** because multiple $y_i$ can share the same $\\theta_i$.\n\n------------------------------------------------------------------------\n\n### 1.5 Chinese Restaurant Process (CRP)\n\nEquivalent generative process:\n\n1.  Customer 1 starts a new table.\\\n2.  Customer $i$ joins an existing table $k$ with probability\\\n    $n_k / (\\alpha + i - 1)$,\\\n    or starts a new one with probability $\\alpha / (\\alpha + i - 1)$.\n\nThis describes how clusters grow adaptively as data arrive.\n\n------------------------------------------------------------------------\n\n### 1.6 Example — Simulated DP Mixture of Normals\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(10)\nlibrary(dirichletprocess)\n\n# Generate mixture data\ny <- c(rnorm(50, -3, 0.5), rnorm(50, 3, 0.5))\n\n# Fit a Dirichlet Process Gaussian Mixture\ndp <- DirichletProcessGaussian(y)\ndp <- Fit(dp, its = 2000)\n\n# Cluster assignments\nplot(dp) + ggplot2::ggtitle(\"Dirichlet Process Gaussian Mixture\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: `aes_()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`\nℹ The deprecated feature was likely used in the dirichletprocess package.\n  Please report the issue at\n  <https://github.com/dm13450/dirichletprocess/issues>.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](week10_files/figure-html/dp-mixture-sim-1.png){width=672}\n:::\n:::\n\n\nInterpretation: the DP mixture automatically discovers clusters without specifying their number in advance.\n\n------------------------------------------------------------------------\n\n### 1.7 Practical Notes\n\n-   The posterior number of clusters depends on $\\alpha$ and data separation.\\\n-   Inference via Gibbs sampling or variational truncation.\\\n-   Extensions: Hierarchical DP, DP regression, and DP topic models.\n\n------------------------------------------------------------------------\n\n## Lecture 2 — Gaussian Processes for Regression\n\n### 2.1 Motivation\n\nA **Gaussian Process (GP)** defines a prior directly over functions, enabling flexible nonlinear regression without specifying a parametric form.\n\n------------------------------------------------------------------------\n\n### 2.2 Definition\n\nA GP is a collection of random variables $f(x)$ such that every finite subset has a joint multivariate normal distribution: $$\nf(x) \\sim \\text{GP}(m(x), k(x,x')),\n$$ where\n  \n  + $m(x) = E[f(x)]$: mean function,\n  + $k(x,x') = \\text{Cov}(f(x),f(x'))$: covariance (kernel) function.\n\n---\n\n### GP Regression Model\n\nFor data $(x_i, y_i)$: $$\ny_i = f(x_i) + \\varepsilon_i, \\quad \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2).\n$$ Posterior of $f(x)$ given $y$: $$\nf_* \\mid y, X, X_* \\sim \\mathcal{N}(\\bar{f}_*, \\text{Cov}(f_*)),\n$$ where the mean and covariance are computed using kernel matrices.\n\n------------------------------------------------------------------------\n\n### 2.4 Common Kernels\n\n| Kernel | Formula | Property |\n|------------------------|------------------------|------------------------|\n| Squared Exponential | $k(x,x') = \\tau^2 \\exp\\!\\left(-\\frac{(x-x')^2}{2\\ell^2}\\right)$ | Smooth, infinitely differentiable |\n| Matérn | $k(x,x') = \\tau\\^2 \\frac{2\\^{1-\\nu}}{\\Gamma(\\nu)}(\\sqrt{2\\nu})$ | x-x' |\n| Periodic | $k(x,x') = \\tau^2\\exp(-2\\sin^2(\\pi))$ | x-x' |\n\n------------------------------------------------------------------------\n\n### 2.5 Example — Gaussian Process Regression in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(GPfit)\nset.seed(11)\n\nx <- seq(-3, 3, length.out = 50)\ny <- sin(x) + rnorm(50, sd = 0.2)\n\n# Make X a column matrix (good practice for GPfit)\ngp_model <- GP_fit(X = matrix(x, ncol = 1), Y = y)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in GP_fit(X = matrix(x, ncol = 1), Y = y): X should be in range (0, 1)\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(gp_model)\n```\n\n::: {.cell-output-display}\n![](week10_files/figure-html/gp-example-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_grid <- seq(-3, 3, length.out = 200)\npred <- predict(gp_model, xnew = matrix(pred_grid, ncol = 1))\n\nyhat <- as.numeric(pred$Y_hat)\nse   <- sqrt(as.numeric(pred$MSE))  # MSE is a variance vector; no diag()\n\nplot(x, y, pch = 19, col = \"#00000055\",\n     main = \"Gaussian Process Regression\", xlab = \"x\", ylab = \"y\")\nlines(pred_grid, yhat, lwd = 2, col = \"darkorange\")\nlines(pred_grid, yhat + 2*se, lty = 2, col = \"gray40\")\nlines(pred_grid, yhat - 2*se, lty = 2, col = \"gray40\")\n```\n\n::: {.cell-output-display}\n![Gaussian Process Regression Fit](week10_files/figure-html/gp-predict-1.png){width=672}\n:::\n:::\n\n\nInterpretation: the GP posterior mean tracks the true function smoothly, with uncertainty quantified by the shaded region.\n\n------------------------------------------------------------------------\n\n### 2.6 GP vs DP: Comparison\n\n| Aspect | Dirichlet Process | Gaussian Process |\n|------------------------|------------------------|------------------------|\n| **Domain** | Distributions / Clusters | Functions |\n| **Output** | Discrete clustering | Continuous regression |\n| **Flexibility** | Unknown number of components | Infinite function space |\n| **Typical Use** | Density estimation, mixture modeling | Nonlinear regression, spatial data |\n\n------------------------------------------------------------------------\n\n### 2.7 Practical Considerations\n\n-   GP computational cost is $O(n^3)$; use sparse or inducing-point approximations for large $n$.\\\n-   Choice of kernel determines function smoothness and inductive bias.\\\n-   In practice, hyperparameters (e.g., $\\ell,~\\tau$) are learned via marginal likelihood maximization.\n\n------------------------------------------------------------------------\n\n## Homework 10\n\n1.  **Conceptual**\n    -   Compare the roles of $\\alpha$ in DP and the kernel parameters in GP.\\\n    -   Explain the difference between parametric and nonparametric Bayesian models.\n2.  **Computational**\n    -   Simulate data from two Gaussian clusters and fit a DP mixture model using `dirichletprocess`.\\\n    -   Fit a GP regression to noisy sinusoidal data using `GPfit`.\\\n    -   Plot both model fits and discuss flexibility.\n3.  **Reflection**\n    -   When might nonparametric models be overkill?\\\n    -   How could hierarchical extensions of DP or GP handle grouped data?\n\n------------------------------------------------------------------------\n\n## Key Takeaways\n\n| Concept | Summary |\n|------------------------------------|------------------------------------|\n| **Dirichlet Process** | Prior over distributions enabling infinite mixture models. |\n| **Stick-Breaking Construction** | Represents DP as weighted infinite discrete components. |\n| **Chinese Restaurant Process** | Intuitive clustering interpretation of DP. |\n| **Gaussian Process** | Defines prior over functions for regression and smoothing. |\n| **Common Feature** | Both model complexity grows with data — no fixed parameter dimension. |\n\n------------------------------------------------------------------------\n\n**Next Week:** Bayesian Time Series and State-Space Models — dynamic modeling and sequential inference using Bayesian methods.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}