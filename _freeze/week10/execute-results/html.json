{
  "hash": "344fe69441bf17f342e19d616a2236b0",
  "result": {
    "engine": "knitr",
    "markdown": "# Week 10 — Bayesian Nonparametrics\n\nThis week introduces **Bayesian Nonparametric (BNP)** models, which allow infinite flexibility in representing uncertainty about functions, densities, or model structure.  \nWe focus on two cornerstone approaches: **Dirichlet Processes** for mixture modeling and clustering, and **Gaussian Processes** for regression on functions.\n\n---\n\n## Learning Goals\n\nBy the end of this week, you should be able to:\n\n- Explain the motivation for nonparametric Bayesian models.  \n- Describe the Dirichlet Process and its stick-breaking and Chinese Restaurant representations.  \n- Explain how DP mixture models perform clustering automatically.  \n- Understand Gaussian Processes for function estimation.  \n- Implement simple DP and GP examples in R using available packages.\n\n---\n\n## Lecture 1 — Dirichlet Process Models\n\n### 1.1 Motivation\n\nClassical parametric models fix the number of parameters (e.g., number of clusters).  \n**Dirichlet Processes (DPs)** let the data decide model complexity by placing a prior on infinite mixture components.\n\n---\n\n### 1.2 The Dirichlet Process\n\nA **Dirichlet Process** $begin:math:text$ \\\\text{DP}(\\\\alpha, G_0) $end:math:text$ is a distribution over distributions such that,  \nfor any partition $begin:math:text$ A_1, \\\\ldots, A_k $end:math:text$ of the space,\n$$\n(G(A_1),\\ldots,G(A_k)) \\sim \\text{Dirichlet}(\\alpha G_0(A_1), \\ldots, \\alpha G_0(A_k)).\n$$\n\n- $begin:math:text$ G_0 $end:math:text$: base (prior mean) distribution.  \n- $begin:math:text$ \\\\alpha $end:math:text$: concentration parameter controlling clustering strength.\n\nAs $begin:math:text$ \\\\alpha \\\\to 0 $end:math:text$: few clusters (more sharing).  \nAs $begin:math:text$ \\\\alpha \\\\to \\\\infty $end:math:text$: many clusters (approaches $begin:math:text$ G_0 $end:math:text$).\n\n---\n\n### 1.3 Stick-Breaking Representation\n\nA constructive definition (Sethuraman, 1994):\n$$\nG = \\sum_{k=1}^{\\infty} \\pi_k \\delta_{\\theta_k}, \\quad\n\\theta_k \\sim G_0, \\quad\n\\pi_k = v_k \\prod_{l<k}(1-v_l), \\quad\nv_k \\sim \\text{Beta}(1,\\alpha).\n$$\n\nThe weights $begin:math:text$ \\\\pi_k $end:math:text$ form an infinite sequence summing to 1 — conceptually “breaking a stick” into random lengths.\n\n---\n\n### 1.4 DP Mixture Model\n\nFor data $begin:math:text$ y_1,\\\\ldots,y_n $end:math:text$:\n$$\ny_i \\mid \\theta_i \\sim F(\\theta_i), \\qquad\n\\theta_i \\mid G \\sim G, \\qquad\nG \\sim \\text{DP}(\\alpha, G_0).\n$$\n\nMarginally, this induces **clustering** because multiple $begin:math:text$ y_i $end:math:text$ can share the same $begin:math:text$ \\\\theta_i $end:math:text$.\n\n---\n\n### 1.5 Chinese Restaurant Process (CRP)\n\nEquivalent generative process:\n\n1. Customer 1 starts a new table.  \n2. Customer $begin:math:text$ i $end:math:text$ joins an existing table $begin:math:text$ k $end:math:text$ with probability  \n   $begin:math:text$ n_k / (\\\\alpha + i - 1) $end:math:text$,  \n   or starts a new one with probability $begin:math:text$ \\\\alpha / (\\\\alpha + i - 1) $end:math:text$.\n\nThis describes how clusters grow adaptively as data arrive.\n\n---\n\n### 1.6 Example — Simulated DP Mixture of Normals\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(10)\nlibrary(dirichletprocess)\n\n# Generate mixture data\ny <- c(rnorm(50, -3, 0.5), rnorm(50, 3, 0.5))\n\n# Fit a Dirichlet Process Gaussian Mixture\ndp <- DirichletProcessGaussian(y)\ndp <- Fit(dp, its = 2000)\n\n# Cluster assignments\nplot(dp) + ggplot2::ggtitle(\"Dirichlet Process Gaussian Mixture\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: `aes_()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`\nℹ The deprecated feature was likely used in the dirichletprocess package.\n  Please report the issue at\n  <https://github.com/dm13450/dirichletprocess/issues>.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](week10_files/figure-html/dp-mixture-sim-1.png){width=672}\n:::\n:::\n\n\nInterpretation: the DP mixture automatically discovers clusters without specifying their number in advance.\n\n---\n\n### 1.7 Practical Notes\n\n- The posterior number of clusters depends on $begin:math:text$ \\\\alpha $end:math:text$ and data separation.  \n- Inference via Gibbs sampling or variational truncation.  \n- Extensions: Hierarchical DP, DP regression, and DP topic models.\n\n---\n\n## Lecture 2 — Gaussian Processes for Regression\n\n### 2.1 Motivation\n\nA **Gaussian Process (GP)** defines a prior directly over functions, enabling flexible nonlinear regression without specifying a parametric form.\n\n---\n\n### 2.2 Definition\n\nA GP is a collection of random variables $begin:math:text$ f(x) $end:math:text$ such that every finite subset has a joint multivariate normal distribution:\n$$\nf(x) \\sim \\text{GP}(m(x), k(x,x')),\n$$\nwhere  \n- $begin:math:text$ m(x) = E[f(x)] $end:math:text$: mean function,  \n- $begin:math:text$ k(x,x') = \\\\text{Cov}(f(x),f(x')) $end:math:text$: covariance (kernel) function.\n\n---\n\n### 2.3 GP Regression Model\n\nFor data $begin:math:text$ (x_i, y_i) $end:math:text$:\n$$\ny_i = f(x_i) + \\varepsilon_i, \\quad \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2).\n$$\nPosterior of $begin:math:text$ f(x) $end:math:text$ given $begin:math:text$ y $end:math:text$:\n$$\nf_* \\mid y, X, X_* \\sim \\mathcal{N}(\\bar{f}_*, \\text{Cov}(f_*)),\n$$\nwhere the mean and covariance are computed using kernel matrices.\n\n---\n\n### 2.4 Common Kernels\n\n| Kernel | Formula | Property |\n|---------|----------|-----------|\n| Squared Exponential | $begin:math:text$ k(x,x') = \\\\tau^2 \\\\exp\\\\!\\\\left(-\\\\frac{(x-x')^2}{2\\\\ell^2}\\\\right) $end:math:text$ | Smooth, infinitely differentiable |\n| Matérn | $begin:math:text$ k(x,x') = \\\\tau^2 \\\\frac{2^{1-\\\\nu}}{\\\\Gamma(\\\\nu)}(\\\\sqrt{2\\\\nu}|x-x'|/\\\\ell)^\\\\nu K_\\\\nu(\\\\sqrt{2\\\\nu}|x-x'|/\\\\ell) $end:math:text$ | Controls smoothness |\n| Periodic | $begin:math:text$ k(x,x') = \\\\tau^2\\\\exp(-2\\\\sin^2(\\\\pi|x-x'|/p)/\\\\ell^2) $end:math:text$ | Captures periodicity |\n\n---\n\n### 2.5 Example — Gaussian Process Regression in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(GPfit)\nset.seed(11)\n\nx <- seq(-3, 3, length.out = 50)\ny <- sin(x) + rnorm(50, sd = 0.2)\n\n# Make X a column matrix (good practice for GPfit)\ngp_model <- GP_fit(X = matrix(x, ncol = 1), Y = y)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in GP_fit(X = matrix(x, ncol = 1), Y = y): X should be in range (0, 1)\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(gp_model)\n```\n\n::: {.cell-output-display}\n![](week10_files/figure-html/gp-example-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_grid <- seq(-3, 3, length.out = 200)\npred <- predict(gp_model, xnew = matrix(pred_grid, ncol = 1))\n\nyhat <- as.numeric(pred$Y_hat)\nse   <- sqrt(as.numeric(pred$MSE))  # MSE is a variance vector; no diag()\n\nplot(x, y, pch = 19, col = \"#00000055\",\n     main = \"Gaussian Process Regression\", xlab = \"x\", ylab = \"y\")\nlines(pred_grid, yhat, lwd = 2, col = \"darkorange\")\nlines(pred_grid, yhat + 2*se, lty = 2, col = \"gray40\")\nlines(pred_grid, yhat - 2*se, lty = 2, col = \"gray40\")\n```\n\n::: {.cell-output-display}\n![Gaussian Process Regression Fit](week10_files/figure-html/gp-predict-1.png){width=672}\n:::\n:::\n\n\nInterpretation: the GP posterior mean tracks the true function smoothly, with uncertainty quantified by the shaded region.\n\n---\n\n### 2.6 GP vs DP: Comparison\n\n| Aspect | Dirichlet Process | Gaussian Process |\n|--------|--------------------|------------------|\n| **Domain** | Distributions / Clusters | Functions |\n| **Output** | Discrete clustering | Continuous regression |\n| **Flexibility** | Unknown number of components | Infinite function space |\n| **Typical Use** | Density estimation, mixture modeling | Nonlinear regression, spatial data |\n\n---\n\n### 2.7 Practical Considerations\n\n- GP computational cost is $begin:math:text$ O(n^3) $end:math:text$; use sparse or inducing-point approximations for large $begin:math:text$n$end:math:text$.  \n- Choice of kernel determines function smoothness and inductive bias.  \n- In practice, hyperparameters (e.g., $begin:math:text$ \\\\ell, \\\\tau $end:math:text$) are learned via marginal likelihood maximization.\n\n---\n\n## Homework 10\n\n1. **Conceptual**  \n   - Compare the roles of $begin:math:text$ \\\\alpha $end:math:text$ in DP and the kernel parameters in GP.  \n   - Explain the difference between parametric and nonparametric Bayesian models.  \n\n2. **Computational**  \n   - Simulate data from two Gaussian clusters and fit a DP mixture model using `dirichletprocess`.  \n   - Fit a GP regression to noisy sinusoidal data using `GPfit`.  \n   - Plot both model fits and discuss flexibility.\n\n3. **Reflection**  \n   - When might nonparametric models be overkill?  \n   - How could hierarchical extensions of DP or GP handle grouped data?\n\n---\n\n## Key Takeaways\n\n| Concept | Summary |\n|----------|----------|\n| **Dirichlet Process** | Prior over distributions enabling infinite mixture models. |\n| **Stick-Breaking Construction** | Represents DP as weighted infinite discrete components. |\n| **Chinese Restaurant Process** | Intuitive clustering interpretation of DP. |\n| **Gaussian Process** | Defines prior over functions for regression and smoothing. |\n| **Common Feature** | Both model complexity grows with data — no fixed parameter dimension. |\n\n---\n\n**Next Week:** Bayesian Time Series and State-Space Models — dynamic modeling and sequential inference using Bayesian methods.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}