{
  "hash": "c2fae2e849cf403f71e88bcd7fc4633b",
  "result": {
    "engine": "knitr",
    "markdown": "# Introduction\n\nThe posterior distribution is obtained from the prior distribution and sampling model via *Bayes’ rule*:\n\n$$p(\\theta \\mid y)=\\frac{p(y \\mid \\theta) p(\\theta)}{\\int_{\\Theta} p(y \\mid \\tilde{\\theta}) p(\\tilde{\\theta}) d \\tilde{\\theta}}.$$\n\n\nThis is a book created from markdown and executable code.\n\nSee @knuth84 for additional discussion of literate programming.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1 + 1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2\n```\n\n\n:::\n:::\n\n\n## Why Bayesian?\n\n\n\n---\n\n\n# Course Topics and Schedule\n\n| **Week** | **Topics** | **Key Concepts / Readings** | **Computing Focus** |\n|:---------:|-------------|-----------------------------|---------------------|\n| **1** | Introduction to Bayesian Thinking | Bayesian vs. Frequentist paradigms; Prior, likelihood, posterior | Review of R basics and reproducible workflows |\n| **2** | Bayesian Inference for Simple Models | Conjugate priors, Beta-Binomial, Normal-Normal, Poisson-Gamma | Simulating posteriors, visualization |\n| **3** | Prior Elicitation and Sensitivity | Informative vs. noninformative priors, Jeffreys prior | Prior sensitivity plots |\n| **4** | Monte Carlo Integration | Law of large numbers, sampling-based inference | Random sampling and Monte Carlo approximation |\n| **5** | Markov Chain Monte Carlo (MCMC) | Metropolis-Hastings, Gibbs sampler | Implementing MCMC in R |\n| **6** | Convergence Diagnostics | Trace plots, autocorrelation, Gelman–Rubin statistic | `coda`, `rstan`, and `bayesplot` packages |\n| **7** | Hierarchical Bayesian Models | Partial pooling, shrinkage, multilevel structures | `rstanarm` / `brms` |\n| **8** | Midterm Project: Bayesian Linear Regression | Posterior inference for regression, model selection | `brms`, `rstanarm`, custom Gibbs samplers |\n| **9** | Bayesian Model Comparison | Bayes factors, BIC, DIC, WAIC, LOO | Practical comparison via cross-validation |\n| **10** | Model Checking and Diagnostics | Posterior predictive checks, residual analysis | `pp_check` in `brms` |\n| **11** | Advanced Computation | Hamiltonian Monte Carlo (HMC), Variational Inference | Using `Stan` and `CmdStanR` |\n| **12** | Bayesian Decision Theory | Utility functions, decision rules, loss minimization | Simple decision problems in R |\n| **13** | Modern Bayesian Methods | Approximate Bayesian computation (ABC), Bayesian neural networks | Examples via `rstan` or `tensorflow-probability` |\n| **14** | Student Project Presentations | Applications and case studies | Full workflow demonstration in R |\n\n---\n\n\n---\n\nInteresting Article: \n\n* Goligher, E.C., Harhay, M.O. (2023). [What Is the Point of Bayesian Analysis?](https://pmc.ncbi.nlm.nih.gov/articles/PMC10919113/), \nAmerican Journal of Respiratory and Critical Care Medicine, 209, 485--487.\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}