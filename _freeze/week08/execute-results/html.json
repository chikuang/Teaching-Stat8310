{
  "hash": "4116b03458dcd67961e95523aa8de547",
  "result": {
    "engine": "knitr",
    "markdown": "# Week 8 — Advanced Bayesian Computation\n\nThis week explores two major developments that enable scalable Bayesian inference for complex or high-dimensional models:\\\n**Hamiltonian Monte Carlo (HMC)** and **Variational Inference (VI)**.\\\nWe study their principles, intuition, and practical use in software such as `Stan` and `brms`.\n\n------------------------------------------------------------------------\n\n## Learning Goals\n\nBy the end of this week, you should be able to:\n\n-   Explain the motivation for advanced sampling and approximation methods.\\\n-   Describe the mechanics and intuition of Hamiltonian Monte Carlo.\\\n-   Understand the trade-offs between exact (MCMC) and approximate (VI) inference.\\\n-   Run basic HMC and VI fits using modern R interfaces.\\\n-   Interpret diagnostics for both approaches.\n\n------------------------------------------------------------------------\n\n## Lecture 1 — Hamiltonian Monte Carlo (HMC)\n\n### 1.1 Motivation\n\nTraditional MCMC (e.g., Metropolis–Hastings, Gibbs) can mix slowly in high dimensions.\\\n**Hamiltonian Monte Carlo** accelerates exploration by using gradient information from the log posterior to simulate physical motion through parameter space.\n\n------------------------------------------------------------------------\n\n### 1.2 Hamiltonian Dynamics\n\nWe introduce an auxiliary “momentum” variable $p$ and define the **Hamiltonian**: $$\nH(\\theta,p) = U(\\theta) + K(p),\n$$ where\\\n- $U(\\theta) = -\\log p(\\theta\\mid y)$ (potential energy = negative log posterior),\\\n- $K(p) = \\tfrac{1}{2} p^\\top M^{-1}p$ (kinetic energy, with mass matrix $M$).\n\nThe system evolves via Hamilton’s equations: $$\n\\frac{d\\theta}{dt} = \\frac{\\partial H}{\\partial p}, \\qquad\n\\frac{dp}{dt} = -\\frac{\\partial H}{\\partial \\theta}.\n$$\n\n------------------------------------------------------------------------\n\n### 1.3 Leapfrog Integration\n\nTo approximate continuous motion, HMC uses a **leapfrog integrator** with step size $\\epsilon$ and $L$ steps:\n\n1.  $p\\_{t+\\epsilon/2} = p_t - \\frac{\\epsilon}{2}\\nabla\\_\\theta U(\\theta_t)$\\\n2.  $\\theta\\_{t+\\epsilon} = \\theta_t + \\epsilon M^{-1}p\\_{t+\\epsilon/2}$\\\n3.  $p\\_{t+\\epsilon} = p\\_{t+\\epsilon/2} - \\frac{\\epsilon}{2}\\nabla\\_\\theta U(\\theta\\_{t+\\epsilon})$\n\nAfter simulating this path, we apply a **Metropolis acceptance step** using the change in $H$.\n\n------------------------------------------------------------------------\n\n### 1.4 Intuition\n\n-   The gradient $\\nabla\\_\\theta U(\\theta)$ guides proposals along high-density regions, avoiding random walk behavior.\\\n-   Proper tuning of step size $\\epsilon$ and number of steps $L$ yields efficient exploration.\\\n-   Modern implementations (e.g., *Stan*) adapt these automatically via the **No-U-Turn Sampler (NUTS)**.\n\n------------------------------------------------------------------------\n\n### 1.5 Example — Logistic Regression with HMC (Stan)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(brms)\nset.seed(8)\n\n# Simulated logistic data\nN <- 200\nx <- rnorm(N)\ny <- rbinom(N, 1, plogis(-1 + 2*x))\ndat <- data.frame(x, y)\n\n# Fit via Hamiltonian Monte Carlo (NUTS)\nfit_hmc <- brm(y ~ x, data=dat, family=bernoulli(), chains=2, iter=2000, refresh=0)\nsummary(fit_hmc)\nplot(fit_hmc)\n```\n:::\n\n\n`Stan`’s NUTS algorithm performs automatic adaptation of step size and trajectory length.\n\n------------------------------------------------------------------------\n\n### 1.6 Diagnosing HMC Performance\n\nKey diagnostics: - **Divergent transitions** → numerical instability (reduce step size or re-scale parameters).\\\n- **Energy Bayesian Fraction of Missing Information (E-BFMI)** → low values (\\<0.3) indicate poor exploration.\\\n- $\\widehat{R}$ and effective sample size → check convergence and mixing.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(bayesplot)\nmcmc_nuts_divergence(fit_hmc)\nmcmc_trace(fit_hmc, pars=c(\"b_Intercept\",\"b_x\"))\n```\n:::\n\n\n------------------------------------------------------------------------\n\n### 1.7 Advantages of HMC\n\n| Feature | Benefit |\n|------------------------------------|------------------------------------|\n| Gradient-based proposals | Rapid movement through high-density regions |\n| Higher acceptance rates | Fewer rejections than random-walk MH |\n| Fewer tuning parameters | Automatic adaptation (NUTS) |\n| Robust for high-dimensional models | Used in most modern Bayesian software |\n\n------------------------------------------------------------------------\n\n## Lecture 2 — Variational Inference (VI)\n\n### 2.1 Motivation\n\nWhen exact sampling is too costly (e.g., massive datasets, deep models),\\\n**Variational Inference** (VI) approximates the posterior by a simpler distribution $q\\_\\lambda(\\theta)$ within a parameterized family.\n\n------------------------------------------------------------------------\n\n### 2.2 Objective Function\n\nWe minimize the **Kullback–Leibler (KL) divergence**: $$\n\\text{KL}(q_\\lambda(\\theta) \\,\\|\\, p(\\theta\\mid y))\n  = \\int q_\\lambda(\\theta)\\log\\frac{q_\\lambda(\\theta)}{p(\\theta\\mid y)}\\,d\\theta.\n$$\n\nEquivalently, we **maximize the Evidence Lower Bound (ELBO):** $$\n\\text{ELBO}(\\lambda)\n= E_{q_\\lambda}[\\log p(y,\\theta)] - E_{q_\\lambda}[\\log q_\\lambda(\\theta)].\n$$ The higher the ELBO, the closer $q\\_\\lambda(\\theta)$ is to the true posterior.\n\n------------------------------------------------------------------------\n\n### 2.3 Mean-Field Approximation\n\nA common simplification assumes factorization: $$\nq_\\lambda(\\theta) = \\prod_j q_{\\lambda_j}(\\theta_j),\n$$ which allows coordinate-wise optimization of each factor.\n\n------------------------------------------------------------------------\n\n### 2.4 Example — Variational Bayes for a Normal Mean\n\nAssume $y_i\\sim N(\\theta,1)$ with prior $\\theta\\sim N(0,1)$.\n\nAnalytically, the posterior is $N\\left(\\frac{n\\bar{y}}{n+1}, \\frac{1}{n+1}\\right)$.\\\nWe approximate it variationally by another normal $q(\\theta)=N(m,s^2)$,\\\nand find $m,s^2$ maximizing ELBO.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(9)\ny <- rnorm(50, mean=1)\nn <- length(y)\nlog_joint <- function(theta) sum(dnorm(y, theta, 1, log=TRUE)) + dnorm(theta, 0, 1, log=TRUE)\n\n# Closed-form optimal q is Normal(m,s^2) with same moments as true posterior:\nm_vi <- n*mean(y)/(n+1)\ns2_vi <- 1/(n+1)\nc(mean=m_vi, sd=sqrt(s2_vi))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     mean        sd \n0.8884051 0.1400280 \n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### 2.5 Automatic VI with `brms`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(brms)\nset.seed(10)\nN <- 1000\nx <- rnorm(N)\ny <- 2 + 1.5*x + rnorm(N)\ndat <- data.frame(x,y)\n\nfit_vi <- brm(y ~ x, data=dat, family=gaussian(),\n              algorithm=\"meanfield\", iter=5000, refresh=0)\nsummary(fit_vi)\n```\n:::\n\n\nVI provides a fast deterministic approximation, trading off accuracy for scalability.\n\n------------------------------------------------------------------------\n\n### 2.6 Comparison: HMC vs VI\n\n| Feature | HMC (NUTS) | Variational Inference |\n|------------------------|------------------------|------------------------|\n| **Type** | Sampling (asymptotically exact) | Optimization (approximate) |\n| **Accuracy** | Very high | Depends on variational family |\n| **Speed** | Slower | Very fast |\n| **Diagnostics** | Convergence via $\\widehat{R}$, ESS | ELBO convergence |\n| **Use case** | Complex or small data | Massive or real-time problems |\n\n------------------------------------------------------------------------\n\n### 2.7 Visual Comparison (Conceptual)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntheta <- seq(-3,3,length=400)\nposterior <- dnorm(theta, 0, 1)              # true posterior\nvi_approx <- dnorm(theta, 0, 1.5)            # wider variational approx\nplot(theta, posterior, type=\"l\", lwd=2, col=\"black\", ylim=c(0,0.5),\n     main=\"Posterior (HMC) vs Variational Approximation\",\n     ylab=\"Density\", xlab=expression(theta))\nlines(theta, vi_approx, col=\"orange\", lwd=2, lty=2)\nlegend(\"topright\", legend=c(\"Exact posterior (HMC)\",\"VI approximation\"),\n       col=c(\"black\",\"orange\"), lwd=2, lty=c(1,2), bty=\"n\")\n```\n\n::: {.cell-output-display}\n![](week08_files/figure-html/vi-vs-hmc-plot-1.png){width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### 2.8 Practical Advice\n\n-   Use **HMC (NUTS)** as the default for accuracy and diagnostics.\\\n-   Use **VI** for large-scale models, initialization, or quick exploration.\\\n-   Compare results: if VI and HMC differ substantially, favor HMC.\n\n------------------------------------------------------------------------\n\n## Homework 8\n\n1.  **Conceptual**\n    -   Explain the difference between sampling-based and optimization-based inference.\\\n    -   What role does the ELBO play in VI?\n2.  **Computational**\n    -   Fit a simple linear regression using both HMC and VI in `brms`.\\\n    -   Compare posterior means, standard deviations, and computation time.\n3.  **Reflection**\n    -   In what types of real-world problems might VI be preferred over HMC?\\\n    -   How would you check whether your VI approximation is adequate?\n\n------------------------------------------------------------------------\n\n## Key Takeaways\n\n| Concept | Summary |\n|------------------------------------|------------------------------------|\n| Hamiltonian Monte Carlo | Uses gradients to propose efficient moves through parameter space. |\n| No-U-Turn Sampler (NUTS) | Adapts step size and trajectory automatically. |\n| Variational Inference | Optimizes a tractable approximation to the posterior. |\n| ELBO | Objective function for VI; measures closeness to the true posterior. |\n| Trade-off | HMC = accuracy, VI = speed; choice depends on model and data size. |\n\n------------------------------------------------------------------------\n\n**Next Week:** Bayesian Model Averaging and Ensemble Learning — combining multiple Bayesian models for improved predictive performance.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}