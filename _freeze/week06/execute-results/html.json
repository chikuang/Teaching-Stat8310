{
  "hash": "1a2f9826d4a477f43c88e801d025d3d7",
  "result": {
    "engine": "knitr",
    "markdown": "# Week 6 — Hierarchical Bayesian Models\n\nThis week introduces **hierarchical (multilevel) Bayesian models**, which allow parameters to vary across groups while sharing information through higher-level priors.  \nWe study partial pooling, shrinkage, and their implementation for normal and regression models.\n\n---\n\n## Learning Goals\n\nBy the end of this week, you should be able to:\n\n- Explain the motivation for hierarchical modeling.  \n- Formulate hierarchical models with group-level parameters.  \n- Interpret partial pooling and shrinkage.  \n- Implement a two-level Bayesian model in R using simulation or `brms`.  \n- Compare complete, no-pooling, and partial-pooling approaches.\n\n---\n\n## Lecture 1 — Motivation and Structure of Hierarchical Models\n\n### 1.1 Why Hierarchical Models?\n\nHierarchical models capture **structured variability** among related groups or units:\n- Repeated measures within individuals  \n- Students within classrooms  \n- Machines within factories  \n\nThey balance **within-group** and **between-group** information by introducing group-specific parameters drawn from a common population distribution.\n\n---\n\n### 1.2 Model Structure\n\nFor group $begin:math:text$ j = 1,\\\\ldots,J $end:math:text$ and observations $begin:math:text$ i = 1,\\\\ldots,n_j $end:math:text$:\n\n$$\ny_{ij} \\mid \\theta_j, \\sigma^2 \\sim \\mathcal{N}(\\theta_j, \\sigma^2), \\qquad\n\\theta_j \\mid \\mu, \\tau^2 \\sim \\mathcal{N}(\\mu, \\tau^2).\n$$\n\nTop-level priors:\n$$\n\\mu \\sim \\mathcal{N}(0,10^2), \\quad \\tau \\sim \\text{Half-Cauchy}(0,5).\n$$\n\n- $begin:math:text$ \\\\mu $end:math:text$: overall population mean  \n- $begin:math:text$ \\\\tau $end:math:text$: between-group standard deviation (pooling strength)  \n- $begin:math:text$ \\\\sigma $end:math:text$: within-group standard deviation\n\n---\n\n### 1.3 Three Extremes of Pooling\n\n| Model Type | Description | Behavior |\n|:------------|:-------------|:----------|\n| **No pooling** | Estimate each $begin:math:text$ \\\\theta_j $end:math:text$ separately | Ignores commonality across groups |\n| **Complete pooling** | Force all groups to share one parameter | Ignores group differences |\n| **Partial pooling** | Combine information via hierarchical prior | Balances both; default Bayesian choice |\n\n---\n\n### 1.4 Shrinkage Intuition\n\nPosterior for each group mean $begin:math:text$ \\\\theta_j $end:math:text$ shrinks toward the global mean $begin:math:text$ \\\\mu $end:math:text$:  \n$$\nE[\\theta_j \\mid y] = w_j \\bar{y}_j + (1-w_j)\\mu,\n$$\nwhere  \n$$\nw_j = \\frac{n_j/\\sigma^2}{n_j/\\sigma^2 + 1/\\tau^2}.\n$$\n\n- Large $begin:math:text$ n_j $end:math:text$ (lots of data): $begin:math:text$ w_j \\\\to 1 $end:math:text$ → less shrinkage.  \n- Small $begin:math:text$ n_j $end:math:text$: $begin:math:text$ w_j \\\\to 0 $end:math:text$ → stronger shrinkage toward $begin:math:text$ \\\\mu $end:math:text$.\n\n---\n\n### 1.5 Example — Simulated Group Means\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(6)\nJ <- 8; n_j <- rep(10, J)\nmu_true <- 5; tau_true <- 2; sigma_true <- 1\n\ntheta_true <- rnorm(J, mu_true, tau_true)\ny <- sapply(theta_true, function(tj) rnorm(10, tj, sigma_true))\nybar <- colMeans(y)\n\n# No pooling (group means)\nno_pool <- ybar\n\n# Complete pooling (global mean)\ncomplete_pool <- mean(y)\n\n# Partial pooling: simple empirical Bayes shrinkage\ntau_hat <- sd(ybar)\nsigma_hat <- sd(as.vector(y))\nw <- (n_j/sigma_hat^2) / (n_j/sigma_hat^2 + 1/tau_hat^2)\npartial_pool <- w*ybar + (1-w)*complete_pool\n\ndata.frame(Group=1:J,\n           ybar=round(ybar,2),\n           NoPool=round(no_pool,2),\n           Partial=round(partial_pool,2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Group ybar NoPool Partial\n1     1 5.53   5.53    5.53\n2     2 4.06   4.06    4.21\n3     3 6.90   6.90    6.76\n4     4 8.19   8.19    7.91\n5     5 4.86   4.86    4.93\n6     6 5.42   5.42    5.43\n7     7 2.20   2.20    2.55\n8     8 6.99   6.99    6.84\n```\n\n\n:::\n:::\n\n\nObserve how partial-pool estimates move small-sample groups toward the global mean.\n\n---\n\n### 1.6 Advantages of Hierarchical Models\n\n- Borrow strength across groups.  \n- Naturally incorporate uncertainty at multiple levels.  \n- Handle unbalanced data and missingness elegantly.  \n- Allow group-level predictors and complex dependence structures.\n\n---\n\n## Lecture 2 — Hierarchical Regression and Implementation\n\n### 2.1 Hierarchical Linear Regression\n\nGeneral form:\n$$\ny_{ij} = \\alpha_j + \\beta_j x_{ij} + \\varepsilon_{ij}, \\quad \\varepsilon_{ij} \\sim \\mathcal{N}(0,\\sigma^2),\n$$\n$$\n\\alpha_j \\sim \\mathcal{N}(\\mu_\\alpha, \\tau_\\alpha^2), \\quad\n\\beta_j \\sim \\mathcal{N}(\\mu_\\beta, \\tau_\\beta^2).\n$$\n\nThis allows both intercepts and slopes to vary by group.\n\n---\n\n### 2.2 Example — Hierarchical Regression with `brms`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(brms)\nset.seed(7)\n\nJ <- 10\nn_j <- 20\ngroup <- rep(1:J, each=n_j)\nx <- rnorm(J*n_j, 0, 1)\n\nalpha_true <- rnorm(J, 2, 1)\nbeta_true  <- rnorm(J, 3, 0.5)\nsigma_true <- 0.8\n\ny <- alpha_true[group] + beta_true[group]*x + rnorm(J*n_j, 0, sigma_true)\ndat <- data.frame(y, x, group=factor(group))\n\n# Hierarchical model (random intercept and slope)\nm_hier <- brm(y ~ 1 + x + (1 + x | group),\n              data=dat, family=gaussian(),\n              chains=2, iter=2000, refresh=0)\n\nsummary(m_hier)\nplot(m_hier)\n```\n:::\n\n\nThe `(1 + x | group)` formula defines a **varying intercept and slope** for each group.\n\n---\n\n### 2.3 Interpretation\n\nPosterior summaries provide:\n- Group-level means $begin:math:text$ \\\\alpha_j, \\\\beta_j $end:math:text$.  \n- Population-level means $begin:math:text$ \\\\mu_\\\\alpha, \\\\mu_\\\\beta $end:math:text$.  \n- Variability estimates $begin:math:text$ \\\\tau_\\\\alpha, \\\\tau_\\\\beta $end:math:text$ showing degree of pooling.\n\nVisualize partial pooling by comparing group-specific fits to the global regression line.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp_check(m_hier)\nplot(conditional_effects(m_hier), points=TRUE)\n```\n:::\n\n\n---\n\n### 2.4 Practical Considerations\n\n- Choose weakly informative hyperpriors for scale parameters (e.g., Half-Cauchy or Exponential).  \n- Inspect group-level posterior intervals to assess pooling.  \n- Center predictors for numerical stability.  \n- Use hierarchical models as the default when groups share a common process.\n\n---\n\n### 2.5 Summary of Hierarchical Modeling Benefits\n\n| Feature | Description |\n|:---------|:-------------|\n| **Partial pooling** | Shares strength across groups while retaining group differences. |\n| **Shrinkage** | Stabilizes small-sample estimates toward population mean. |\n| **Interpretability** | Captures multi-level variation naturally. |\n| **Predictive accuracy** | Usually superior to separate or fully pooled models. |\n\n---\n\n## Homework 6\n\n1. **Conceptual**  \n   - Explain why hierarchical modeling is often superior to analyzing groups separately.  \n   - Distinguish between complete pooling, no pooling, and partial pooling.\n\n2. **Computational**  \n   - Simulate a small dataset with several groups and fit:  \n     a. Separate regressions (no pooling).  \n     b. A single pooled regression.  \n     c. A hierarchical model (partial pooling).  \n   - Compare estimates for each group and interpret shrinkage behavior.\n\n3. **Reflection**  \n   - In what situations would you *not* use a hierarchical model?  \n   - How does the hierarchical prior act as a regularizer?\n\n---\n\n## Key Takeaways\n\n| Concept | Summary |\n|----------|----------|\n| Hierarchical Model | Combines group-level and population-level inference. |\n| Partial Pooling | Balances within- and between-group information. |\n| Shrinkage | Moves noisy group estimates toward a global mean. |\n| Hierarchical Regression | Extends pooling to both intercepts and slopes. |\n| Practical Insight | Default choice when analyzing grouped or multilevel data. |\n\n---\n\n**Next Week:** Bayesian Decision Theory — introducing utilities, losses, and optimal decision rules under uncertainty.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}