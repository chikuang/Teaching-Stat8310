{
  "hash": "989b67d7e2f0ac26420032ef20d5557e",
  "result": {
    "engine": "knitr",
    "markdown": "# Week 5 — Model Checking and Comparison\n\nThis week introduces methods for evaluating Bayesian model adequacy and comparing models.  \nWe focus on **Posterior Predictive Checking (PPC)** and **Bayesian Model Comparison** via Bayes factors, WAIC, and LOO.  \nStudents will diagnose model fit using replicated data and compare predictive accuracy among competing models.\n\n---\n\n## Learning Goals\n\nBy the end of this week, you should be able to:\n\n- Generate and interpret posterior predictive distributions.  \n- Use posterior predictive checks to detect model misspecification.  \n- Compute and interpret WAIC, LOO, and Bayes factors.  \n- Evaluate model adequacy visually and numerically in R.\n\n---\n\n## Lecture 1 — Posterior Predictive Checking\n\n### Posterior Predictive Distribution\n\nFor data $y$ and parameters $\\theta$,\n$$\np(\\tilde{y}\\mid y) = \\int p(\\tilde{y}\\mid\\theta)\\,p(\\theta\\mid y)\\,d\\theta.\n$$\nIf a model is adequate, the observed data $y$ should look typical among replicated datasets $\\tilde{y}$ simulated from this distribution.\n\n### Implementation Steps\n\n1. Choose a **discrepancy statistic** $T(y,\\theta)$ capturing an aspect of fit.  \n2. For each posterior draw $\\theta^{(m)}$:  \n   - Simulate $\\tilde{y}^{(m)}\\!\\sim\\!p(\\tilde{y}\\mid\\theta^{(m)})$.  \n   - Compute $T(y,\\theta^{(m)})$ and $T(\\tilde{y}^{(m)},\\theta^{(m)})$.  \n3. Compute posterior predictive $p$-value:  \n   $$\n   p_{\\text{ppc}}=P\\!\\left(T(\\tilde{y},\\theta)\\!>\\!T(y,\\theta)\\mid y\\right).\n   $$\n\nValues near 0 or 1 suggest lack of fit.\n\n### Example A — Binomial Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(5)\nM <- 5000\ny_obs <- 7; n <- 10\n\ntheta <- rbeta(M, 2 + y_obs, 2 + n - y_obs)   # posterior draws\ny_rep <- rbinom(M, n, theta)\n\nppc_p <- mean(y_rep >= y_obs)\nppc_p\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.509\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(y_rep, breaks=seq(-0.5, n+0.5, by=1),\n     col=\"skyblue\", main=\"Posterior Predictive Distribution\", xlab=\"Replicated ỹ\")\nabline(v=y_obs, col=\"red\", lwd=2)\nlegend(\"topright\", legend=c(\"Observed y\"), col=\"red\", lwd=2, bty=\"n\")\n```\n\n::: {.cell-output-display}\n![Posterior predictive distribution of ỹ](week05_files/figure-pdf/ppc-binomial-plot-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n### Example B — Normal Model (Standard Deviation Check)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(6)\ny <- rnorm(30, mean=0, sd=1)\nmu_draw <- rnorm(1000, 0, 1)\ny_rep_mat <- replicate(200, rnorm(length(y), sample(mu_draw,1), 1))\n\nT_obs <- sd(y)\nT_rep <- apply(y_rep_mat, 2, sd)\n\nmean(T_rep >= T_obs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.145\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(T_rep, col=\"lightgray\", main=\"Posterior Predictive Check: SD\",\n     xlab=\"Replicated sd(ỹ)\")\nabline(v=T_obs, col=\"red\", lwd=2)\nlegend(\"topright\", legend=c(\"Observed sd(y)\"), col=\"red\", lwd=2, bty=\"n\")\n```\n\n::: {.cell-output-display}\n![PPC for sample standard deviation](week05_files/figure-pdf/ppc-normal-plot-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n### Practical Tips\n\n- Use **multiple** statistics ($T_1, T_2,\\dots$).  \n- Visual checks often outperform a single numeric $p_{\\text{ppc}}$.  \n- Integrate PPC with subject-matter knowledge and residual plots.\n\n---\n\n## Lecture 2 — Bayesian Model Comparison\n\n### Motivation\n\nCompeting Bayesian models are compared by their fit and complexity.  \nCommon approaches include **Bayes factors**, **WAIC**, and **LOO**.\n\n---\n\n### Bayes Factors\n\nGiven two models $M_1$ and $M_2$,\n$$\n\\text{BF}_{12} = \\frac{p(y\\mid M_1)}{p(y\\mid M_2)} ,\\qquad\np(y\\mid M)=\\int p(y\\mid\\theta_M,M)\\,p(\\theta_M\\mid M)\\,d\\theta_M.\n$$\n- $\\text{BF}_{12}>1$ favors $M_1$.  \n- Express in $\\log_{10}$ scale for interpretation.\n\n| $\\log_{10}\\text{BF}_{12}$ | Evidence for $M_1$ |\n|:--------------------------:|:------------------|\n| 0–0.5 | Barely worth mentioning |\n| 0.5–1 | Substantial |\n| 1–2 | Strong |\n| >2 | Decisive |\n\n---\n\n### WAIC and LOO (Predictive Criteria)\n\nWhen computing marginal likelihoods is infeasible, we use predictive criteria:\n\n- **WAIC (Watanabe–Akaike Information Criterion)**  \n  $$\n  \\text{WAIC} = -2(\\text{lppd} - p_{\\text{WAIC}}),\n  $$\n  where $\\text{lppd}=\\sum_i \\log\\!\\left(\\frac{1}{S}\\sum_{s=1}^S p(y_i\\mid\\theta^{(s)})\\right)$.\n\n- **LOO (Leave-One-Out Cross-Validation)**  \n  Approximates the out-of-sample predictive performance:\n  $$\n  \\text{LOO} = \\sum_i \\log p(y_i \\mid y_{-i}),\n  $$\n  usually estimated via Pareto-smoothed importance sampling (PSIS-LOO).\n\nLower WAIC (or higher elpd_loo) indicates better predictive performance.\n\n---\n\n### Example A — Comparing Two Regression Models with `brms` (optional heavy computation)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Uncomment and install if needed\n# install.packages(c(\"brms\", \"loo\"))\n\nlibrary(brms)\nlibrary(loo)\n\nset.seed(7)\ndat <- data.frame(x = rnorm(200))\ndat$y <- 2 + 3*dat$x + 1.5*dat$x^2 + rnorm(200, sd = 1)  # true quadratic\n\n# Fit linear vs quadratic models\nm1 <- brm(y ~ x, data = dat, family = gaussian(), refresh = 0)\nm2 <- brm(y ~ x + I(x^2), data = dat, family = gaussian(), refresh = 0)\n\nloo1 <- loo(m1)\nloo2 <- loo(m2)\nloo_compare(loo1, loo2)\n\npp_check(m1)\npp_check(m2)\n```\n:::\n\n\n---\n\n### Example B — Quick WAIC Comparison via Frequentist Approximation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nN <- 150\nx <- rnorm(N)\ny <- 1.5 + 2.2*x + rnorm(N, sd=1.2)\n\nm1 <- lm(y ~ x)\nm2 <- lm(y ~ poly(x, 2, raw = TRUE))\n\nsigma1 <- summary(m1)$sigma\nsigma2 <- summary(m2)$sigma\n\n# Pseudo-WAIC: approximate lppd - 2p using residual sum of squares\nRSS1 <- sum(resid(m1)^2)\nRSS2 <- sum(resid(m2)^2)\nnpar1 <- length(coef(m1))\nnpar2 <- length(coef(m2))\n\nWAIC1 <- -2 * (sum(dnorm(y, predict(m1), sigma1, log=TRUE)) - npar1)\nWAIC2 <- -2 * (sum(dnorm(y, predict(m2), sigma2, log=TRUE)) - npar2)\n\ndata.frame(Model=c(\"Linear\",\"Quadratic\"), WAIC=c(WAIC1,WAIC2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      Model     WAIC\n1    Linear 473.9530\n2 Quadratic 475.7823\n```\n\n\n:::\n:::\n\n\n---\n\n### Visual Predictive Comparison\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(x, y, pch=19, col=\"#00000055\", main=\"Observed Data with Fitted Models\")\nxs <- seq(min(x), max(x), length.out=200)\nlines(xs, predict(m1, newdata=data.frame(x=xs)), col=\"steelblue\", lwd=2)\nlines(xs, predict(m2, newdata=data.frame(x=xs)), col=\"firebrick\", lwd=2)\nlegend(\"topleft\", lwd=2, col=c(\"steelblue\",\"firebrick\"),\n       legend=c(\"Linear\",\"Quadratic\"), bty=\"n\")\n```\n\n::: {.cell-output-display}\n![Overlay of linear and quadratic model fits](week05_files/figure-pdf/visual-overlay-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n---\n\n### Practical Summary\n\n| Method | Strength | Limitation |\n|:--|:--|:--|\n| Posterior Predictive Check | Diagnoses lack of fit to **observed** data | Not inherently comparative |\n| Bayes Factor | Theoretically coherent model evidence | Sensitive to priors; hard integration |\n| WAIC / LOO | Out-of-sample predictive performance | Approximate; needs posterior draws |\n\n---\n\n## Lab 5 — Model Checking and Comparison\n\n**Objectives**\n\n1. Perform PPC using both numerical and visual methods.  \n2. Compute and interpret WAIC and LOO for model selection.  \n3. Visualize predictive differences among models.\n\n**Packages**\n\n`brms`, `loo`, `bayesplot`, `ggplot2`\n\n**Tasks**\n\n- Fit two Bayesian regression models on the same dataset.  \n- Conduct posterior predictive checks and compare simulated vs. observed data.  \n- Compute WAIC and LOO; summarize which model performs better.\n\n---\n\n## Homework 5\n\n1. **Conceptual**  \n   - Explain the purpose of posterior predictive checks.  \n   - Compare WAIC and LOO conceptually.\n\n2. **Computational**  \n   - Simulate data from a known model. Fit two Bayesian models in R.  \n   - Use PPC, WAIC, and LOO to assess fit.  \n   - Discuss how model choice depends on criterion used.\n\n3. **Reflection**  \n   - Why might visual checks and numerical metrics disagree?  \n   - Which model would you report and why?\n\n---\n\n## Key Takeaways\n\n| Concept | Summary |\n|----------|----------|\n| Posterior Predictive Check | Compares observed data to replicated draws under the posterior. |\n| Posterior Predictive p-Value | Quantifies fit; extremes suggest model misfit. |\n| WAIC / LOO | Predictive performance measures for Bayesian models. |\n| Bayes Factor | Ratio of marginal likelihoods for model comparison. |\n| Combined Evaluation | Use graphical and numerical criteria together. |\n\n---\n\n**Next Week:** Hierarchical Bayesian Models — introducing partial pooling and shrinkage.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}