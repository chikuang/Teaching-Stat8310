{
  "hash": "1794309140a96e6a932ba1aa99ecf69a",
  "result": {
    "engine": "knitr",
    "markdown": "# Week 1 — Introduction to Bayesian Thinking\n\n---\n\n## Lecture 1: Motivation and Philosophy of the Bayesian Approach\n\n### 1.1 Probability as Belief\n- In the frequentist view, probability describes *long-run frequencies* of repeated events.  \n- In the Bayesian view, probability represents *degrees of belief* about uncertain quantities.  \n  This interpretation allows us to express uncertainty about parameters, models, and hypotheses.\n\n### 1.2 Why Bayesian?\n1. **Unified logic of inference:**  \n   All uncertainty (parameters, predictions, models) is treated probabilistically.  \n2. **Incorporation of prior knowledge:**  \n   Prior distributions let analysts integrate existing evidence or expert opinion.  \n3. **Flexibility:**  \n   The Bayesian framework handles hierarchical, missing-data, and complex models naturally.  \n4. **Decision-theoretic foundation:**  \n   Bayesian inference directly supports optimal decisions under uncertainty.  \n5. **Computational advances:**  \n   MCMC and modern probabilistic programming (e.g., Stan, PyMC, JAGS) make Bayesian analysis practical.\n\n### 1.3 When to Use the Bayesian Approach\n- Small-sample or sparse data problems where prior knowledge helps stabilize inference.  \n- Situations with sequential data collection or adaptive designs.  \n- Contexts demanding *direct probability statements* about parameters or hypotheses.  \n- Decision-making scenarios that require explicit uncertainty quantification.\n\n### 1.4 Illustrative Example\nSuppose a factory tests 10 light bulbs and finds 8 working.  \n- A frequentist estimates the proportion as 0.8 with a confidence interval.  \n- A Bayesian treats the true proportion $ \\theta $ as random and updates beliefs via  \n  $ p(\\theta \\mid y) \\propto p(y \\mid \\theta)\\, p(\\theta) $.  \nThe result: an explicit posterior distribution over $ \\theta $, not a single point estimate.\n\n---\n\n## Lecture 2: Bayes’ Theorem and the Building Blocks of Inference\n\n### 2.1 Bayes' Theorem\n\nFor parameters $\\\\theta$ and observed data $y$:\n\n$$\np(\\theta \\mid y) = \\frac{p(y \\mid \\theta)\\, p(\\theta)}{p(y)} = \n\\frac{p(y \\mid \\theta)\\, p(\\theta)}{\\int p(y \\mid \\theta)\\, p(\\theta)\\, d\\theta}.\n$$\n\nWhere:\n- $p(\\theta)$: **Prior** — expresses beliefs before seeing data.  \n- $p(y \\mid \\theta)$: **Likelihood** — the data-generating model.  \n- $p(\\theta \\mid y)$: **Posterior** — updated belief after seeing data.  \n- $p(y)$: **Marginal likelihood / evidence** — normalizing constant.\n\n### 2.2 The Three Key Components\n| Component | Description | Example |\n|------------|-------------|----------|\n| **Prior** | Encodes information about $ \\theta $ before data | $ \\text{Beta}(2,2) $ for coin bias |\n| **Likelihood** | Probability model for data given $ \\theta $ | $ \\text{Binomial}(n=10, \\theta) $ |\n| **Posterior** | Updated distribution combining both | $ \\text{Beta}(2+y, 2+n-y) $ |\n\n### 2.3 Interpretation\n- Posterior mean: expected value of $ \\theta $ after observing data.  \n- Posterior credible interval: range where $ \\theta $ lies with high probability (e.g., 95%).  \n- Posterior predictive distribution: used to predict future data.\n\n### 2.4 Key Insight\n> The likelihood updates the prior in light of data — the posterior is the result of this update.\n\n---\n\n## Lecture 3: Simple Analytical Examples of Bayesian Updating\n\n### 3.1 Beta–Binomial Model (Coin-Flip Example)\n\n**Setup:**  \nWe flip a coin $ n $ times and observe $ y $ heads.  \nLet $ \\theta $ be the true probability of heads.\n\n$$\ny \\mid \\theta \\sim \\text{Binomial}(n, \\theta), \\quad \\theta \\sim \\text{Beta}(\\alpha_0, \\beta_0).\n$$\n\n**Posterior:**\n$$\n\\theta \\mid y \\sim \\text{Beta}(\\alpha_0 + y, \\beta_0 + n - y).\n$$\n\n**Posterior mean:**\n$$\nE[\\theta \\mid y] = \\frac{\\alpha_0 + y}{\\alpha_0 + \\beta_0 + n}.\n$$\n\n**Interpretation:**  \n- The prior acts as *pseudo-data*: $\\alpha_0 - 1$ prior successes and $\\beta_0 - 1$ prior failures.  \n- As $ n $ grows large, the data dominate the posterior.\n\n**Visualization:**  \nPlot prior, likelihood, and posterior to show how the distribution tightens around the true value.\n\n---\n\n### 3.2 Normal–Normal Model (Inference on a Mean)\n\n**Setup:**  \nData $ y_1, \\dots, y_n $ are i.i.d. $ \\mathcal{N}(\\mu, \\sigma^2) $, with known variance $ \\sigma^2 $.  \nWe place a prior $ \\mu \\sim \\mathcal{N}(\\mu_0, \\tau_0^2) $.\n\n**Posterior:**\n$$\n\\mu \\mid y \\sim \\mathcal{N}(\\mu_1, \\tau_1^2),\n$$\nwhere  \n$$\n\\tau_1^2 = \\left( \\frac{1}{\\tau_0^2} + \\frac{n}{\\sigma^2} \\right)^{-1}, \\quad\n\\mu_1 = \\tau_1^2 \\left( \\frac{\\mu_0}{\\tau_0^2} + \\frac{n \\bar{y}}{\\sigma^2} \\right).\n$$\n\n**Interpretation:**  \nThe posterior mean is a *weighted average* of the prior mean and sample mean:\n$$\n\\mu_1 = w \\mu_0 + (1-w)\\bar{y}, \\quad w = \\frac{\\sigma^2}{\\sigma^2 + n\\tau_0^2}.\n$$\n\nWhen $ \\tau_0^2 $ is large (weak prior), $ \\mu_1 \\approx \\bar{y} $.  \nWhen data are scarce, the posterior leans more on the prior.\n\n---\n\n### 3.3 Posterior Predictive Distribution\nFor a future observation $ \\tilde{y} $:\n$$\np(\\tilde{y} \\mid y) = \\int p(\\tilde{y} \\mid \\theta)\\, p(\\theta \\mid y)\\, d\\theta.\n$$\nExample (Beta–Binomial):\n$$\np(\\tilde{y} = 1 \\mid y) = E[\\theta \\mid y] =\n\\frac{\\alpha_0 + y}{\\alpha_0 + \\beta_0 + n}.\n$$\n\nThis predictive probability reflects both uncertainty in $ \\theta $ and random variation in new data.\n\n---\n\n### 3.4 Discussion and Concept Reinforcement\n- Priors influence the posterior most when data are limited.  \n- With sufficient data, Bayesian results converge to frequentist ones (Bernstein–von Mises theorem).  \n- Credible intervals directly express probability statements about parameters.  \n- Model assumptions (e.g., conjugacy, independence) simplify computation but can be relaxed using MCMC.\n\n---\n\n### 3.5 Practical Example (R Demonstration)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Posterior update for a Binomial model\nalpha0 <- 2; beta0 <- 2  # prior\nn <- 10; y <- 7          # data\nalpha1 <- alpha0 + y; beta1 <- beta0 + n - y\n\ntheta <- seq(0, 1, length.out = 500)\nplot(theta, dbeta(theta, alpha0, beta0), type=\"l\", lwd=2, col=\"blue\",\n     ylab=\"Density\", xlab=expression(theta),\n     main=\"Prior, Likelihood, and Posterior\")\nlines(theta, dbeta(theta, alpha1, beta1), col=\"red\", lwd=2)\nlegend(\"topright\",\n       legend=c(\"Prior Beta(2,2)\", \"Posterior Beta(9,5)\"),\n       col=c(\"blue\", \"red\"), lwd=2)\n```\n\n::: {.cell-output-display}\n![](week01_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}