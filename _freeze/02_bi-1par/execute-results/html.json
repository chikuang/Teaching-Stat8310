{
  "hash": "503def5187d3823c8213559ed3efa3eb",
  "result": {
    "engine": "knitr",
    "markdown": "# Bayesian Inference for single parameter models\n\n> Leading objectives:\n>\n> Understand how to perform Bayesian inference on a single parameter model.\n>\n> -   Binomial model with given n\n> -   Poission model\n> -   Exponential family\n\nRecall the important ingredients of Bayesian inference:\n\n1.  **Prior distribution:** $p(\\theta)$\n2.  **Likelihood function:** $p(y \\mid \\theta)$\n3.  **Posterior distribution:** $p(\\theta \\mid y) \\propto p(y \\mid \\theta) p(\\theta)$\n\n## Three basic ingredients of Bayesian inference\n\n### Prior\n\nThe prior distribution encodes our beliefs about the parameter $\\theta$ *before* conduct any experiments.\n\n\n::: {.callout-note title = \"Prior and Data are independent\"}\n\nNote that, the prior distribution is independent of the data. It represents our knowledge or beliefs about the parameter before seeing the data. \n\n:::\n\nHow do we choose a prior?\n\n1.  **Informative priors:** Based on previous studies or expert knowledge\n2.  **Weakly informative priors:** Provide some regularization without dominating the data\n3.  **Non-informative priors:** Attempt to be \"objective\" (e.g., uniform, Jeffreys prior)\n\n### Likelihood\n\nThe likelihood function represents the probability of observing the data given the parameter $\\theta$. It can be derived from the assumed statistical model for the data or experiment, i.e., $y \\sim p(y \\mid \\theta)$, or we can estimate this non-parametrically (i.e., without assuming the underlying distribution is the one we know.).\n\n\n::: {.callout-note title = \"Likelihood is NOT a probability distribution for $\\theta$\"}\n\nNote that, the likelihood function is not a probability distribution for $\\theta$ itself. It is a function of $\\theta$ for fixed data $y$. \n\n:::\n\n### Posterior\n\nThe posterior distribution combines the prior and likelihood to update our beliefs about $\\theta$ after observing the data. It is given by Bayes' theorem: $$\np(\\theta \\mid y) = \\frac{p(y \\mid \\theta) p(\\theta)}{p(y)},\n$$ where $p(y) = \\int p(y \\mid \\theta) p(\\theta) d\\theta$ is the marginal likelihood or evidence.\n\n### An simple example\n\n\n**Examples:**\n\n-   Beta prior + Binomial likelihood → Beta posterior\n-   Normal prior + Normal likelihood (known variance) → Normal posterior\n-   Gamma prior + Poisson likelihood → Gamma posterior\n\n**Advantages:** - Analytical posteriors (no numerical integration needed) - Interpretable parameters - Computationally efficient\n\n**Limitations:**\n\n-   May not reflect true prior beliefs\n-   Modern computing makes non-conjugate priors feasible\n\n::: {.callout-example title = \"Why Conjugate Priors?\"}\n\nLet's look a simple example to illustrate the convenience of conjugate priors. Consider a Binomial model with unknown success probability $\\theta$ and known number of trials $n$. We can use a Beta prior for $\\theta$.\n\nSuppose we have a Binomial model with known number of trials $n$ and unknown success probability $\\theta$. We can use a Beta prior for $\\theta$.\n\n-   **Prior:** $\\theta \\sim \\text{Beta}(\\alpha, \\beta)$\n-   **Likelihood:** $y \\mid \\theta \\sim \\text{Binomial}(n, \\theta)$\n\nThe derivation of the posterior is as follows:\n\n$$\n\\begin{aligned}\np(y \\mid \\theta) & = \\binom{n}{y} \\theta^y (1 - \\theta)^{n - y}, \\\\\np(\\theta) & = \\frac{\\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1}}{B(\\alpha, \\beta)},\n\\end{aligned}\n$$ where $B(\\alpha, \\beta)$ is the Beta function. Then the posterior is proportional to: $$\np(\\theta \\mid y) \\propto p(y \\mid \\theta) p(\\theta) \\propto \\theta^{y + \\alpha - 1} (1 -  \\theta)^{n - y + \\beta - 1}.\n$$ This is the kernel of a Beta distribution with parameters $(\\alpha + y, \\beta + n - y)$. Thus, the posterior distribution is: $$\n\\theta \\mid y \\sim \\text{Beta}(\\alpha + y, \\beta\n + n - y).\n$$\n\nThus, the **Posterior** is $\\theta \\mid y \\sim \\text{Beta}(\\alpha + y, \\beta + n - y)$.\n\n:::\n\n## Binomial model with given $n$\n\n### Conjugate Priors\n\n::: {.callout-definition title = \"Conjugate Prior\"}\n\nA prior distribution is **conjugate** to a likelihood if the posterior distribution is in the same family as the prior.\n\n:::\n\n### Happiness data\n\n## 3.1 The Binomial Model\n\n### Happiness data\n\nIn the 1998 General Social Survey, each female respondent aged 65 or over was asked whether she was generally happy.\n\nDefine the response variable\n$$\nY_i =\n\\begin{cases}\n1, & \\text{if respondent } i \\text{ reports being generally happy},\\\\\n0, & \\text{otherwise},\n\\end{cases}\n\\qquad i = 1,\\ldots,n,\n$$\nwhere $n = 129$.\n\nBecause we do not have additional information that distinguishes these individuals, it is reasonable to treat their responses as **exchangeable**.  \nIn other words, before seeing the data, we do not believe the ordering or labels of the respondents carry any information.\n\nMoreover, since the sample size $n = 129$ is small compared to the total population size $N$ of female senior citizens, the results from the previous chapter suggest that the following approximation is appropriate.\n\n### Modeling assumptions\n\nOur joint beliefs about \\((Y_1,\\ldots,Y_{129})\\) are described by two components:\n\n- **Uncertainty about a population proportion**\n  \n  $$\n  \\theta = \\frac{1}{N}\\sum_{i=1}^N Y_i,\n  $$\nwhere $\\theta$ represents the proportion of generally happy individuals in the population.\n\n- **Sampling model given $\\theta$**  \n  Conditional on $\\theta$, the responses $Y_1,\\ldots,Y_{129}$ are independent and identically distributed Bernoulli random variables with\n$$\n  \\Pr(Y_i = 1 \\mid \\theta) = \\theta.\n$$\n\nIn words:  \n\n> Given the population proportion $\\theta$, each respondent independently reports being happy with probability $\\theta$.\n\n\n### Likelihood\n\nUnder this model, the probability of observing a particular outcome\n$\\{y_1,\\ldots,y_{129}\\}$, conditional on $\\theta$, is\n$$\np(y_1,\\ldots,y_{129} \\mid \\theta)\n= \\theta^{\\sum_{i=1}^{129} y_i}\n(1-\\theta)^{129-\\sum_{i=1}^{129} y_i}.\n$$\n\nThis expression depends on the data only through the sufficient statistic\n$$\nS = \\sum_{i=1}^{129} Y_i,\n$$\nthe total number of respondents who report being generally happy.\n\nWhat remains to be specified is our **prior distribution** for $\\theta$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\n# Grid of theta values\ntheta <- seq(0, 1, length.out = 1000)\n\n# Prior specifications\npriors <- data.frame(\n  alpha = c(1, 2, 5, 20),\n  beta  = c(1, 2, 5, 20),\n  label = c(\n    \"Beta(1,1): Uniform\",\n    \"Beta(2,2): Mildly informative\",\n    \"Beta(5,5): Concentrated near 0.5\",\n    \"Beta(20,20): Strong prior belief\"\n  )\n)\n\n# Compute densities\nprior_df <- do.call(rbind, lapply(1:nrow(priors), function(i) {\n  data.frame(\n    theta = theta,\n    density = dbeta(theta, priors$alpha[i], priors$beta[i]),\n    Prior = priors$label[i]\n  )\n}))\n\n# Plot\nggplot(prior_df, aes(x = theta, y = density, color = Prior)) +\n  geom_line(linewidth = 1.1) +\n  labs(\n    title = \"Different Prior Distributions for a Binomial Proportion\",\n    x = expression(theta),\n    y = \"Density\",\n    color = \"Prior\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"right\",\n    legend.box = \"vertical\"\n  )\n```\n\n::: {.cell-output-display}\n![](02_bi-1par_files/figure-html/Different Priors for theta-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\n# ----------------------------\n# Data (GSS happiness example)\n# ----------------------------\nn <- 129\nS <- 118  # happy\n\n# ----------------------------\n# Prior (edit if you want)\n# ----------------------------\nalpha <- 2\nbeta  <- 2\n\n# Posterior parameters\nalpha_post <- alpha + S\nbeta_post  <- beta + (n - S)\n\n# Grid for theta\ntheta <- seq(0, 1, length.out = 2000)\n\n# Prior density\nprior <- dbeta(theta, alpha, beta)\n\n# Likelihood (scaled to max = 1 for plotting)\nlikelihood_raw <- theta^S * (1 - theta)^(n - S)\nlikelihood <- likelihood_raw / max(likelihood_raw)\n\n# Posterior density\nposterior <- dbeta(theta, alpha_post, beta_post)\n\n# Combine\ndf <- data.frame(\n  theta = rep(theta, 3),\n  density = c(prior, likelihood, posterior),\n  Distribution = factor(\n    rep(c(\"Prior\", \"Likelihood (scaled)\", \"Posterior\"), each = length(theta)),\n    levels = c(\"Prior\", \"Likelihood (scaled)\", \"Posterior\")\n  )\n)\n\n# Posterior mean (for annotation)\npost_mean <- alpha_post / (alpha_post + beta_post)\n\n# Plot\nggplot(df, aes(x = theta, y = density, color = Distribution)) +\n  geom_line(linewidth = 1.2) +\n  geom_vline(xintercept = post_mean, linetype = \"dashed\", linewidth = 0.9) +\n  labs(\n    title = sprintf(\"Bayesian Updating (n=%d, happy=%d)\", n, S),\n    subtitle = sprintf(\"Prior: Beta(%d,%d)  |  Posterior: Beta(%d,%d)  |  Posterior mean = %.3f\",\n                       alpha, beta, alpha_post, beta_post, post_mean),\n    x = expression(theta),\n    y = \"Density (Likelihood Scaled)\",\n    color = \"\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"right\",\n    legend.box = \"vertical\"\n  )\n```\n\n::: {.cell-output-display}\n![](02_bi-1par_files/figure-html/posterior-1.png){width=672}\n:::\n:::\n\n\n## 3.1.1 A Uniform Prior Distribution\n\nRecall that the parameter $\\theta$ represents an unknown proportion and must lie between 0 and 1.\n\nSuppose our prior information is very weak, in the sense that **all subintervals of $[0,1]$ with the same length are equally plausible**.  \nSymbolically, this means that for any $0 \\le a < b < b+c \\le 1$,\n$$\n\\Pr(a \\le \\theta \\le b)\n=\n\\Pr(a+c \\le \\theta \\le b+c).\n$$\n\nThis condition implies that the prior density for $\\theta$ must be **uniform** on $[0,1]$:\n$$\np(\\theta) = 1, \\qquad 0 \\le \\theta \\le 1.\n$$\n\n---\n\n## Likelihood under the Binomial Model\n\nRecall the data:\n\n- $n = 129$ individuals were surveyed;\n- $118$ individuals report being generally happy;\n- $11$ individuals report not being generally happy.\n\nLet\n$$\nS = \\sum_{i=1}^{129} Y_i = 118.\n$$\n\nUnder the binomial sampling model, the probability of observing this data, conditional on $\\theta$, is\n$$\np(y_1,\\ldots,y_{129} \\mid \\theta)\n= \\theta^{118}(1-\\theta)^{11}.\n$$\n\nThis function of $\\theta$ is called the **likelihood**.\n\n---\n\n## Posterior Distribution: Shape vs Scale\n\nBayes’ rule gives\n$$\np(\\theta \\mid y_1,\\ldots,y_{129})\n=\n\\frac{p(y_1,\\ldots,y_{129} \\mid \\theta)\\,p(\\theta)}\n     {p(y_1,\\ldots,y_{129})}.\n$$\n\nSince the prior is uniform, $p(\\theta)=1$, we have\n$$\np(\\theta \\mid y_1,\\ldots,y_{129})\n\\propto\n\\theta^{118}(1-\\theta)^{11}.\n$$\n\nThis shows an important principle:\n\n> **With a uniform prior, the posterior distribution has the same shape as the likelihood.**\n\nHowever, the posterior must integrate to 1, so it requires a **normalizing constant**.\n\n---\n\n## Normalizing the Posterior\n\nBecause $p(\\theta \\mid y)$ is a probability density,\n$$\n\\int_0^1 p(\\theta \\mid y)\\,d\\theta = 1.\n$$\n\nUsing Bayes’ rule again,\n$$\n1\n=\n\\frac{1}{p(y_1,\\ldots,y_{129})}\n\\int_0^1 \\theta^{118}(1-\\theta)^{11}\\,d\\theta.\n$$\n\nFrom calculus, we know that\n$$\n\\int_0^1 \\theta^{a-1}(1-\\theta)^{b-1}\\,d\\theta\n=\n\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}.\n$$\n\nApplying this result with $a=119$ and $b=12$,\n$$\np(y_1,\\ldots,y_{129})\n=\n\\frac{\\Gamma(119)\\Gamma(12)}{\\Gamma(131)}.\n$$\n\n---\n\n## The Posterior Distribution\n\nPutting everything together, the posterior density is\n$$\np(\\theta \\mid y_1,\\ldots,y_{129})\n=\n\\frac{\\Gamma(131)}{\\Gamma(119)\\Gamma(12)}\n\\theta^{119-1}(1-\\theta)^{12-1}.\n$$\n\nThis is a **Beta distribution** with parameters\n$$\n\\theta \\mid y \\sim \\mathrm{Beta}(119,\\,12).\n$$\n\n---\n\n## The Beta Distribution\n\nIn general, a random variable $\\theta \\in (0,1)$ has a Beta distribution with parameters $a>0$ and $b>0$ if\n$$\np(\\theta)\n=\n\\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\n\\theta^{a-1}(1-\\theta)^{b-1},\n\\qquad 0 \\le \\theta \\le 1.\n$$\n\nFor a Beta$(a,b)$ distribution:\n\n- Mean:\n  $$\n  \\mathbb{E}(\\theta) = \\frac{a}{a+b},\n  $$\n- Variance:\n  $$\n  \\mathrm{Var}(\\theta)\n  =\n  \\frac{ab}{(a+b)^2(a+b+1)}.\n  $$\n\n---\n\n## Posterior Summary for the Happiness Data\n\nFor the happiness data,\n$$\n\\theta \\mid y \\sim \\mathrm{Beta}(119,12).\n$$\n\nFrom this distribution:\n\n- Posterior mean:\n  $$\n  \\mathbb{E}(\\theta \\mid y) = 0.915,\n  $$\n- Posterior standard deviation:\n  $$\n  \\mathrm{sd}(\\theta \\mid y) = 0.025.\n  $$\n\nThese summaries quantify both our **best estimate** of the population proportion and our **remaining uncertainty** after observing the data.\n\n\n---\n\nThis Chapter follows closely with Chapter 3 in Hoff (2009).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}