{
  "hash": "4994cb7ecba27146092a8cbd756d6bbb",
  "result": {
    "engine": "knitr",
    "markdown": "# Bayesian Inference for single parameter models\n\n> Leading objectives:\n>\n> Understand how to perform Bayesian inference on a single parameter model.\n>\n> -   Binomial model with given n\n> -   Poission model\n> -   Exponential family\n\nRecall the important ingredients of Bayesian inference:\n\n1.  **Prior distribution:** $p(\\theta)$\n2.  **Likelihood function:** $p(y \\mid \\theta)$\n3.  **Posterior distribution:** $p(\\theta \\mid y) \\propto p(y \\mid \\theta) p(\\theta)$\n\n## Three basic ingredients of Bayesian inference\n\n### Prior\n\nThe prior distribution encodes our beliefs about the parameter $\\theta$ *before* conduct any experiments.\n\n\n::: {.callout-note title = \"Prior and Data are independent\"}\n\nNote that, the prior distribution is independent of the data. It represents our knowledge or beliefs about the parameter before seeing the data. \n\n:::\n\nHow do we choose a prior?\n\n1.  **Informative priors:** Based on previous studies or expert knowledge\n2.  **Weakly informative priors:** Provide some regularization without dominating the data\n3.  **Non-informative priors:** Attempt to be \"objective\" (e.g., uniform, Jeffreys prior)\n\n### Likelihood\n\nThe likelihood function represents the probability of observing the data given the parameter $\\theta$. It can be derived from the assumed statistical model for the data or experiment, i.e., $y \\sim p(y \\mid \\theta)$, or we can estimate this non-parametrically (i.e., without assuming the underlying distribution is the one we know.).\n\n\n::: {.callout-note title = \"Likelihood is NOT a probability distribution for $\\theta$\"}\n\nNote that, the likelihood function is not a probability distribution for $\\theta$ itself. It is a function of $\\theta$ for fixed data $y$. \n\n:::\n\n### Posterior\n\nThe posterior distribution combines the prior and likelihood to update our beliefs about $\\theta$ after observing the data. It is given by Bayes' theorem: $$\np(\\theta \\mid y) = \\frac{p(y \\mid \\theta) p(\\theta)}{p(y)},\n$$ where $p(y) = \\int p(y \\mid \\theta) p(\\theta) d\\theta$ is the marginal likelihood or evidence.\n\n### An simple example\n\n\n**Examples:**\n\n-   Beta prior + Binomial likelihood → Beta posterior\n-   Normal prior + Normal likelihood (known variance) → Normal posterior\n-   Gamma prior + Poisson likelihood → Gamma posterior\n\n**Advantages:** - Analytical posteriors (no numerical integration needed) - Interpretable parameters - Computationally efficient\n\n**Limitations:**\n\n-   May not reflect true prior beliefs\n-   Modern computing makes non-conjugate priors feasible\n\n::: {.callout-example title = \"Why Conjugate Priors?\"}\n\nLet's look a simple example to illustrate the convenience of conjugate priors. Consider a Binomial model with unknown success probability $\\theta$ and known number of trials $n$. We can use a Beta prior for $\\theta$.\n\nSuppose we have a Binomial model with known number of trials $n$ and unknown success probability $\\theta$. We can use a Beta prior for $\\theta$.\n\n-   **Prior:** $\\theta \\sim \\text{Beta}(\\alpha, \\beta)$\n-   **Likelihood:** $y \\mid \\theta \\sim \\text{Binomial}(n, \\theta)$\n\nThe derivation of the posterior is as follows:\n\n$$\n\\begin{aligned}\np(y \\mid \\theta) & = \\binom{n}{y} \\theta^y (1 - \\theta)^{n - y}, \\\\\np(\\theta) & = \\frac{\\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1}}{B(\\alpha, \\beta)},\n\\end{aligned}\n$$ where $B(\\alpha, \\beta)$ is the Beta function. Then the posterior is proportional to: $$\np(\\theta \\mid y) \\propto p(y \\mid \\theta) p(\\theta) \\propto \\theta^{y + \\alpha - 1} (1 -  \\theta)^{n - y + \\beta - 1}.\n$$ This is the kernel of a Beta distribution with parameters $(\\alpha + y, \\beta + n - y)$. Thus, the posterior distribution is: $$\n\\theta \\mid y \\sim \\text{Beta}(\\alpha + y, \\beta\n + n - y).\n$$\n\nThus, the **Posterior** is $\\theta \\mid y \\sim \\text{Beta}(\\alpha + y, \\beta + n - y)$.\n\n:::\n\n## Happiness Data -- the first example of Bayesian inference procedure\n\nWe study Bayesian inference for a binomial proportion $\\theta$ when the sample size $n$ is fixed. In this example, we want to see what is the procedure of doing Bayesian inference\n\n\n::: {.callout-example title = \"Happiness Data\"}\n\nIn the 1998 General Social Survey, each female respondent aged 65 or over was asked whether she was generally happy.\n\nDefine the response variable\n$$\nY_i =\n\\begin{cases}\n1, & \\text{if respondent } i \\text{ reports being generally happy},\\\\\n0, & \\text{otherwise},\n\\end{cases}\n\\qquad i = 1,\\ldots,n,\n$$\nwhere $n = 129$.\n\nBecause we lack information that distinguishes individuals, it is reasonable to treat the responses as **exchangeable**.  \nThat is, before observing the data, the labels or ordering of respondents carry no information.\n\nSince the sample size $n$ is small relative to the population size $N$ of senior women, results from the previous chapter justify the following modeling approximation.\n\n**Modeling Assumptions**:  Our beliefs about $(Y_1,\\ldots,Y_{129})$ are described by:\n\n- **An unknown population proportion**\n  $$\n  \\theta = \\frac{1}{N}\\sum_{i=1}^N Y_i,\n  $$\n  where $\\theta$ represents the proportion of generally happy individuals in the population.\n\n- **A sampling model given $\\theta$**\n\n  Conditional on $\\theta$, the responses $Y_1,\\ldots,Y_{129}$ are independent and identically distributed Bernoulli random variables with\n  $$\n  \\Pr(Y_i = 1 \\mid \\theta) = \\theta.\n  $$\n\n> Given the population proportion $\\theta$, each respondent independently reports being happy with probability $\\theta$.\n\n**Likelihood**:  Under this model, the probability of observing data $\\{y_1,\\ldots,y_{129}\\}$ given $\\theta$ is\n$$\np(y_1,\\ldots,y_{129} \\mid \\theta)\n=\n\\theta^{\\sum_{i=1}^{129} y_i}\n(1-\\theta)^{129-\\sum_{i=1}^{129} y_i}.\n$$\n\nThis expression depends on the data only through the sufficient statistic\n$$\nS = \\sum_{i=1}^{129} Y_i,\n$$\nthe total number of respondents who report being generally happy.\n\nFor the happiness data,\n$$\nS = 118,\n$$\nso the likelihood simplifies to\n$$\np(y_1,\\ldots,y_{129} \\mid \\theta)\n=\n\\theta^{118}(1-\\theta)^{11}.\n$$\n\nQ: Which prior to be used? \n\n\nA prior distribution is **conjugate** to a likelihood if the posterior distribution belongs to the same family as the prior. For the binomial likelihood, the **Beta distribution** is conjugate. But we have another choice of prior, to use *non-informative prior*.\n\n**A Uniform Prior Distribution**:  Suppose our prior information about $\\theta$ is very weak, in the sense that all subintervals of $[0,1]$ with equal length are equally plausible.  \nSymbolically, for any $0 \\le a < b < b+c \\le 1$,\n$$\n\\Pr(a \\le \\theta \\le b)\n=\n\\Pr(a+c \\le \\theta \\le b+c).\n$$\n\nThis implies a **uniform prior**:\n$$\np(\\theta) = 1, \\qquad 0 \\le \\theta \\le 1.\n$$\n\n**Posterior Distribution**:  Bayes’ rule gives\n$$\np(\\theta \\mid y_1,\\ldots,y_{129})\n=\n\\frac{p(y_1,\\ldots,y_{129} \\mid \\theta)\\,p(\\theta)}\n     {p(y_1,\\ldots,y_{129})}.\n$$\n\nWith a uniform prior, this reduces to\n$$\np(\\theta \\mid y_1,\\ldots,y_{129})\n\\propto\n\\theta^{118}(1-\\theta)^{11}.\n$$\n\n> **Key idea:** with a uniform prior, the posterior has the **same shape** as the likelihood.\n\nTo obtain a proper probability distribution, we must normalize.\n\n**Normalizing Constant and the Beta Distribution**: Using the identity\n$$\n\\int_0^1 \\theta^{a-1}(1-\\theta)^{b-1}\\,d\\theta\n=\n\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)},\n$$\nwe find\n$$\np(y_1,\\ldots,y_{129})\n=\n\\frac{\\Gamma(119)\\Gamma(12)}{\\Gamma(131)}.\n$$\n\nTherefore, the posterior density is\n$$\np(\\theta \\mid y_1,\\ldots,y_{129})\n=\n\\frac{\\Gamma(131)}{\\Gamma(119)\\Gamma(12)}\n\\theta^{119-1}(1-\\theta)^{12-1}.\n$$\n\nThat is,\n$$\n\\theta \\mid y \\sim \\mathrm{Beta}(119,\\,12).\n$$\n\n\nRecall that, a random variable $\\theta \\sim \\mathrm{Beta}(a,b)$ distribution if\n$$\np(\\theta)\n=\n\\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\n\\theta^{a-1}(1-\\theta)^{b-1}.\n$$\n\nFor $\\theta \\sim \\mathrm{Beta}(a,b)$, the expectation (i.e., mean or the first moment) is $\\mathbb{E}(\\theta) = \\frac{a}{a+b}$, and the variance is $\\mathrm{Var}(\\theta)=\\frac{ab}{(a+b)^2(a+b+1)}.$\n\nIn our example, the happiness data, the posterior distribution is\n$$\n\\theta \\mid y \\sim \\mathrm{Beta}(119,12).\n$$\n\nThus, the posterior mean is $\\mathbb{E}(\\theta \\mid y) = 0.915$, and the posterior standard deviation is $\\mathrm{sd}(\\theta \\mid y) = 0.025$.\n\nThese summaries quantify both our **best estimate** of the population proportion and our **remaining uncertainty** after observing the data.\n\n:::\n\n### Inference about exchangeable binary data\n\n**Posterior Inference under a Uniform Prior**\n\nSuppose $Y_1, \\ldots, Y_n \\mid \\theta \\stackrel{\\text{i.i.d.}}{\\sim} \\text{Bernoulli}(\\theta)$, and we place a uniform prior on $\\theta$. The posterior distribution of $\\theta$ given the observed data\n$y_1, \\ldots, y_n$ is proportional to\n$$\n\\begin{aligned}\np(\\theta \\mid y_1, \\ldots, y_n) \n&= \\frac{p(y_1, \\ldots, y_n \\mid \\theta) p(\\theta)}{p(y_1, \\ldots, y_n)} \\\\\n&= \\theta^{\\sum_i y_i}(1 - \\theta)^{n - \\sum_i y_i} \\times \\frac{p(\\theta)}{p(y_1, \\ldots, y_n)}\\\\\n&\\propto\n\\theta^{\\sum_i y_i}(1 - \\theta)^{n - \\sum_i y_i}.\n\\end{aligned}\n$$\n\nConsider two parameter values $\\theta_a$ and $\\theta_b$. The ratio of their posterior densities is\n$$\n\\begin{aligned}\n\\frac{p(\\theta_a \\mid y_1, \\ldots, y_n)}\n     {p(\\theta_b \\mid y_1, \\ldots, y_n)}\n&=\\frac{\\theta_a^{\\sum y_i}\\left(1-\\theta_a\\right)^{n-\\sum y_i} \\times p\\left(\\theta_a\\right) / p\\left(y_1, \\ldots, y_n\\right)}{\\theta_b^{\\sum y_i}\\left(1-\\theta_b\\right)^{n-\\sum y_i} \\times p\\left(\\theta_b\\right) / p\\left(y_1, \\ldots, y_n\\right)} \\\\\n&=\n\\left(\\frac{\\theta_a}{\\theta_b}\\right)^{\\sum_i y_i}\n\\left(\\frac{1 - \\theta_a}{1 - \\theta_b}\\right)^{n - \\sum_i y_i}\n\\frac{p(\\theta_a)}{p(\\theta_b)}.\n\\end{aligned}\n$$\n\nThis expression shows that the data affect the posterior distribution\n**only through the sum of the data** $\\sum_{i=1}^n y_i$ based on the relative probability density at $\\theta_a$ to $\\theta_b$.\n\nAs a result, for any set $A$, one can show that\n$$\n\\Pr(\\theta \\in A \\mid Y_1 = y_1, \\ldots, Y_n = y_n)\n=\n\\Pr\\left(\\theta \\in A \\mid \\sum_{i=1}^n Y_i = \\sum_{i=1}^n y_i\\right).\n$$\n\nThis means that $\\sum_{i=1}^n Y_i$ contains **all the information** in the data relevant for inference about $\\theta$. We therefore say that $Y = \\sum_{i=1}^n Y_i$ is a **sufficient statistic** for $\\theta$.  The term *sufficient* is used because knowing $\\sum_{i=1}^n Y_i$\nis sufficient to carry out inference about $\\theta$; no additional information\nfrom the individual observations $Y_1, \\ldots, Y_n$ is required.\n\nIn the case where $Y_1, \\ldots, Y_n \\mid \\theta$ are i.i.d. Bernoulli$(\\theta)$ random variables, the sufficient statistic $Y = \\sum_{i=1}^n Y_i$ follows a **binomial distribution** with parameters $(n, \\theta)$.\n\n\n**The Binomial Model**\n\nBecause each $Y_i$ is Bernoulli$(\\theta)$ and the observations are independent, the sufficient statistic $Y = \\sum_{i=1}^n Y_i$\nfollows a **binomial distribution** with parameters $(n, \\theta)$.\n\nThat is, $\\Pr(Y = y \\mid \\theta)=\\binom{n}{y} \\theta^y (1 - \\theta)^{n - y}, \\qquad y = 0, 1, \\ldots, n.$ For a binomial$(n, \\theta)$ random variable $Y$,\n\n+ $\\mathbb{E}[Y \\mid \\theta] = n\\theta$, \n+ $\\mathrm{Var}(Y \\mid \\theta) = n\\theta(1 - \\theta).$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(cowplot)\n\n# helper: make facet labels parse as plotmath\nmake_theta_labels <- function(df) {\n  df %>%\n    mutate(theta_lab = paste0(\"theta==\", as.character(theta)))\n}\n\n# -------------------------\n# Figure for n = 10\n# -------------------------\nn_val <- 10\nthetas <- c(0.2, 0.8)\n\ndf10 <- expand_grid(theta = thetas, y = 0:n_val) %>%\n  mutate(p = dbinom(y, size = n_val, prob = theta)) %>%\n  make_theta_labels()\n\np10 <- ggplot(df10, aes(x = y, y = p)) +\n  geom_col(width = 0.7, fill = \"black\") +\n  facet_wrap(~ theta_lab, nrow = 1, labeller = label_parsed) +\n  scale_x_continuous(breaks = 0:n_val) +\n  labs(\n    x = \"y\",\n    y = expression(Pr(Y == y ~ \"|\" ~ theta ~ \",\" ~ n == 10))\n  ) +\n  theme_bw(base_size = 14) +\n  theme(\n    panel.grid = element_blank(),\n    strip.background = element_blank()\n  )\n\n# -------------------------\n# Figure for n = 100\n# -------------------------\nn_val <- 100\n\ndf100 <- expand_grid(theta = thetas, y = 0:n_val) %>%\n  mutate(p = dbinom(y, size = n_val, prob = theta)) %>%\n  make_theta_labels()\n\np100 <- ggplot(df100, aes(x = y, y = p)) +\n  geom_col(width = 0.9, fill = \"black\") +\n  facet_wrap(~ theta_lab, nrow = 1, labeller = label_parsed) +\n  labs(\n    x = \"y\",\n    y = expression(Pr(Y == y ~ \"|\" ~ theta ~ \",\" ~ n == 100))\n  ) +\n  theme_bw(base_size = 14) +\n  theme(\n    panel.grid = element_blank(),\n    strip.background = element_blank()\n  )\n\n# -------------------------\n# Combine using cowplot\n# -------------------------\nfinal_plot <- plot_grid(p10, p100, ncol = 1, align = \"v\")\nfinal_plot\n```\n\n::: {.cell-output-display}\n![](02_bi-1par_files/figure-html/binomal-1.png){width=672}\n:::\n:::\n\n\n---\n\nThis Chapter follows closely with Chapter 3 in Hoff (2009).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}