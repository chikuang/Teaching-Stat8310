{
  "hash": "9077c5b604d70b9a1709c697594503be",
  "result": {
    "engine": "knitr",
    "markdown": "# Bayesian Inference for single parameter models\n\n> Leading objectives:\n>\n> Understand how to perform Bayesian inference on a single parameter model.\n>\n> -   Binomial model with given n\n> -   Poission model\n> -   Exponential family\n\nRecall the important ingredients of Bayesian inference:\n\n1.  **Prior distribution:** $p(\\theta)$\n2.  **Likelihood function:** $p(y \\mid \\theta)$\n3.  **Posterior distribution:** $p(\\theta \\mid y) \\propto p(y \\mid \\theta) p(\\theta)$\n\n## Three basic ingredients of Bayesian inference\n\n### Prior\n\nThe prior distribution encodes our beliefs about the parameter $\\theta$ *before* conduct any experiments.\n\n\n::: {.callout-note title = \"Prior and Data are independent\"}\n\nNote that, the prior distribution is independent of the data. It represents our knowledge or beliefs about the parameter before seeing the data. \n\n:::\n\nHow do we choose a prior?\n\n1.  **Informative priors:** Based on previous studies or expert knowledge\n2.  **Weakly informative priors:** Provide some regularization without dominating the data\n3.  **Non-informative priors:** Attempt to be \"objective\" (e.g., uniform, Jeffreys prior)\n\n### Likelihood\n\nThe likelihood function represents the probability of observing the data given the parameter $\\theta$. It can be derived from the assumed statistical model for the data or experiment, i.e., $y \\sim p(y \\mid \\theta)$, or we can estimate this non-parametrically (i.e., without assuming the underlying distribution is the one we know.).\n\n\n::: {.callout-note title = \"Likelihood is NOT a probability distribution for $\\theta$\"}\n\nNote that, the likelihood function is not a probability distribution for $\\theta$ itself. It is a function of $\\theta$ for fixed data $y$. \n\n:::\n\n### Posterior\n\nThe posterior distribution combines the prior and likelihood to update our beliefs about $\\theta$ after observing the data. It is given by Bayes' theorem: $$\np(\\theta \\mid y) = \\frac{p(y \\mid \\theta) p(\\theta)}{p(y)},\n$$ where $p(y) = \\int p(y \\mid \\theta) p(\\theta) d\\theta$ is the marginal likelihood or evidence.\n\n### An simple example\n\n\n**Examples:**\n\n-   Beta prior + Binomial likelihood → Beta posterior\n-   Normal prior + Normal likelihood (known variance) → Normal posterior\n-   Gamma prior + Poisson likelihood → Gamma posterior\n\n**Advantages:** - Analytical posteriors (no numerical integration needed) - Interpretable parameters - Computationally efficient\n\n**Limitations:**\n\n-   May not reflect true prior beliefs\n-   Modern computing makes non-conjugate priors feasible\n\n::: {.callout-example title = \"Why Conjugate Priors?\"}\n\nLet's look a simple example to illustrate the convenience of conjugate priors. Consider a Binomial model with unknown success probability $\\theta$ and known number of trials $n$. We can use a Beta prior for $\\theta$.\n\nSuppose we have a Binomial model with known number of trials $n$ and unknown success probability $\\theta$. We can use a Beta prior for $\\theta$.\n\n-   **Prior:** $\\theta \\sim \\text{Beta}(\\alpha, \\beta)$\n-   **Likelihood:** $y \\mid \\theta \\sim \\text{Binomial}(n, \\theta)$\n\nThe derivation of the posterior is as follows:\n\n$$\n\\begin{aligned}\np(y \\mid \\theta) & = \\binom{n}{y} \\theta^y (1 - \\theta)^{n - y}, \\\\\np(\\theta) & = \\frac{\\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1}}{B(\\alpha, \\beta)},\n\\end{aligned}\n$$ where $B(\\alpha, \\beta)$ is the Beta function. Then the posterior is proportional to: $$\np(\\theta \\mid y) \\propto p(y \\mid \\theta) p(\\theta) \\propto \\theta^{y + \\alpha - 1} (1 -  \\theta)^{n - y + \\beta - 1}.\n$$ This is the kernel of a Beta distribution with parameters $(\\alpha + y, \\beta + n - y)$. Thus, the posterior distribution is: $$\n\\theta \\mid y \\sim \\text{Beta}(\\alpha + y, \\beta\n + n - y).\n$$\n\nThus, the **Posterior** is $\\theta \\mid y \\sim \\text{Beta}(\\alpha + y, \\beta + n - y)$.\n\n:::\n\n## Happiness Data -- the first example of Bayesian inference procedure\n\nWe study Bayesian inference for a binomial proportion $\\theta$ when the sample size $n$ is fixed. In this example, we want to see what is the procedure of doing Bayesian inference\n\n\n::: {.callout-example title = \"Happiness Data\"}\n\nIn the 1998 General Social Survey, each female respondent aged 65 or over was asked whether she was generally happy.\n\nDefine the response variable\n$$\nY_i =\n\\begin{cases}\n1, & \\text{if respondent } i \\text{ reports being generally happy},\\\\\n0, & \\text{otherwise},\n\\end{cases}\n\\qquad i = 1,\\ldots,n,\n$$\nwhere $n = 129$.\n\nBecause we lack information that distinguishes individuals, it is reasonable to treat the responses as **exchangeable**.  \nThat is, before observing the data, the labels or ordering of respondents carry no information.\n\nSince the sample size $n$ is small relative to the population size $N$ of senior women, results from the previous chapter justify the following modeling approximation.\n\n**Modeling Assumptions**:  Our beliefs about $(Y_1,\\ldots,Y_{129})$ are described by:\n\n- **An unknown population proportion**\n  $$\n  \\theta = \\frac{1}{N}\\sum_{i=1}^N Y_i,\n  $$\n  where $\\theta$ represents the proportion of generally happy individuals in the population.\n\n- **A sampling model given $\\theta$**\n\n  Conditional on $\\theta$, the responses $Y_1,\\ldots,Y_{129}$ are independent and identically distributed Bernoulli random variables with\n  $$\n  \\Pr(Y_i = 1 \\mid \\theta) = \\theta.\n  $$\n\n> Given the population proportion $\\theta$, each respondent independently reports being happy with probability $\\theta$.\n\n**Likelihood**:  Under this model, the probability of observing data $\\{y_1,\\ldots,y_{129}\\}$ given $\\theta$ is\n$$\np(y_1,\\ldots,y_{129} \\mid \\theta)\n=\n\\theta^{\\sum_{i=1}^{129} y_i}\n(1-\\theta)^{129-\\sum_{i=1}^{129} y_i}.\n$$\n\nThis expression depends on the data only through the sufficient statistic\n$$\nS = \\sum_{i=1}^{129} Y_i,\n$$\nthe total number of respondents who report being generally happy.\n\nFor the happiness data,\n$$\nS = 118,\n$$\nso the likelihood simplifies to\n$$\np(y_1,\\ldots,y_{129} \\mid \\theta)\n=\n\\theta^{118}(1-\\theta)^{11}.\n$$\n\nQ: Which prior to be used? \n\n\nA prior distribution is **conjugate** to a likelihood if the posterior distribution belongs to the same family as the prior. For the binomial likelihood, the **Beta distribution** is conjugate. But we have another choice of prior, to use *non-informative prior*.\n\n**A Uniform Prior Distribution**:  Suppose our prior information about $\\theta$ is very weak, in the sense that all subintervals of $[0,1]$ with equal length are equally plausible.  \nSymbolically, for any $0 \\le a < b < b+c \\le 1$,\n$$\n\\Pr(a \\le \\theta \\le b)\n=\n\\Pr(a+c \\le \\theta \\le b+c).\n$$\n\nThis implies a **uniform prior**:\n$$\np(\\theta) = 1, \\qquad 0 \\le \\theta \\le 1.\n$$\n\n**Posterior Distribution**:  Bayes’ rule gives\n$$\np(\\theta \\mid y_1,\\ldots,y_{129})\n=\n\\frac{p(y_1,\\ldots,y_{129} \\mid \\theta)\\,p(\\theta)}\n     {p(y_1,\\ldots,y_{129})}.\n$$\n\nWith a uniform prior, this reduces to\n$$\np(\\theta \\mid y_1,\\ldots,y_{129})\n\\propto\n\\theta^{118}(1-\\theta)^{11}.\n$$\n\n> **Key idea:** with a uniform prior, the posterior has the **same shape** as the likelihood.\n\nTo obtain a proper probability distribution, we must normalize.\n\n**Normalizing Constant and the Beta Distribution**: Using the identity\n$$\n\\int_0^1 \\theta^{a-1}(1-\\theta)^{b-1}\\,d\\theta\n=\n\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)},\n$$\nwe find\n$$\np(y_1,\\ldots,y_{129})\n=\n\\frac{\\Gamma(119)\\Gamma(12)}{\\Gamma(131)}.\n$$\n\nTherefore, the posterior density is\n$$\np(\\theta \\mid y_1,\\ldots,y_{129})\n=\n\\frac{\\Gamma(131)}{\\Gamma(119)\\Gamma(12)}\n\\theta^{119-1}(1-\\theta)^{12-1}.\n$$\n\nThat is,\n$$\n\\theta \\mid y \\sim \\mathrm{Beta}(119,\\,12).\n$$\n\n\nRecall that, a random variable $\\theta \\sim \\mathrm{Beta}(a,b)$ distribution if\n$$\np(\\theta)\n=\n\\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\n\\theta^{a-1}(1-\\theta)^{b-1}.\n$$\n\nFor $\\theta \\sim \\mathrm{Beta}(a,b)$, the expectation (i.e., mean or the first moment) is $\\mathbb{E}(\\theta) = \\frac{a}{a+b}$, and the variance is $\\mathrm{Var}(\\theta)=\\frac{ab}{(a+b)^2(a+b+1)}.$\n\nIn our example, the happiness data, the posterior distribution is\n$$\n\\theta \\mid y \\sim \\mathrm{Beta}(119,12).\n$$\n\nThus, the posterior mean is $\\mathbb{E}(\\theta \\mid y) = 0.915$, and the posterior standard deviation is $\\mathrm{sd}(\\theta \\mid y) = 0.025$.\n\nThese summaries quantify both our **best estimate** of the population proportion and our **remaining uncertainty** after observing the data.\n\n:::\n\n### Inference about exchangeable binary data\n\n**Posterior Inference under a Uniform Prior**\n\nSuppose $Y_1, \\ldots, Y_n \\mid \\theta \\stackrel{\\text{i.i.d.}}{\\sim} \\text{Bernoulli}(\\theta)$, and we place a uniform prior on $\\theta$. The posterior distribution of $\\theta$ given the observed data\n$y_1, \\ldots, y_n$ is proportional to\n$$\n\\begin{aligned}\np(\\theta \\mid y_1, \\ldots, y_n) \n&= \\frac{p(y_1, \\ldots, y_n \\mid \\theta) p(\\theta)}{p(y_1, \\ldots, y_n)} \\\\\n&= \\theta^{\\sum_i y_i}(1 - \\theta)^{n - \\sum_i y_i} \\times \\frac{p(\\theta)}{p(y_1, \\ldots, y_n)}\\\\\n&\\propto\n\\theta^{\\sum_i y_i}(1 - \\theta)^{n - \\sum_i y_i}.\n\\end{aligned}\n$$\n\nConsider two parameter values $\\theta_a$ and $\\theta_b$. The ratio of their posterior densities is\n$$\n\\begin{aligned}\n\\frac{p(\\theta_a \\mid y_1, \\ldots, y_n)}\n     {p(\\theta_b \\mid y_1, \\ldots, y_n)}\n&=\\frac{\\theta_a^{\\sum y_i}\\left(1-\\theta_a\\right)^{n-\\sum y_i} \\times p\\left(\\theta_a\\right) / p\\left(y_1, \\ldots, y_n\\right)}{\\theta_b^{\\sum y_i}\\left(1-\\theta_b\\right)^{n-\\sum y_i} \\times p\\left(\\theta_b\\right) / p\\left(y_1, \\ldots, y_n\\right)} \\\\\n&=\n\\left(\\frac{\\theta_a}{\\theta_b}\\right)^{\\sum_i y_i}\n\\left(\\frac{1 - \\theta_a}{1 - \\theta_b}\\right)^{n - \\sum_i y_i}\n\\frac{p(\\theta_a)}{p(\\theta_b)}.\n\\end{aligned}\n$$\n\nThis expression shows that the data affect the posterior distribution\n**only through the sum of the data** $\\sum_{i=1}^n y_i$ based on the relative probability density at $\\theta_a$ to $\\theta_b$.\n\nAs a result, for any set $A$, one can show that\n$$\n\\Pr(\\theta \\in A \\mid Y_1 = y_1, \\ldots, Y_n = y_n)\n=\n\\Pr\\left(\\theta \\in A \\mid \\sum_{i=1}^n Y_i = \\sum_{i=1}^n y_i\\right).\n$$\n\nThis means that $\\sum_{i=1}^n Y_i$ contains **all the information** in the data relevant for inference about $\\theta$. We therefore say that $Y = \\sum_{i=1}^n Y_i$ is a **sufficient statistic** for $\\theta$.  The term *sufficient* is used because knowing $\\sum_{i=1}^n Y_i$\nis sufficient to carry out inference about $\\theta$; no additional information\nfrom the individual observations $Y_1, \\ldots, Y_n$ is required.\n\nIn the case where $Y_1, \\ldots, Y_n \\mid \\theta$ are i.i.d. Bernoulli$(\\theta)$ random variables, the sufficient statistic $Y = \\sum_{i=1}^n Y_i$ follows a **binomial distribution** with parameters $(n, \\theta)$.\n\n\n**The Binomial Model**\n\nBecause each $Y_i$ is Bernoulli$(\\theta)$ and the observations are independent, the sufficient statistic $Y = \\sum_{i=1}^n Y_i$\nfollows a **binomial distribution** with parameters $(n, \\theta)$.\n\nThat is, $\\Pr(Y = y \\mid \\theta)=\\binom{n}{y} \\theta^y (1 - \\theta)^{n - y}$, $y = 0, 1, \\ldots, n$. For a binomial$(n, \\theta)$ random variable $Y$,\n\n+ $\\mathbb{E}[Y \\mid \\theta] = n\\theta$, \n+ $\\mathrm{Var}(Y \\mid \\theta) = n\\theta(1 - \\theta).$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_bi-1par_files/figure-html/binomal-1.png){width=672}\n:::\n:::\n\n\n**Posterior inference under a uniform prior distribution**\n\nHaving observed $Y = y$ our task is to obtain the posterior distribution of $\\theta$. By Bayes’ theorem,\n$$\np(\\theta \\mid y)\n= \\frac{p(y \\mid \\theta),p(\\theta)}{p(y)}.\n$$\n\nFor a binomial model with $Y \\sim \\text{Binomial}(n,\\theta)$, the likelihood is\n$$\np(y \\mid \\theta) = \\binom{n}{y}\\theta^y(1-\\theta)^{n-y}.\n$$\n\nTherefore,\n$$\np(\\theta \\mid y)\n= \\frac{\\binom{n}{y} \\theta^y(1-\\theta)^{n-y}p(\\theta)}{p(y)}\n= c(y) \\theta^y(1-\\theta)^{n-y}\\pi(\\theta),\n$$\nwhere $c(y)$ is a normalizing constant that depends only on $y$, not on $\\theta$. When using the uniform distribution, $\\pi(\\theta)$, we can calculate $c(y)$ easily as\n$$\n\\begin{aligned}\n1&=\\int_0^1 c(y) \\theta^y(1-\\theta)^{n-y} d \\theta \\\\\n&=c(y) \\int_0^1 \\theta^y(1-\\theta)^{n-y} d \\theta \\\\\n&=c(y) \\frac{\\Gamma(y+1) \\Gamma(n-y+1)}{\\Gamma(n+2)}\n\\end{aligned}.\n$$\nHence, $c(y)=\\Gamma(n+2)/\\{\\Gamma(y+1) \\Gamma(n-y+1)\\}$, and the posterior distribution is \n$$\n\\begin{aligned}\np(\\theta \\mid y) & =\\frac{\\Gamma(n+2)}{\\Gamma(y+1) \\Gamma(n-y+1)} \\theta^y(1-\\theta)^{n-y} \\\\\n& =\\frac{\\Gamma(n+2)}{\\Gamma(y+1) \\Gamma(n-y+1)} \\theta^{(y+1)-1}(1-\\theta)^{(n-y+1)-1},\n\\end{aligned}\n$$\nWhich is exactly the $\\operatorname{beta}(y+1, n-y+1)$. In the happiness example, we have $n=129$ and $Y=\\sum Y_i=118$, so the posterior distribution is $\\operatorname{beta}(119,12)$, written as \n$$\nn=129, Y \\equiv \\sum Y_i=118 \\quad \\Rightarrow \\quad \\theta \\mid\\{Y=118\\} \\sim \\operatorname{beta}(119,12) .\n$$\n\nThis confirms the sufficiency result for this model and prior distribution, by showing that if $\\sum y_i = y = 118$,  $p(\\theta\\mid  y_1,\\dots y_n) = p(\\theta\\mid y) = \\mathrm{beta}(119, 12)$.  That is, the information contained in $\\{Y_1 = y_1, \\dots, Y_n = y_n\\}$ is the same as the information contained in $\\{Y = y\\}$, where $Y = \\sum Y_i$ and $y = \\sum y_i$. This show the posterior when we use **uniform prior**. One may ask, what if we use a different prior?\n\n\n**Posterior distributions under beta prior distributions**\n\nThe uniform prior distribution has $\\pi(\\theta) = 1$ for all $\\theta\\in [0,1]$. This distribution can be thought of as a beta prior distribution with parameters $a = 1, b = 1$\n$$\np(\\theta)=\\frac{\\Gamma(2)}{\\Gamma(1) \\Gamma(1)} \\theta^{1-1}(1-\\theta)^{1-1}=\\frac{1}{1 \\times 1} 1 \\times 1=1\n$$\nfor all $\\theta \\in[0,1]$.\n\n::: {.callout-definition title = \"Gamma function\"}\n\nThe gamma function is defined as $$\n\\Gamma(x)=\\int_0^{\\infty} t^{x-1} e^{-t} d t, \\quad x>0.\n$$ \n\nIt satisfies the following properties:\n\n- $\\Gamma(n)=(n-1)!$ for any positive integer $n$.\n- $\\Gamma(x+1)=x \\Gamma(x)$ for any $x>0$.\n- $\\Gamma(1 / 2)=\\sqrt{\\pi}$.\n- $\\Gamma(1)=1$ by convention.\n  \n:::\n\nNow, from the previous part, recall that we have,\n$$\n\\text { if }\\left\\{\\begin{array}{c}\n\\theta \\sim \\operatorname{beta}(1,1) \\text { (uniform) } \\\\\nY \\sim \\operatorname{binomial}(n, \\theta)\n\\end{array}\\right\\}, \\text { then }\\{\\theta \\mid Y=y\\} \\sim \\operatorname{beta}(1+y, 1+n-y).\n$$\n\nTo get the posterior distribution under a general beta prior distribution, we just need to add the number of 1's to the $\\alpha$ parameter and the number of 0's to the $\\beta$ parameter. To see this, assume $\\theta\\sim \\operatorname{beta}(\\alpha, \\beta)$, and $Y\\mid \\theta \\sim \\operatorname{binomial}(n, \\theta)$. Then, once we observed $\\{Y=y\\}$, by Bayes' theorem, the posterior distribution is\n$$\n\\begin{aligned}\np(\\theta \\mid y) & =\\frac{p(\\theta) p(y \\mid \\theta)}{p(y)} \\\\\n& =\\frac{1}{p(y)} \\times \\frac{\\Gamma(a+b)}{\\Gamma(a) \\Gamma(b)} \\theta^{a-1}(1-\\theta)^{b-1} \\times\\binom{ n}{y} \\theta^y(1-\\theta)^{n-y} \\\\\n& =c(n, y, a, b) \\times \\theta^{a+y-1}(1-\\theta)^{b+n-y-1} \\\\\n&\\propto beta(a+y, b+n-y) .\n\\end{aligned}\n$$\n\n::: {.callout-note title = \"One-to-one correspondence between the distribution\"}\n\nNote that, there is a one-to-one correspondence between the prior distribution parameters and the posterior distribution parameters. Two distributions are said to be the same if\n\n+ Their CDFs are the same.\n+ Their PDFs are the same.\n+ All of their moments are the same. This implies that they are equal if and only if the moment generating function or the probability generating functions are the same.\n\n:::\n\nWe have seen the beta-binomial example twice, which is an example of **conjugate prior**, let's definite this formally,\n\n::: {.callout-definition title=\"Conjugate prior\"}\n\nA class $\\mathcal{P}$ of prior distribution for $\\theta$ is said **conjugate** for the likelihood function $p(y \\mid \\theta)$ if for every prior distribution $\\pi(\\theta) \\in \\mathcal{P}$, the corresponding posterior distribution $p(\\theta \\mid y)$ is also in $\\mathcal{P}$, that is \n$$\n\\pi(\\theta) \\in \\mathcal{P} \\Rightarrow p(\\theta \\mid y) \\in \\mathcal{P}.\n$$\n\n:::\n\n::: {.callout-note}\n\nConjugate priors simplify posterior calculations, but they may not accurately reflect genuine prior beliefs. Still, mixtures of conjugate priors offer substantially greater flexibility while remaining computationally tractable.\n\n:::\n\nIf the likelihood $\\theta \\mid \\{Y = y\\} \\sim beta(a + y, b + n − y)$, recall that \n$$\n\\mathrm{E}[\\theta \\mid y]=\\frac{a+y}{a+b+n}, \\operatorname{mode}[\\theta \\mid y]=\\frac{a+y-1}{a+b+n-2}, \\operatorname{Var}[\\theta \\mid y]=\\frac{\\mathrm{E}[\\theta \\mid y] \\mathrm{E}[1-\\theta \\mid y]}{a+b+n+1} .\n$$\n\nThe posterior mean can be expressed as a weighted average of the prior mean and the maximum likelihood estimate (MLE) of $\\theta$:\n$$\n\\begin{aligned}\n\\mathrm{E}[\\theta \\mid y] & =\\frac{a+y}{a+b+n} \\\\\n& =\\frac{a+b}{a+b+n} \\times\\frac{a}{a+b}+\\frac{n}{a+b+n}\\times \\frac{y}{n} \\\\\n& =\\frac{a+b}{a+b+n} \\times \\text { prior expectation }+\\frac{n}{a+b+n} \\times \\text { data mean }\n\\end{aligned}\n$$\nFor this model and prior distribution, the posterior expectation (also known as the posterior mean) can be expressed as a weighted average of the prior expectation and the sample mean. The weights are proportional to the prior sample size a + b and the observed sample size n, respectively. This representation leads to a natural interpretation of the Beta prior parameters as prior data:\n\n+ $a \\approx \\text{``prior \\# of 1’s,''}$\n+ $b \\approx \\text{``prior \\# of 0’s,''}$\n+ $a + b \\approx \\text{``prior sample size.''}$\n\n\nWhen $n \\gg a+b$, it is reasonable to expect that most of the information about $\\theta$ should come from the data rather than from the prior distribution. This intuition is confirmed mathematically. In particular, when $n \\gg a + b$,\n\n+ $\\frac{a + b}{a + b + n} \\approx 0$,\n+ $\\mathbb{E}[\\theta \\mid y] \\approx \\frac{y}{n}$,\n+ $\\mathrm{Var}(\\theta \\mid y) \\approx \\frac{1}{n}\\,\\frac{y}{n}\\left(1 - \\frac{y}{n}\\right)$.\n\nThus, in large samples, the posterior mean approaches the sample proportion and the posterior variance shrinks at rate $1/n$, reflecting increasing information from the data.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_bi-1par_files/figure-html/posterior-change-n-1.png){width=672}\n:::\n:::\n\n\n\n**Prediction**\n\nAn important feature of Bayesian inference is the existence of a **predictive distribution** for new observations.\n\n\n::: {.callout-definition title=\"Posterior Predictive Distribution\"}\n\nThe posterior predictive distribution for a new observation $Y_{\\text{new}}$ given the observed data $y$ is obtained by integrating over the posterior distribution of $\\theta$.\n\n:::\n\nReturning to our notation for binary data, let  \n$y_1, \\ldots, y_n$ be the observed outcomes from a sample of $n$ binary rvs, and let  \n$\\tilde Y \\in \\{0,1\\}$ denote a future observation from the same population that has not yet been observed. The **predictive distribution** of $\\tilde Y$ is defined as the conditional distribution of $\\tilde Y$ given the observed data $\\{Y_1=y_1,\\ldots,Y_n=y_n\\}$. For conditionally i.i.d. binary observations, the predictive distribution can be derived by integrating out the unknown parameter $\\theta$:\n$$\n\\begin{aligned}\n\\operatorname{Pr}\\left(\\tilde{Y}=1 \\mid y_1, \\ldots, y_n\\right) & =\\int \\operatorname{Pr}\\left(\\tilde{Y}=1, \\theta \\mid y_1, \\ldots, y_n\\right) d \\theta \\\\\n& =\\int \\operatorname{Pr}\\left(\\tilde{Y}=1 \\mid \\theta, y_1, \\ldots, y_n\\right) p\\left(\\theta \\mid y_1, \\ldots, y_n\\right) d \\theta \\\\\n& =\\int  p\\left(\\theta \\mid y_1, \\ldots, y_n\\right) \\theta d \\theta \\\\\n& =\\mathrm{E}\\left[\\theta \\mid y_1, \\ldots, y_n\\right]\\\\\n&=\\frac{a+\\sum_{i=1}^n y_i}{a+b+n}.\n\\end{aligned}\n$$\n\nHence, we also have,\n$$\n\\operatorname{Pr}\\left(\\tilde{Y}=0 \\mid y_1, \\ldots, y_n\\right)  =1-\\mathrm{E}\\left[\\theta \\mid y_1, \\ldots, y_n\\right]=\\frac{b+\\sum_{i=1}^n\\left(1-y_i\\right)}{a+b+n} .\n$$\n\n\n::: {.callout-note title =\"Properties of the predictive distribution\"}\n\n1. It  **does not depend on any unknown quantities.** If it did, it could not be used to make predictions.\n\n2. It **depends on the observed data.**  In particular, $\\tilde Y$ is not independent of $Y_1,\\ldots,Y_n$, because the observed data provide information about $\\theta$, which in turn influences $\\tilde Y$. If $\\tilde Y$ were independent of the observed data, learning from data would be impossible.\n\n:::\n\n\n::: {.callout-example}\n\nThe uniform prior distribution on [0,1], also known as the \\text{Beta}(1,1) prior, can be interpreted as containing the same information as a hypothetical prior dataset consisting of one success (“1”) and one failure (“0”).\n\nUnder this prior, the posterior predictive probability of a future success is\n$$\n\\Pr(\\tilde Y = 1 \\mid Y = y)\n= \\mathbb{E}[\\theta \\mid Y = y]\n= \\frac{2}{2+n}\\cdot\\frac{1}{2}\n\t•\t\\frac{n}{2+n}\\cdot\\frac{y}{n}.\n$$\n\nThis expression highlights that the predictive probability is a weighted average of:\n\t•\tthe prior mean 1/2, and\n\t•\tthe sample proportion y/n,\n\nwith weights proportional to the prior sample size 2 and the observed sample size n, respectively.\n\nThe posterior mode under this prior is\n$$\n\\text{mode}(\\theta \\mid Y = y) = \\frac{y}{n},\n$$\nwhere\n$$\nY = \\sum_{i=1}^n Y_i.\n$$\n\nAt first glance, the discrepancy between these two posterior summaries may seem surprising. However, it reflects the fact that different summaries capture different features of the posterior distribution.\n\nTo see this clearly, consider the case Y = 0. In this case,\n$$\n\\text{mode}(\\theta \\mid Y = 0) = 0,\n$$\nbut the predictive probability remains\n$$\n\\Pr(\\tilde Y = 1 \\mid Y = 0) = \\frac{1}{2+n}.\n$$\n\nThus, even when no successes have been observed, the Bayesian predictive distribution assigns a positive probability to a future success due to the prior information. This illustrates how Bayesian prediction naturally balances prior beliefs with observed data.\n\n:::\n\n### Confidence regions\n\n::: {.callout-definition title=\"Bayesian Coverage\"}\n\nAn interval $[\\ell(y), u(y)]$, based on the observed data $Y = y$, has 100(1-$\\alpha$)% Bayesian coverage for $\\theta$ if  \n$$\n\\Pr(\\ell(y) < \\theta < u(y)\\mid Y = y) = 1-\\alpha.\n$$\n\n:::\n\n::: {.callout-definition title = \"Frequentist Coverage\"}\n\nA random interval $[\\ell(Y ), u(Y )]$ has 100 (1-$\\alpha)% frequentist coverage for $\\theta$ if, before the data are gathered,  \n$$\n\\Pr(\\ell(Y ) < \\theta < u(Y )\\mid\\theta) = 1-\\alpha.\n$$\n\n:::\n\n## Poisson distribution\n\n\n---\n\nThis Chapter follows closely with Chapter 3 in Hoff (2009).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}