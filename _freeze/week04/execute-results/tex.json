{
  "hash": "ed3d98c0da4516e1520755431002fc7c",
  "result": {
    "engine": "knitr",
    "markdown": "# Week 4 — Markov Chain Monte Carlo (MCMC) Methods\n\n---\n\n## Overview\nThis week introduces **Markov Chain Monte Carlo (MCMC)** — a powerful class of algorithms for simulating from complex posterior distributions that are difficult to sample from directly.  \nYou will learn the logic of constructing Markov chains with a desired stationary distribution, how to implement the Metropolis–Hastings (MH) and Gibbs samplers, and how to assess convergence and mixing of MCMC chains.\n\n---\n\n## Learning Goals\n\nBy the end of Week 4, you should be able to:\n\n- Explain the intuition behind MCMC and why it works.  \n- Implement simple Metropolis–Hastings and Gibbs algorithms in R.  \n- Diagnose convergence using trace plots and summary statistics.  \n- Compute posterior means, variances, and credible intervals from MCMC samples.  \n- Understand practical issues such as burn-in, thinning, and autocorrelation.\n\n---\n\n## Lecture 1: Introduction to MCMC\n\n### 1.1 Motivation\nFor many posteriors, sampling directly is infeasible.  \nWe instead build a *Markov chain* whose stationary distribution is the target posterior $p(\\theta \\mid y)$.  \nAfter sufficient iterations, the draws from the chain behave like samples from the true posterior.\n\n### 1.2 Markov Chain Basics\nA Markov chain $\\{\\theta^{(t)}\\}$ has the **Markov property**:\n$$\np(\\theta^{(t)} \\mid \\theta^{(t-1)}, \\ldots, \\theta^{(1)}) = p(\\theta^{(t)} \\mid \\theta^{(t-1)}).\n$$\nIf the chain is **ergodic**, the distribution of $\\theta^{(t)}$ converges to a stationary distribution $\\pi(\\theta)$.\n\nMCMC constructs such chains so that $\\pi(\\theta) = p(\\theta \\mid y)$.\n\n### 1.3 Core Idea\nRepeatedly propose a new value $\\theta^\\*$ and decide whether to **accept** or **reject** it  \nbased on how likely it is under the posterior.  \nThis ensures that samples eventually represent the posterior distribution.\n\n---\n\n## Lecture 2: The Metropolis–Hastings Algorithm\n\n### 2.1 Algorithm Steps\n\n1. Initialize with $\\theta^{(0)}$.  \n2. For each iteration $t=1,2,\\ldots,T$:  \n   a. Propose $\\theta^\\* \\sim q(\\theta^\\* \\mid \\theta^{(t-1)})$.  \n   b. Compute the **acceptance probability**:\n      $$\n      \\alpha = \\min\\left(1,\\;\n      \\frac{p(y \\mid \\theta^\\*)\\, p(\\theta^\\*)\\, q(\\theta^{(t-1)} \\mid \\theta^\\*)}\n           {p(y \\mid \\theta^{(t-1)})\\, p(\\theta^{(t-1)})\\, q(\\theta^\\* \\mid \\theta^{(t-1)})}\n      \\right).\n      $$\n   c. Accept $\\theta^\\*$ with probability $\\alpha$; otherwise, keep $\\theta^{(t)} = \\theta^{(t-1)}$.\n\n3. After burn-in, the samples $\\{\\theta^{(t)}\\}$ approximate draws from $p(\\theta \\mid y)$.\n\n### 2.2 Special Case: Symmetric Proposal\nIf $q(\\theta^\\* \\mid \\theta^{(t-1)}) = q(\\theta^{(t-1)} \\mid \\theta^\\*)$,  \nthen\n$$\n\\alpha = \\min\\left(1,\\; \\frac{p(y \\mid \\theta^\\*)\\, p(\\theta^\\*)}\n                         {p(y \\mid \\theta^{(t-1)})\\, p(\\theta^{(t-1)})}\\right).\n$$\nThis is the **Metropolis algorithm**.\n\n### 2.3 Example: Posterior for a Normal Mean (Unknown Mean, Known Variance)\n\nLet $y_i \\sim N(\\mu, 1)$ for $i=1,\\ldots,n$ and prior $\\mu \\sim N(0,10^2)$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n# Data\ny <- rnorm(50, mean = 3, sd = 1)\nn <- length(y)\npost_log <- function(mu) {\n  sum(dnorm(y, mu, 1, log=TRUE)) + dnorm(mu, 0, 10, log=TRUE)\n}\n\n# Metropolis sampler\nT <- 10000\nmu <- numeric(T)\nmu[1] <- 0\nproposal_sd <- 0.5\n\nfor(t in 2:T) {\n  mu_star <- rnorm(1, mu[t-1], proposal_sd)\n  log_alpha <- post_log(mu_star) - post_log(mu[t-1])\n  if(log(runif(1)) < log_alpha) mu[t] <- mu_star else mu[t] <- mu[t-1]\n}\n\nburnin <- 1000\npost_samples <- mu[(burnin+1):T]\n\nhist(post_samples, prob=TRUE, col=\"skyblue\", main=\"Posterior Samples for μ\")\nabline(v = mean(post_samples), col=\"red\", lwd=2)\n```\n\n::: {.cell-output-display}\n![](week04_files/figure-pdf/unnamed-chunk-1-1.pdf){fig-pos='H'}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}