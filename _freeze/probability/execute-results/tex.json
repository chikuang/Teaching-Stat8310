{
  "hash": "9b5582dea6da8e2ef2bc4a584282cba8",
  "result": {
    "engine": "knitr",
    "markdown": "# Belife function and Probability Review\n\n\\newcommand{\\be}{\\mathrm{Be}}\n\n>Outline\n>\n>- Belief Functions\n>- Probability\n>- Bayes' Rule\n>- Random Variables\n>- Exchangeability\n\n## Belief functions\n\nProbability is a way to express rational beliefs.\n\n\n::: {.callout-definition title=\"Belief Functions\"}\nA **belief function** $\\be(\\cdot)$ is a function that assigns number to statements such that the large the number, the higher the degree of belief.\n:::\n\nBayesian inference is based on a simple principle: the **posterior distribution** (our updated beliefs after observing data) is obtained from the **prior distribution** (our initial beliefs) and the **sampling model** (how data are generated) via **Bayes' rule**:\n\n$$p(\\theta \\mid y)=\\frac{p(y \\mid \\theta) p(\\theta)}{\\int_{\\Theta} p(y \\mid \\theta') p(\\theta') d \\theta'}$$\n\nThis elegant formula is the foundation of all Bayesian inference. It tells us how to update our beliefs in light of new evidence.\n\n------------------------------------------------------------------------\n\n## Foundational Concepts\n\n### Why Use Bayesian Methods?\n\n**Probability as Uncertainty:** The Bayesian framework treats probability as a measure of uncertainty about unknown quantities, not just long-run frequencies. This allows direct probability statements about parameters given observed data.\n\n**Incorporates Prior Knowledge:** Bayesian methods naturally combine prior information (from expert judgment, previous studies, or domain knowledge) with observed data. This is particularly valuable in: - Medical research where historical trials exist - Engineering where physical constraints are known - Sequential analysis where data arrive over time\n\n**Direct Inference:** Bayesian inference answers the questions researchers actually ask: - \"What is the probability that treatment B is better than A given my data?\" - \"What is a plausible range for this parameter?\" - Rather than \"If the null hypothesis were true, what is the probability of observing this data?\"\n\n**Flexible Modeling:** Complex models with multiple parameters, hierarchical structures, or missing data are more naturally expressed in Bayesian frameworks.\n\n**Better Small-Sample Performance:** With limited data, informative priors can stabilize estimates and provide more stable inference than frequentist methods.\n\n------------------------------------------------------------------------\n\n## Bayesian vs. Frequentist Comparison\n\nBoth approaches have merits and limitations. The choice depends on the problem context, available prior information, and the questions being asked.\n\n### Motivating Examples\n\n#### Example 1.1: Inference for a proportion\n\nSuppose we are interested in estimating the rate at which a disease occurs in a population. We sample $n = 20$ individuals and observe $y = 8$ with the disease.\n\n**Questions:** - What is our estimate of the disease rate $\\theta$? - How certain are we about this estimate? - How would we predict the number with disease in a future sample?\n\n**Two approaches:**\n\n**Frequentist approach:** - Point estimate: $\\hat{\\theta} = y/n = 8/20 = 0.4$ - Confidence interval based on sampling distribution - Does not directly provide $P(\\theta \\in [a,b] \\mid \\text{data})$\n\n**Bayesian approach:** - Treat $\\theta$ as a random variable with a prior distribution - Update beliefs using data via Bayes' theorem - Obtain posterior distribution: direct probability statements about $\\theta$\n\n#### Example 1.2: Comparing two groups\n\nConsider two treatment groups with success rates $\\theta_A$ and $\\theta_B$.\n\n-   Group A: 8 successes out of 20 trials\n-   Group B: 12 successes out of 20 trials\n\n**Questions:** - Is treatment B better than treatment A? - What is $P(\\theta_B > \\theta_A \\mid \\text{data})$? - Bayesian methods provide direct answers to such questions.\n\n### Probability as a Measure of Uncertainty\n\n-   **Frequentist interpretation:** Probability as long-run frequency of repeated events.\n-   **Bayesian interpretation:** Probability as a degree of belief or uncertainty about unknown quantities.\n\nThe Bayesian view allows us to: - Make probability statements about parameters (not just data) - Incorporate prior information naturally - Update beliefs coherently as new data arrive\n\n### Building Blocks of Bayesian Inference\n\nFor parameter $\\theta$ and observed data $y$, we need three components:\n\n1.  **Prior distribution** $p(\\theta)$: expresses our beliefs about $\\theta$ before seeing data\n2.  **Likelihood** $p(y \\mid \\theta)$: probability model for the data given $\\theta$\\\n3.  **Posterior distribution** $p(\\theta \\mid y)$: updated beliefs after seeing data\n\n### Bayes' Theorem\n\nThese three components are combined via **Bayes' theorem:**\n\n$$\np(\\theta \\mid y) = \\frac{p(y \\mid \\theta)\\, p(\\theta)}{p(y)} = \n\\frac{p(y \\mid \\theta)\\, p(\\theta)}{\\int p(y \\mid \\theta)\\, p(\\theta)\\, d\\theta}\n$$\n\nIn words: $$\\text{Posterior} = \\frac{\\text{Likelihood} \\times \\text{Prior}}{\\text{Marginal likelihood}}$$\n\n**Key insight:** The posterior is proportional to the likelihood times the prior: $$\np(\\theta \\mid y) \\propto p(y \\mid \\theta) \\times p(\\theta)\n$$\n\nThe denominator $p(y) = \\int p(y \\mid \\theta)\\, p(\\theta)\\, d\\theta$ is a normalizing constant ensuring $\\int p(\\theta \\mid y)\\, d\\theta = 1$.\n\n### Inference from the Posterior Distribution\n\nOnce we obtain the posterior $p(\\theta \\mid y)$, we can:\n\n1.  **Point estimation:**\n    -   Posterior mean: $E[\\theta \\mid y]$\n    -   Posterior median or mode\n2.  **Interval estimation:**\n    -   Credible intervals: $P(a < \\theta < b \\mid y) = 0.95$\n    -   Direct probability statements about parameters\n3.  **Hypothesis testing:**\n    -   $P(\\theta_B > \\theta_A \\mid y)$\n4.  **Prediction:**\n    -   Posterior predictive distribution for future data $\\tilde{y}$: $$p(\\tilde{y} \\mid y) = \\int p(\\tilde{y} \\mid \\theta)\\, p(\\theta \\mid y)\\, d\\theta$$\n\n------------------------------------------------------------------------\n\n## One-Parameter Models\n\n### The Beta-Binomial Model\n\n#### Setup\n\nConsider binary outcome data: $y_1, \\ldots, y_n$ where each $y_i \\in \\{0, 1\\}$.\n\nLet $y = \\sum_{i=1}^n y_i$ be the number of successes. We model:\n\n$$y \\mid \\theta \\sim \\text{Binomial}(n, \\theta)$$\n\nwhere $\\theta$ is the probability of success.\n\n**Likelihood:** $$p(y \\mid \\theta) = \\binom{n}{y} \\theta^y (1-\\theta)^{n-y} \\propto \\theta^y (1-\\theta)^{n-y}$$\n\n#### Prior Distribution\n\nWe use a **Beta prior** for $\\theta$:\n\n$$\\theta \\sim \\text{Beta}(\\alpha, \\beta)$$\n\nwith density: $$p(\\theta) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}, \\quad 0 < \\theta < 1$$\n\n**Prior properties:** - $E[\\theta] = \\frac{\\alpha}{\\alpha + \\beta}$ - $\\text{Mode}[\\theta] = \\frac{\\alpha - 1}{\\alpha + \\beta - 2}$ (if $\\alpha, \\beta > 1$) - $\\text{Var}[\\theta] = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}$\n\n**Interpretation:** Think of $\\alpha$ as prior successes and $\\beta$ as prior failures.\n\n#### Posterior Distribution\n\nBy Bayes' theorem: \\begin{align}\np(\\theta \\mid y) &\\propto p(y \\mid \\theta) \\times p(\\theta) \\\\\n&\\propto \\theta^y (1-\\theta)^{n-y} \\times \\theta^{\\alpha-1}(1-\\theta)^{\\beta-1} \\\\\n&= \\theta^{y + \\alpha - 1}(1-\\theta)^{n - y + \\beta - 1}\n\\end{align}\n\nThis is the kernel of a $\\text{Beta}(\\alpha + y, \\beta + n - y)$ distribution.\n\n**Posterior:** $$\\theta \\mid y \\sim \\text{Beta}(\\alpha + y, \\beta + n - y)$$\n\n**Posterior mean:** $$E[\\theta \\mid y] = \\frac{\\alpha + y}{\\alpha + \\beta + n}$$\n\nThis is a weighted average of the prior mean $\\frac{\\alpha}{\\alpha+\\beta}$ and the sample proportion $\\frac{y}{n}$.\n\n#### Example 2.1: Disease Rate\n\nReturn to the disease rate example: $n = 20$, $y = 8$.\n\nSuppose we use a weakly informative prior: $\\theta \\sim \\text{Beta}(2, 2)$ (prior mean = 0.5).\n\n**Posterior:** $\\theta \\mid y \\sim \\text{Beta}(10, 14)$\n\n**Posterior mean:** $E[\\theta \\mid y] = \\frac{10}{24} = 0.417$\n\n**95% credible interval:** We can compute quantiles of $\\text{Beta}(10, 14)$ to get a 95% interval for $\\theta$.\n\n------------------------------------------------------------------------\n\n### The Normal Model with Known Variance\n\n#### Setup\n\nSuppose we observe data $y_1, \\ldots, y_n$ that are i.i.d. from:\n\n$$y_i \\mid \\theta \\sim \\mathcal{N}(\\theta, \\sigma^2)$$\n\nwhere $\\theta$ is the unknown mean and $\\sigma^2$ is a known variance.\n\n**Likelihood:** For the sample mean $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i$: $$\\bar{y} \\mid \\theta \\sim \\mathcal{N}\\left(\\theta, \\frac{\\sigma^2}{n}\\right)$$\n\nThe likelihood is: $$p(y_1, \\ldots, y_n \\mid \\theta) \\propto \\exp\\left\\{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i - \\theta)^2\\right\\}$$\n\n#### Prior Distribution\n\nWe use a **Normal prior** for $\\theta$:\n\n$$\\theta \\sim \\mathcal{N}(\\mu_0, \\tau_0^2)$$\n\nwith density: $$p(\\theta) \\propto \\exp\\left\\{-\\frac{1}{2\\tau_0^2}(\\theta - \\mu_0)^2\\right\\}$$\n\n#### Posterior Distribution\n\nBy Bayes' theorem: \\begin{align}\np(\\theta \\mid y) &\\propto p(y \\mid \\theta) \\times p(\\theta) \\\\\n&\\propto \\exp\\left\\{-\\frac{n}{2\\sigma^2}(\\bar{y} - \\theta)^2\\right\\} \\times \\exp\\left\\{-\\frac{1}{2\\tau_0^2}(\\theta - \\mu_0)^2\\right\\}\n\\end{align}\n\nAfter completing the square, we obtain:\n\n$$\\theta \\mid y \\sim \\mathcal{N}(\\mu_n, \\tau_n^2)$$\n\nwhere: $$\n\\tau_n^2 = \\left(\\frac{1}{\\tau_0^2} + \\frac{n}{\\sigma^2}\\right)^{-1} = \\frac{1}{\\text{prior precision} + \\text{data precision}}\n$$\n\n$$\n\\mu_n = \\tau_n^2\\left(\\frac{\\mu_0}{\\tau_0^2} + \\frac{n\\bar{y}}{\\sigma^2}\\right)\n$$\n\n**Alternative form:** $$\\mu_n = w \\mu_0 + (1-w)\\bar{y}$$\n\nwhere $w = \\frac{\\sigma^2/n}{\\sigma^2/n + \\tau_0^2}$ is the weight on the prior mean.\n\n#### Interpretation\n\n-   The posterior mean is a **weighted average** of the prior mean and the sample mean\n-   As $n \\to \\infty$, the posterior mean converges to $\\bar{y}$ (data dominate)\n-   With little data, the posterior is pulled toward the prior\n-   The posterior precision is the sum of the prior and data precisions\n\n------------------------------------------------------------------------\n\n### Posterior Predictive Distribution\n\nAfter observing data $y = (y_1, \\ldots, y_n)$, we often want to predict a future observation $\\tilde{y}$.\n\nThe **posterior predictive distribution** is:\n\n$$p(\\tilde{y} \\mid y) = \\int p(\\tilde{y} \\mid \\theta)\\, p(\\theta \\mid y)\\, d\\theta$$\n\nThis averages the conditional distribution of $\\tilde{y}$ given $\\theta$ over the posterior uncertainty in $\\theta$.\n\n#### Example: Beta-Binomial\n\nFor the beta-binomial model with $\\theta \\mid y \\sim \\text{Beta}(\\alpha + y, \\beta + n - y)$:\n\n$$P(\\tilde{y} = 1 \\mid y) = E[\\theta \\mid y] = \\frac{\\alpha + y}{\\alpha + \\beta + n}$$\n\n#### Example: Normal Model\n\nFor the normal model with known variance, if $\\theta \\mid y \\sim \\mathcal{N}(\\mu_n, \\tau_n^2)$ and $\\tilde{y} \\mid \\theta \\sim \\mathcal{N}(\\theta, \\sigma^2)$:\n\n$$\\tilde{y} \\mid y \\sim \\mathcal{N}(\\mu_n, \\tau_n^2 + \\sigma^2)$$\n\nThe predictive variance includes both parameter uncertainty ($\\tau_n^2$) and sampling variability ($\\sigma^2$).\n\n------------------------------------------------------------------------\n\n### Conjugate Priors\n\n**Definition:** A prior distribution is **conjugate** to a likelihood if the posterior distribution is in the same family as the prior.\n\n**Examples:** - Beta prior + Binomial likelihood → Beta posterior - Normal prior + Normal likelihood (known variance) → Normal posterior - Gamma prior + Poisson likelihood → Gamma posterior\n\n**Advantages:** - Analytical posteriors (no numerical integration needed) - Interpretable parameters - Computationally efficient\n\n**Limitations:** - May not reflect true prior beliefs - Modern computing makes non-conjugate priors feasible\n\n------------------------------------------------------------------------\n\n### Practical Considerations\n\n#### Prior Elicitation\n\nHow do we choose a prior?\n\n1.  **Informative priors:** Based on previous studies or expert knowledge\n2.  **Weakly informative priors:** Provide some regularization without dominating the data\n3.  **Non-informative priors:** Attempt to be \"objective\" (e.g., uniform, Jeffreys prior)\n\n#### Sensitivity Analysis\n\n-   Try different priors and check if conclusions change substantially\n-   If posterior is sensitive to prior choice with substantial data, investigate further\n\n#### Comparing Bayesian and Frequentist Inference\n\n| Aspect | Bayesian | Frequentist |\n|-------------------|-----------------------|------------------------------|\n| Parameters | Random variables with distributions | Fixed unknown constants |\n| Probability statements | Direct: $P(\\theta \\in [a,b] \\mid y)$ | Indirect: confidence intervals |\n| Prior information | Naturally incorporated | Difficult to include |\n| Small samples | Can be more stable | May have poor properties |\n| Interpretation | Conditional on observed data | Based on repeated sampling |\n\n------------------------------------------------------------------------\n\n### R Examples\n\n#### Example 3.1: Beta-Binomial Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data\nn <- 20\ny <- 8\n\n# Prior: Beta(2, 2)\nalpha0 <- 2\nbeta0 <- 2\n\n# Posterior: Beta(10, 14)\nalpha1 <- alpha0 + y\nbeta1 <- beta0 + n - y\n\n# Grid for plotting\ntheta <- seq(0, 1, length.out = 500)\n\n# Plot prior and posterior\nplot(theta, dbeta(theta, alpha0, beta0), type = \"l\", lwd = 2, col = \"blue\",\n     ylab = \"Density\", xlab = expression(theta),\n     main = \"Beta-Binomial Model: Prior and Posterior\")\nlines(theta, dbeta(theta, alpha1, beta1), col = \"red\", lwd = 2)\nabline(v = y/n, lty = 2, col = \"gray\")\n\nlegend(\"topright\",\n       legend = c(\"Prior Beta(2,2)\", \"Posterior Beta(10,14)\", \"MLE\"),\n       col = c(\"blue\", \"red\", \"gray\"), lwd = c(2, 2, 1), lty = c(1, 1, 2))\n```\n\n::: {.cell-output-display}\n![Prior, Likelihood, and Posterior for Beta-Binomial Model](probability_files/figure-pdf/beta-binomial-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n# Posterior summary\ncat(\"Posterior mean:\", alpha1/(alpha1 + beta1), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPosterior mean: 0.4166667 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"95% credible interval:\", qbeta(c(0.025, 0.975), alpha1, beta1), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n95% credible interval: 0.2319142 0.614581 \n```\n\n\n:::\n:::\n\n\n#### Example 3.2: Normal Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data\ny <- c(1.2, 0.8, 1.5, 1.1, 0.9)\nn <- length(y)\nybar <- mean(y)\nsigma2 <- 0.25  # known variance\n\n# Prior: N(0, 1)\nmu0 <- 0\ntau0_sq <- 1\n\n# Posterior\ntau_n_sq <- 1 / (1/tau0_sq + n/sigma2)\nmu_n <- tau_n_sq * (mu0/tau0_sq + n*ybar/sigma2)\n\ncat(\"Posterior: N(\", round(mu_n, 3), \",\", round(tau_n_sq, 3), \")\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPosterior: N( 1.048 , 0.048 )\n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot\ntheta <- seq(-1, 3, length.out = 500)\nplot(theta, dnorm(theta, mu0, sqrt(tau0_sq)), type = \"l\", lwd = 2, col = \"blue\",\n     ylab = \"Density\", xlab = expression(theta),\n     main = \"Normal Model: Prior and Posterior\")\nlines(theta, dnorm(theta, mu_n, sqrt(tau_n_sq)), col = \"red\", lwd = 2)\nabline(v = ybar, lty = 2, col = \"gray\")\n\nlegend(\"topright\",\n       legend = c(\"Prior\", \"Posterior\", \"Sample mean\"),\n       col = c(\"blue\", \"red\", \"gray\"), lwd = c(2, 2, 1), lty = c(1, 1, 2))\n```\n\n::: {.cell-output-display}\n![Prior, Likelihood, and Posterior for Normal Model](probability_files/figure-pdf/normal-model-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n--- \n\nThis Chapter follows closely with Chapter 2 in Hoff (2009).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}