{
  "hash": "2b30887efbd6ee4378eeb440fc6ed3a0",
  "result": {
    "engine": "knitr",
    "markdown": "# Week 11 — Bayesian Time Series and State-Space Models\n\nThis week introduces **Bayesian approaches to time series analysis** and **state-space modeling**, which unify filtering, forecasting, and dynamic parameter estimation under a probabilistic framework.\\\nWe study both classical dynamic linear models (DLMs) and modern Bayesian filtering methods.\n\n------------------------------------------------------------------------\n\n## Learning Goals\n\nBy the end of this week, you should be able to:\n\n-   Formulate Bayesian dynamic models for time series data.\\\n-   Understand latent (state) variable representations.\\\n-   Apply Bayesian updating and filtering for sequential data.\\\n-   Implement simple state-space and autoregressive models in R.\\\n-   Interpret uncertainty propagation over time.\n\n------------------------------------------------------------------------\n\n## Lecture 1 — Dynamic Linear Models (DLMs)\n\n### 1.1 Motivation\n\nTime series exhibit temporal dependence.\\\nDynamic models describe how latent states evolve over time and how observations depend on those states: $$\n\\text{State equation: } \\theta_t = G_t \\theta_{t-1} + \\omega_t, \\quad \\omega_t \\sim N(0,W_t),\n$$ $$\n\\text{Observation equation: } y_t = F_t^\\top \\theta_t + \\nu_t, \\quad \\nu_t \\sim N(0,V_t).\n$$\n\nHere,\\\n- $$ \\\\theta_t $$: latent state vector,\\\n- $$ y_t $$: observed data,\\\n- $$ G_t, F_t $$: known system matrices,\\\n- $$ W_t, V_t $$: process and observation noise covariances.\n\n------------------------------------------------------------------------\n\n### 1.2 Bayesian Updating\n\nGiven data up to time $$t-1$$, the **prior for** $$\\\\theta_t$$ is: $$\np(\\theta_t \\mid y_{1:t-1}) = N(a_t, R_t),\n$$ where\\\n$$ a_t = G_t m\\_{t-1} $$,\\\n$$ R_t = G_t C\\_{t-1} G_t\\^\\\\top + W_t $$.\n\nAfter observing $$ y_t $$: $$\np(\\theta_t \\mid y_{1:t}) = N(m_t, C_t),\n$$ where\\\n$$ m_t = a_t + A_t (y_t - F_t\\^\\\\top a_t) $$,\\\n$$ C_t = R_t - A_t F_t\\^\\\\top R_t $$,\\\nand $$ A_t = R_t F_t (F_t\\^\\\\top R_t F_t + V_t)\\^{-1} $$ is the **Kalman gain**.\n\n------------------------------------------------------------------------\n\n### 1.3 Example — Local Level Model\n\nSimplest DLM: $$\ny_t = \\theta_t + \\nu_t, \\quad \\theta_t = \\theta_{t-1} + \\omega_t,\n$$ with $$ \\\\nu_t \\\\sim N(0,V) $$, $$ \\\\omega_t \\\\sim N(0,W) $$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(11)\nn <- 100\ntheta <- numeric(n); y <- numeric(n)\ntheta[1] <- 0\nfor (t in 2:n) theta[t] <- theta[t-1] + rnorm(1, 0, 0.2)\ny <- theta + rnorm(n, 0, 0.5)\nplot.ts(cbind(y, theta), col=c(\"black\",\"blue\"), lwd=2,\n        main=\"Local Level Model: True State vs Observed y\", ylab=\"\")\nlegend(\"topleft\", legend=c(\"Observed y\",\"True θ\"), col=c(\"black\",\"blue\"), lwd=2, bty=\"n\")\n```\n\n::: {.cell-output-display}\n![](week11_files/figure-html/local-level-sim-1.png){width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n### 1.4 Filtering with the Kalman Algorithm\n\nWe estimate the evolving state mean $$ m_t $$ recursively:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- numeric(n); C <- numeric(n)\nm[1] <- 0; C[1] <- 1\nV <- 0.5^2; W <- 0.2^2\n\nfor (t in 2:n) {\n  a <- m[t-1]\n  R <- C[t-1] + W\n  A <- R / (R + V)\n  m[t] <- a + A * (y[t] - a)\n  C[t] <- (1 - A) * R\n}\n\nplot.ts(cbind(y, m), col=c(\"black\",\"red\"), lwd=2,\n        main=\"Kalman Filter Estimate of Latent State\", ylab=\"\")\nlegend(\"topleft\", legend=c(\"Observed y\",\"Filtered mean m_t\"),\n       col=c(\"black\",\"red\"), lwd=2, bty=\"n\")\n```\n\n::: {.cell-output-display}\n![](week11_files/figure-html/kalman-filter-1.png){width=672}\n:::\n:::\n\n\nThe red line tracks the smoothed latent process inferred from noisy data.\n\n------------------------------------------------------------------------\n\n### 1.5 Forecasting and Uncertainty\n\nPredictive distribution for the next observation: $$\ny_{t+1} \\mid y_{1:t} \\sim N(F_{t+1}^\\top a_{t+1}, F_{t+1}^\\top R_{t+1} F_{t+1} + V_{t+1}).\n$$\n\nForecast variance increases as the state uncertainty grows over time.\n\n------------------------------------------------------------------------\n\n### 1.6 Advantages of Bayesian DLMs\n\n-   Naturally handle missing observations.\\\n-   Flexible hierarchical extensions (time-varying parameters).\\\n-   Probabilistic forecasting with credible intervals.\\\n-   Online updating suitable for real-time applications.\n\n------------------------------------------------------------------------\n\n## Lecture 2 — State-Space Models and Bayesian Filtering\n\n### 2.1 General State-Space Models\n\nGeneral form: $$\nx_t = f(x_{t-1}) + \\omega_t, \\qquad y_t = g(x_t) + \\nu_t,\n$$ where $$ f $$ and $$ g $$ may be nonlinear or non-Gaussian.\\\nExamples: stochastic volatility, epidemic dynamics, tracking models.\n\n------------------------------------------------------------------------\n\n### 2.2 Bayesian Filtering\n\nWe recursively compute: $$\np(x_t \\mid y_{1:t}) \\propto p(y_t \\mid x_t)\\int p(x_t \\mid x_{t-1})\\,p(x_{t-1}\\mid y_{1:t-1})\\,dx_{t-1}.\n$$\n\nClosed forms exist only for linear-Gaussian models (Kalman).\\\nOtherwise, approximate methods are required: - **Particle Filtering** (Sequential Monte Carlo).\\\n- **Extended Kalman Filter** (linearization).\\\n- **Unscented Kalman Filter** (deterministic sampling).\n\n------------------------------------------------------------------------\n\n### 2.3 Particle Filtering (Sequential Monte Carlo)\n\nIdea: Represent $$ p(x_t\\\\mid y\\_{1:t}) $$ by weighted particles $$ \\\\{x_t\\^{(i)}, w_t\\^{(i)}\\\\} $$.\n\n1.  **Prediction:** Sample $$ x_t\\^{(i)} \\\\sim p(x_t\\\\mid x\\_{t-1}\\^{(i)}) $$$.\\\n2.  **Weighting:** $$ w_t\\^{(i)} \\\\propto p(y_t\\\\mid x_t\\^{(i)}) $$.\\\n3.  **Resampling:** Normalize and resample particles based on $$ w_t\\^{(i)} $$.\n\nAs $$ N \\\\to \\\\infty $$, the empirical distribution approximates the true posterior.\n\n------------------------------------------------------------------------\n\n### 2.4 Example — Particle Filter for a Simple Nonlinear Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12)\nn <- 50; Np <- 500\nx_true <- numeric(n); y <- numeric(n)\nx_true[1] <- 0\nfor (t in 2:n) x_true[t] <- 0.7*x_true[t-1] + rnorm(1,0,0.5)\ny <- x_true^2/2 + rnorm(n,0,0.2)\n\n# Initialize particles\nx_pf <- matrix(0, nrow=n, ncol=Np)\nw <- matrix(1/Np, nrow=n, ncol=Np)\nx_pf[1,] <- rnorm(Np, 0, 1)\n\nfor (t in 2:n) {\n  # Propagate\n  x_pf[t,] <- 0.7*x_pf[t-1,] + rnorm(Np,0,0.5)\n  # Weight\n  w[t,] <- dnorm(y[t], mean=x_pf[t,]^2/2, sd=0.2)\n  w[t,] <- w[t,]/sum(w[t,])\n  # Resample\n  idx <- sample(1:Np, Np, replace=TRUE, prob=w[t,])\n  x_pf[t,] <- x_pf[t,idx]\n}\n\nx_est <- colMeans(x_pf)\nplot.ts(cbind(y, x_true, x_est), col=c(\"black\",\"blue\",\"red\"), lwd=2,\n        main=\"Particle Filter Tracking\", ylab=\"\")\nlegend(\"topleft\", legend=c(\"Observed y\",\"True state\",\"PF estimate\"),\n       col=c(\"black\",\"blue\",\"red\"), lwd=2, bty=\"n\")\n```\n\n::: {.cell-output-display}\n![](week11_files/figure-html/particle-filter-1.png){width=672}\n:::\n:::\n\n\nThe particle filter captures nonlinear state dynamics unavailable to standard Kalman filtering.\n\n------------------------------------------------------------------------\n\n### 2.5 Extensions and Modern Bayesian State Models\n\n-   **Dynamic GLMs** for count or binary data.\\\n-   **Stochastic Volatility Models** in finance.\\\n-   **Dynamic Factor Models** for multivariate time series.\\\n-   **Bayesian Structural Time Series (BSTS)** — trend + seasonality decomposition.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(bsts)\ndata(Seatbelts)\ny <- Seatbelts[,\"drivers\"]\nss <- AddLocalLevel(list(), y)\nbsts_fit <- bsts(y ~ 1, state.specification = ss, niter = 2000)\nplot(bsts_fit)\n```\n:::\n\n\n------------------------------------------------------------------------\n\n### 2.6 Practical Considerations\n\n-   Choose priors for $$ V_t, W_t $$ that balance smoothness and responsiveness.\\\n-   Use HMC or SMC samplers for full Bayesian inference.\\\n-   Check convergence and predictive calibration through one-step-ahead forecasts.\n\n------------------------------------------------------------------------\n\n## Homework 11\n\n1.  **Conceptual**\n    -   Explain the difference between filtering and smoothing in Bayesian time series analysis.\\\n    -   When does the Kalman filter provide exact inference?\n2.  **Computational**\n    -   Simulate a local-level model and implement a Kalman filter in R.\\\n    -   Extend it with time-varying variance or drift.\\\n    -   Compare filtered estimates to true latent states.\n3.  **Reflection**\n    -   How does particle filtering generalize the Kalman filter?\\\n    -   Discuss a potential application of Bayesian state-space modeling in your field.\n\n------------------------------------------------------------------------\n\n## Key Takeaways\n\n| Concept | Summary |\n|------------------------------------|------------------------------------|\n| **Dynamic Linear Model (DLM)** | Linear-Gaussian state-space model solved by Kalman filter. |\n| **Kalman Filter** | Sequential Bayesian updating for latent states. |\n| **Particle Filter** | Simulation-based approach for nonlinear or non-Gaussian models. |\n| **Forecasting** | Naturally derived from predictive posterior distribution. |\n| **Extensions** | BSTS, stochastic volatility, dynamic regression, multivariate models. |\n\n------------------------------------------------------------------------\n\n**Next Week:** Hierarchical Bayesian Inference for Complex Systems — multi-level time series and dynamic hierarchical modeling.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}