{
  "hash": "80e82ab6cd75f97beb1b77a9128cf6a5",
  "result": {
    "engine": "knitr",
    "markdown": "# Week 11 — Bayesian Time Series and State-Space Models\n\nThis week introduces **Bayesian approaches to time series analysis** and **state-space modeling**, which unify filtering, forecasting, and dynamic parameter estimation under a probabilistic framework.  \nWe study both classical dynamic linear models (DLMs) and modern Bayesian filtering methods.\n\n---\n\n## Learning Goals\n\nBy the end of this week, you should be able to:\n\n- Formulate Bayesian dynamic models for time series data.  \n- Understand latent (state) variable representations.  \n- Apply Bayesian updating and filtering for sequential data.  \n- Implement simple state-space and autoregressive models in R.  \n- Interpret uncertainty propagation over time.\n\n---\n\n## Lecture 1 — Dynamic Linear Models (DLMs)\n\n### 1.1 Motivation\n\nTime series exhibit temporal dependence.  \nDynamic models describe how latent states evolve over time and how observations depend on those states:\n$$\n\\text{State equation: } \\theta_t = G_t \\theta_{t-1} + \\omega_t, \\quad \\omega_t \\sim N(0,W_t),\n$$\n$$\n\\text{Observation equation: } y_t = F_t^\\top \\theta_t + \\nu_t, \\quad \\nu_t \\sim N(0,V_t).\n$$\n\nHere,  \n- $begin:math:text$ \\\\theta_t $end:math:text$: latent state vector,  \n- $begin:math:text$ y_t $end:math:text$: observed data,  \n- $begin:math:text$ G_t, F_t $end:math:text$: known system matrices,  \n- $begin:math:text$ W_t, V_t $end:math:text$: process and observation noise covariances.\n\n---\n\n### 1.2 Bayesian Updating\n\nGiven data up to time $begin:math:text$t-1$end:math:text$, the **prior for $begin:math:text$\\\\theta_t$end:math:text$** is:\n$$\np(\\theta_t \\mid y_{1:t-1}) = N(a_t, R_t),\n$$\nwhere  \n$begin:math:text$ a_t = G_t m_{t-1} $end:math:text$,  \n$begin:math:text$ R_t = G_t C_{t-1} G_t^\\\\top + W_t $end:math:text$.\n\nAfter observing $begin:math:text$ y_t $end:math:text$:\n$$\np(\\theta_t \\mid y_{1:t}) = N(m_t, C_t),\n$$\nwhere  \n$begin:math:text$ m_t = a_t + A_t (y_t - F_t^\\\\top a_t) $end:math:text$,  \n$begin:math:text$ C_t = R_t - A_t F_t^\\\\top R_t $end:math:text$,  \nand $begin:math:text$ A_t = R_t F_t (F_t^\\\\top R_t F_t + V_t)^{-1} $end:math:text$ is the **Kalman gain**.\n\n---\n\n### 1.3 Example — Local Level Model\n\nSimplest DLM:\n$$\ny_t = \\theta_t + \\nu_t, \\quad \\theta_t = \\theta_{t-1} + \\omega_t,\n$$\nwith $begin:math:text$ \\\\nu_t \\\\sim N(0,V) $end:math:text$, $begin:math:text$ \\\\omega_t \\\\sim N(0,W) $end:math:text$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(11)\nn <- 100\ntheta <- numeric(n); y <- numeric(n)\ntheta[1] <- 0\nfor (t in 2:n) theta[t] <- theta[t-1] + rnorm(1, 0, 0.2)\ny <- theta + rnorm(n, 0, 0.5)\nplot.ts(cbind(y, theta), col=c(\"black\",\"blue\"), lwd=2,\n        main=\"Local Level Model: True State vs Observed y\", ylab=\"\")\nlegend(\"topleft\", legend=c(\"Observed y\",\"True θ\"), col=c(\"black\",\"blue\"), lwd=2, bty=\"n\")\n```\n\n::: {.cell-output-display}\n![](week11_files/figure-html/local-level-sim-1.png){width=672}\n:::\n:::\n\n\n---\n\n### 1.4 Filtering with the Kalman Algorithm\n\nWe estimate the evolving state mean $begin:math:text$ m_t $end:math:text$ recursively:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- numeric(n); C <- numeric(n)\nm[1] <- 0; C[1] <- 1\nV <- 0.5^2; W <- 0.2^2\n\nfor (t in 2:n) {\n  a <- m[t-1]\n  R <- C[t-1] + W\n  A <- R / (R + V)\n  m[t] <- a + A * (y[t] - a)\n  C[t] <- (1 - A) * R\n}\n\nplot.ts(cbind(y, m), col=c(\"black\",\"red\"), lwd=2,\n        main=\"Kalman Filter Estimate of Latent State\", ylab=\"\")\nlegend(\"topleft\", legend=c(\"Observed y\",\"Filtered mean m_t\"),\n       col=c(\"black\",\"red\"), lwd=2, bty=\"n\")\n```\n\n::: {.cell-output-display}\n![](week11_files/figure-html/kalman-filter-1.png){width=672}\n:::\n:::\n\n\nThe red line tracks the smoothed latent process inferred from noisy data.\n\n---\n\n### 1.5 Forecasting and Uncertainty\n\nPredictive distribution for the next observation:\n$$\ny_{t+1} \\mid y_{1:t} \\sim N(F_{t+1}^\\top a_{t+1}, F_{t+1}^\\top R_{t+1} F_{t+1} + V_{t+1}).\n$$\n\nForecast variance increases as the state uncertainty grows over time.\n\n---\n\n### 1.6 Advantages of Bayesian DLMs\n\n- Naturally handle missing observations.  \n- Flexible hierarchical extensions (time-varying parameters).  \n- Probabilistic forecasting with credible intervals.  \n- Online updating suitable for real-time applications.\n\n---\n\n## Lecture 2 — State-Space Models and Bayesian Filtering\n\n### 2.1 General State-Space Models\n\nGeneral form:\n$$\nx_t = f(x_{t-1}) + \\omega_t, \\qquad y_t = g(x_t) + \\nu_t,\n$$\nwhere $begin:math:text$ f $end:math:text$ and $begin:math:text$ g $end:math:text$ may be nonlinear or non-Gaussian.  \nExamples: stochastic volatility, epidemic dynamics, tracking models.\n\n---\n\n### 2.2 Bayesian Filtering\n\nWe recursively compute:\n$$\np(x_t \\mid y_{1:t}) \\propto p(y_t \\mid x_t)\\int p(x_t \\mid x_{t-1})\\,p(x_{t-1}\\mid y_{1:t-1})\\,dx_{t-1}.\n$$\n\nClosed forms exist only for linear-Gaussian models (Kalman).  \nOtherwise, approximate methods are required:\n- **Particle Filtering** (Sequential Monte Carlo).  \n- **Extended Kalman Filter** (linearization).  \n- **Unscented Kalman Filter** (deterministic sampling).\n\n---\n\n### 2.3 Particle Filtering (Sequential Monte Carlo)\n\nIdea: Represent $begin:math:text$ p(x_t\\\\mid y_{1:t}) $end:math:text$ by weighted particles $begin:math:text$ \\\\{x_t^{(i)}, w_t^{(i)}\\\\} $end:math:text$.\n\n1. **Prediction:** Sample $begin:math:text$ x_t^{(i)} \\\\sim p(x_t\\\\mid x_{t-1}^{(i)}) $end:math:text$.  \n2. **Weighting:** $begin:math:text$ w_t^{(i)} \\\\propto p(y_t\\\\mid x_t^{(i)}) $end:math:text$.  \n3. **Resampling:** Normalize and resample particles based on $begin:math:text$ w_t^{(i)} $end:math:text$.\n\nAs $begin:math:text$ N \\\\to \\\\infty $end:math:text$, the empirical distribution approximates the true posterior.\n\n---\n\n### 2.4 Example — Particle Filter for a Simple Nonlinear Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12)\nn <- 50; Np <- 500\nx_true <- numeric(n); y <- numeric(n)\nx_true[1] <- 0\nfor (t in 2:n) x_true[t] <- 0.7*x_true[t-1] + rnorm(1,0,0.5)\ny <- x_true^2/2 + rnorm(n,0,0.2)\n\n# Initialize particles\nx_pf <- matrix(0, nrow=n, ncol=Np)\nw <- matrix(1/Np, nrow=n, ncol=Np)\nx_pf[1,] <- rnorm(Np, 0, 1)\n\nfor (t in 2:n) {\n  # Propagate\n  x_pf[t,] <- 0.7*x_pf[t-1,] + rnorm(Np,0,0.5)\n  # Weight\n  w[t,] <- dnorm(y[t], mean=x_pf[t,]^2/2, sd=0.2)\n  w[t,] <- w[t,]/sum(w[t,])\n  # Resample\n  idx <- sample(1:Np, Np, replace=TRUE, prob=w[t,])\n  x_pf[t,] <- x_pf[t,idx]\n}\n\nx_est <- colMeans(x_pf)\nplot.ts(cbind(y, x_true, x_est), col=c(\"black\",\"blue\",\"red\"), lwd=2,\n        main=\"Particle Filter Tracking\", ylab=\"\")\nlegend(\"topleft\", legend=c(\"Observed y\",\"True state\",\"PF estimate\"),\n       col=c(\"black\",\"blue\",\"red\"), lwd=2, bty=\"n\")\n```\n\n::: {.cell-output-display}\n![](week11_files/figure-html/particle-filter-1.png){width=672}\n:::\n:::\n\n\nThe particle filter captures nonlinear state dynamics unavailable to standard Kalman filtering.\n\n---\n\n### 2.5 Extensions and Modern Bayesian State Models\n\n- **Dynamic GLMs** for count or binary data.  \n- **Stochastic Volatility Models** in finance.  \n- **Dynamic Factor Models** for multivariate time series.  \n- **Bayesian Structural Time Series (BSTS)** — trend + seasonality decomposition.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(bsts)\ndata(Seatbelts)\ny <- Seatbelts[,\"drivers\"]\nss <- AddLocalLevel(list(), y)\nbsts_fit <- bsts(y ~ 1, state.specification = ss, niter = 2000)\nplot(bsts_fit)\n```\n:::\n\n\n---\n\n### 2.6 Practical Considerations\n\n- Choose priors for $begin:math:text$ V_t, W_t $end:math:text$ that balance smoothness and responsiveness.  \n- Use HMC or SMC samplers for full Bayesian inference.  \n- Check convergence and predictive calibration through one-step-ahead forecasts.\n\n---\n\n## Homework 11\n\n1. **Conceptual**  \n   - Explain the difference between filtering and smoothing in Bayesian time series analysis.  \n   - When does the Kalman filter provide exact inference?\n\n2. **Computational**  \n   - Simulate a local-level model and implement a Kalman filter in R.  \n   - Extend it with time-varying variance or drift.  \n   - Compare filtered estimates to true latent states.\n\n3. **Reflection**  \n   - How does particle filtering generalize the Kalman filter?  \n   - Discuss a potential application of Bayesian state-space modeling in your field.\n\n---\n\n## Key Takeaways\n\n| Concept | Summary |\n|----------|----------|\n| **Dynamic Linear Model (DLM)** | Linear-Gaussian state-space model solved by Kalman filter. |\n| **Kalman Filter** | Sequential Bayesian updating for latent states. |\n| **Particle Filter** | Simulation-based approach for nonlinear or non-Gaussian models. |\n| **Forecasting** | Naturally derived from predictive posterior distribution. |\n| **Extensions** | BSTS, stochastic volatility, dynamic regression, multivariate models. |\n\n---\n\n**Next Week:** Hierarchical Bayesian Inference for Complex Systems — multi-level time series and dynamic hierarchical modeling.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}