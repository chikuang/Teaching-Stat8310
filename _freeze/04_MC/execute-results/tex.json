{
  "hash": "e0ee012adc89d21f9a03c2f7f1b68486",
  "result": {
    "engine": "knitr",
    "markdown": "# Monte Carlo Method and its variations\n\n> Leading objectives:\n>\n> -   understand how Monte Carlo (MC) and Markov chain Monte Carlo (MCMC) methods differ\n> -   implement a Metropolis–Hastings algorithm to draw samples from the posterior\n> -   use output of MCMC to obtain estimates and standard errors\n> -   use efficient proposals and tuning for MCMC\n\n## Background and Motivation\n\nWhat we have seen in the last chapter up to now, is to use the `conjugate prior` to obtain closed form expressions for the posterior distribution. However, in many cases, conjugate priors are not available or not desirable. In such cases, we need to resort to numerical methods to approximate the posterior distribution.\n\n> **Question.**\\\n> How can we perform Bayesian inference when conjugate priors are not available and the posterior has no closed-form expression?\n\nThere are two broad classes of approaches:\n\n-   **Simulation-based methods**:\\\n    accept–reject sampling, Markov chain Monte Carlo (MCMC), particle filters, and related algorithms.\n\n-   **Deterministic approximation methods**:\\\n    Laplace approximations (including INLA), variational Bayes, expectation propagation, and related techniques.\n\nWe will be focusing on the Monte Carlo (MC) methods and its variation.\n\n## Overview\n\n### Monte Carlo (MC)\n\nMonte Carlo methods approximate expectations or probabilities using **random sampling**.\\\nIf samples can be drawn **directly** from the target distribution, Monte Carlo methods provide simple and effective estimators.\n\nTypical use: - Numerical integration - Bootstrap methods - Simulation-based probability estimation\n\n### Markov Chain Monte Carlo (MCMC)\n\nMCMC methods are used when **direct sampling is infeasible**.\\\nThey construct a **Markov chain** whose stationary distribution is the target distribution, and use dependent samples from the chain after burn-in.\n\nTypical use: - Bayesian posterior sampling - High-dimensional or unnormalized distributions\n\n### Gibbs Sampler\n\nThe Gibbs sampler is a **special case of MCMC** that samples sequentially from **full conditional distributions**.\\\nBecause proposals are drawn exactly from conditionals, all updates are automatically accepted.\n\nTypical use: - Bayesian hierarchical models - Models with conjugate full conditionals\n\n### Relationship Between Monte Carlo, MCMC, and Gibbs Sampling\n\nThese three concepts are **not competing methods**, but rather form a **nested hierarchy** of ideas used for approximating expectations and probability distributions using randomness.\n\n![](fig/MC/mc_mcmc_gs.png)\n\n### Summary Table\n\n| Method | Independent Samples | Uses Markov Chain | Accept–Reject Step | Typical Application |\n|---------------|---------------|---------------|---------------|---------------|\n| Monte Carlo (MC) | Yes | No | No | Direct simulation, integration |\n| MCMC | No | Yes | Usually | Bayesian posterior sampling |\n| Gibbs Sampler | No | Yes | No (always accept) | Bayesian models with tractable conditionals |\n\n::: {.callout-note title=\"Key summary\"}\n-   **Monte Carlo** is the general idea of using randomness for approximation.\n-   **MCMC** is Monte Carlo with dependent samples generated by a Markov chain.\n-   **Gibbs sampling** is a specific MCMC algorithm based on full conditional distributions.\n:::\n\n## Monte Carlo Methods\n\n**Motivation: why Monte Carlo?**\n\nIn Bayesian inference we repeatedly encounter integrals such as\n\n$$\n\\mathbb{E}[g(\\theta)\\mid y]\n=\\int g(\\theta)\\,p(\\theta\\mid y)\\,d\\theta,\n\\qquad\n\\Pr(\\theta\\in A\\mid y)=\\int_A p(\\theta\\mid y)\\,d\\theta,\n$$\n\nthat are not available in closed form. **Monte Carlo** replaces these integrals by averages of random draws.\n\n::: callout-note\nKey idea: replace an intractable integral by an empirical mean.\n:::\n\n## The Monte Carlo Method\n\nIn the previous chapter, we obtained the following posterior distributions for the birth rates of women without and with bachelor’s degrees:\n\n$$\n\\theta_1 \\mid \\sum_{i=1}^{111} Y_{i,1} = 217\n\\sim \\text{Gamma}(219, 112),\n$$\n\n$$\n\\theta_2 \\mid \\sum_{i=1}^{44} Y_{i,2} = 66\n\\sim \\text{Gamma}(68, 45).\n$$\n\nIt was claimed that\n\n> $$\n> \\Pr(\\theta_1 > \\theta_2 \\mid \\text{data}) = 0.97.\n> $$\n\nHow do we compute such a probability? From Chapter 2, since $\\theta_1$ and $\\theta_2$ are conditionally independent given the data $y$, we have $$\n\\Pr(\\theta_1 > \\theta_2 \\mid y)\n=\n\\int_0^\\infty \\int_0^{\\theta_1}\np(\\theta_1 \\mid y)\np(\\theta_2 \\mid y)\n\\, d\\theta_2 \\, d\\theta_1.\n$$\n\nSubstituting the gamma densities gives\n\n$$\n\\int_0^\\infty \\int_0^{\\theta_1}\n\\text{dgamma}(\\theta_1; 219, 112)\n\\,\n\\text{dgamma}(\\theta_2; 68, 45)\n\\, d\\theta_2 \\, d\\theta_1.\n$$\n\nThis integral can be evaluated numerically. However, in realistic Bayesian models, such integrals quickly become high-dimensional and analytically intractable. This motivates **Monte Carlo (MC) methods**.\n\n### MC Approximation\n\nSuppose we wish to compute\n\n$$\n\\mathbb{E}[g(\\theta) \\mid y]\n=\n\\int g(\\theta) \\, p(\\theta \\mid y) \\, d\\theta.\n$$\n\nIf we can generate independent samples\n\n$$\n\\theta^{(1)}, \\ldots, \\theta^{(S)}\n\\sim p(\\theta \\mid y),\n$$\n\nthen we approximate the expectation by\n\n$$\n\\frac{1}{S}\n\\sum_{s=1}^S g(\\theta^{(s)}).\n$$\n\nThis is called a **Monte Carlo approximation**. By the Law of Large Numbers,\n\n$$\n\\frac{1}{S}\n\\sum_{s=1}^S g(\\theta^{(s)})\n\\;\\longrightarrow\\;\n\\mathbb{E}[g(\\theta) \\mid y] = \\int g(\\theta) p(\\theta \\mid y) d\\theta \n\\quad \\text{as } S \\to \\infty.\n$$ With the property above, we can calculate many quantities of interest about the posterior distribution. For example, suppose $\\bar{\\theta}$ is the average of $\\{\\theta^{(1)}, \\dots, \\theta^{(S)}\\}$, then as $S \\to \\infty$:\n\n-   $\\bar{\\theta} \\to \\mathbb{E}[\\theta \\mid y]$,\n-   $\\frac{1}{S-1}\\sum_{s=1}^S (\\theta^{(s)} - \\bar{\\theta})^2\\to \\mathrm{Var}(\\theta \\mid y)$.\n-   $\\frac{1}{S}\\sum_{s=1}^S \\mathbf{1}\\{\\theta^{(s)} \\in A\\}\\to \\Pr(\\theta \\in A \\mid y)$.\n-   the empirical distribution of $\\{\\theta^{(1)}, \\dots, \\theta^{(S)}\\}$ converges to $p(\\theta \\mid y)$.\n-   the sample median converges to the posterior median $\\theta_{1/2}$.\n-   the sample $\\alpha$-quantile converges to $\\theta_\\alpha$.\n\n### Convergence Properties\n\nLet $\\theta^{(1)}, \\dots, \\theta^{(S)} \\sim p(\\theta \\mid y)$.\n\nAs $S \\to \\infty$:\n\n-   $\\displaystyle \\frac{\\#\\{\\theta^{(s)} \\le c\\}}{S}\n    \\;\\longrightarrow\\;\n    \\Pr(\\theta \\le c \\mid y)$\n\n-   The empirical distribution of $\\{\\theta^{(1)},\\dots,\\theta^{(S)}\\}$ converges to $p(\\theta \\mid y)$\n\n-   The sample median converges to the posterior median $\\theta_{1/2}$\n\n-   The sample $\\alpha$-quantile converges to $\\theta_\\alpha$\n\n**Key message:**\\\nAlmost any aspect of a posterior distribution can be approximated arbitrarily well using a sufficiently large Monte Carlo sample.\n\nThus Monte Carlo sampling allows us to approximate:\n\n-   posterior means,\n-   posterior variances,\n-   posterior probabilities,\n-   credible intervals,\n-   many more\n\n::: {.callout-example title=\"(Cont) Estimating probability of  theta1 > theta2\"}\nTo approximate $$\n\\Pr(\\theta_1 > \\theta_2 \\mid y),\n$$\n\nwe can:\n\n0.  Choose a (large) number of samples $S$ (e.g., $S=10,000$).\n1.  Draw $\\theta_1^{(s)} \\sim \\text{Gamma}(219, 112)$.\n2.  Draw $\\theta_2^{(s)} \\sim \\text{Gamma}(68, 45)$.\n3.  Compute the indicator $$\n    I^{(s)} = \\mathbf{1}\\{\\theta_1^{(s)} > \\theta_2^{(s)}\\}.\n    $$\n\nThen\n\n$$\n\\Pr(\\theta_1 > \\theta_2 \\mid y)\n\\approx\n\\frac{1}{S}\n\\sum_{s=1}^S I^{(s)}.\n$$\n\nThis avoids evaluating any double integrals.\n:::\n\n> Why Monte Carlo?\n>\n> Works in high dimensions. Requires only the ability to simulate. Avoids symbolic integration. Scales to complex hierarchical models.\n\nThis is the foundation of modern Bayesian computation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Figure 4.1 — Monte Carlo Approximation (Gamma(68,45))\n# Histograms + KDEs for S = 10, 100, 1000; true density in gray\n\nset.seed(8670)\nlibrary(ggplot2)\n\n# Posterior: Gamma(shape=68, rate=45)\nshape_post <- 68\nrate_post  <- 45\n\n# MC samples\nS_list <- c(10, 100, 1000)\nmc_df <- do.call(rbind, lapply(S_list, function(S) {\n  data.frame(theta = rgamma(S, shape = shape_post, rate = rate_post),\n             S = factor(S, levels = S_list))\n}))\n\n# Grid for true density (choose a sensible range around the mass)\nxgrid <- seq(\n  qgamma(0.001, shape = shape_post, rate = rate_post),\n  qgamma(0.999, shape = shape_post, rate = rate_post),\n  length.out = 600\n)\n\ntrue_df <- data.frame(\n  theta = xgrid,\n  dens  = dgamma(xgrid, shape = shape_post, rate = rate_post)\n)\n\n# Plot\nggplot(mc_df, aes(x = theta)) +\n  # histogram (density scale so it overlays with densities)\n  geom_histogram(aes(y = after_stat(density)),\n                 bins = 18, color = \"black\", fill = \"white\") +\n  # KDE from MC samples\n  geom_density(linewidth = 1.1) +\n  # True density (gray)\n  geom_line(data = true_df, aes(x = theta, y = dens),\n            linewidth = 1.2, color = \"gray50\") +\n  facet_wrap(~ S, nrow = 1, scales = \"free_y\") +\n  labs(\n    title = \"Monte Carlo Approximation\",\n  subtitle = paste(\n    \"Histograms and KDEs for Monte Carlo samples\",\n    \"True Gamma(68, 45) density shown in gray\",\n    sep = \"\\n\"\n  ),\n    x = expression(theta),\n    y = \"Density\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(size = 18, face = \"bold\"),\n    plot.subtitle = element_text(size = 13),\n    strip.text = element_text(size = 14, face = \"bold\")\n  )\n```\n\n::: {.cell-output-display}\n![](04_MC_files/figure-pdf/unnamed-chunk-1-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n## Numerical Evaluation\n\nWe now compare Monte Carlo approximations to quantities that can be computed analytically in this conjugate example.\n\nSuppose\n\n$$\nY_1,\\dots,Y_n \\mid \\theta \\sim \\text{Poisson}(\\theta),\n\\quad\n\\theta \\sim \\text{Gamma}(a,b).\n$$\n\nAfter observing $y_1,\\dots,y_n$ with $\\sum y_i = sy$ and sample size $n$, the posterior distribution is\n\n$$\n\\theta \\mid y \\sim \\text{Gamma}(a+sy,\\; b+n).\n$$\n\n::: {.callout-example title=\"College-Educated Group\"}\nFor the birth-rate example:\n\n-   $a = 2$\n-   $b = 1$\n-   $sy = 66$\n-   $n = 44$\n\nPosterior:\n\n$$\n\\theta \\mid y \\sim \\text{Gamma}(68,45).\n$$\n\nPosterior mean:\n\n$$\n\\mathbb{E}[\\theta \\mid y]\n=\n\\frac{a+sy}{b+n}\n=\n\\frac{68}{45}\n=\n1.51.\n$$\n\n### Monte Carlo in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(8670)\n\n## Posterior parameters\na  <- 2\nb  <- 1\nsy <- 66\nn  <- 44\n\nshape_post <- a + sy\nrate_post  <- b + n\n\n## Exact quantities\nmean_exact <- shape_post / rate_post\np_exact    <- pgamma(1.75, shape = shape_post, rate = rate_post)\nci_exact   <- qgamma(c(0.025, 0.975),\n                     shape = shape_post,\n                     rate  = rate_post)\n\n## Monte Carlo samples\ntheta_mc10   <- rgamma(10,   shape_post, rate_post)\ntheta_mc100  <- rgamma(100,  shape_post, rate_post)\ntheta_mc1000 <- rgamma(1000, shape_post, rate_post)\n\n## Function to summarize MC output\nmc_summary <- function(theta_sample) {\n  c(\n    Mean        = mean(theta_sample),\n    Prob_less   = mean(theta_sample < 1.75),\n    CI_lower    = quantile(theta_sample, 0.025),\n    CI_upper    = quantile(theta_sample, 0.975)\n  )\n}\n\n## Build comparison table\nresults <- rbind(\n  Exact   = c(Mean      = mean_exact,\n              Prob_less = p_exact,\n              CI_lower  = ci_exact[1],\n              CI_upper  = ci_exact[2]),\n\n  MC_10   = mc_summary(theta_mc10),\n  MC_100  = mc_summary(theta_mc100),\n  MC_1000 = mc_summary(theta_mc1000)\n)\n\nround(results, 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Mean Prob_less CI_lower CI_upper\nExact   1.5111    0.8998   1.1734   1.8908\nMC_10   1.5077    1.0000   1.3628   1.6228\nMC_100  1.5033    0.8500   1.1536   1.9305\nMC_1000 1.5180    0.8860   1.1810   1.8937\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSmax <- 1000\ntheta_seq <- rgamma(Smax, shape = shape_post, rate = rate_post)\ncum_mean  <- cumsum(theta_seq) / seq_along(theta_seq)\n\nplot(cum_mean, type=\"l\",\n     xlab=\"Number of draws (S)\",\n     ylab=\"Cumulative Monte Carlo mean\")\nabline(h = mean_exact, lty = 2, lwd = 2)\n```\n\n::: {.cell-output-display}\n![](04_MC_files/figure-pdf/unnamed-chunk-3-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxgrid <- seq(0.5, 2.5, length.out = 400)\ntrue_pdf <- dgamma(xgrid, shape = shape_post, rate = rate_post)\n\npar(mfrow=c(1,3))\n\nfor (S in c(10,100,1000)) {\n  x <- get(paste0(\"theta_mc\", S))\n  hist(x, prob=TRUE,\n       main=paste0(\"S = \", S),\n       xlab=expression(theta),\n       border=\"black\")\n  lines(xgrid, true_pdf, lwd=2)\n}\n```\n\n::: {.cell-output-display}\n![](04_MC_files/figure-pdf/unnamed-chunk-4-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\npar(mfrow=c(1,1))\n```\n:::\n\n:::\n\n### There are much more about the MC method\n\n-   Variance reduction methods\n-   Antithetic variates\n-   Control variables\n-   Importance Sampling\n-   Stratified Sampling\n-   Stratified Importance Sampling\n-   etc...\n\nYou may refer to my notes in Chapter 4 in the [Computational Methods in Statistics Course](https://chikuang.github.io/course/stat8670/04-monte-carlo.html#other-methods).\n\n### MC for predictive distribution and Sampling from it\n\nAs discussed earlier, the **predictive distribution** of a future random variable $\\tilde Y$ is the probability distribution that reflects uncertainty about $\\tilde Y$ after accounting for both:\n\n-   known quantities (conditioned on observed data), and\n-   unknown quantities (integrated out).\n\n### Sampling Model vs Predictive Model\n\nSuppose $\\tilde Y$ denotes the number of children for a randomly selected woman aged 40 with a college degree.\n\nIf the true mean birthrate $\\theta$ were known, uncertainty about $\\tilde Y$ would be described by the **sampling model** $$\n\\Pr(\\tilde Y = \\tilde y \\mid \\theta) = p(\\tilde y \\mid \\theta)\n= \\frac{\\theta^{\\tilde y} e^{-\\theta}}{\\tilde y!},\n$$ that is, $$\n\\tilde Y \\mid \\theta \\sim \\text{Poisson}(\\theta).\n$$\n\nIn practice, however, $\\theta$ is unknown. Therefore, predictions must account for uncertainty in $\\theta$.\n\n### Prior Predictive Distribution\n\nIf no data have been observed, predictions are obtained by integrating out $\\theta$ using the prior distribution: $$\n\\Pr(\\tilde Y = \\tilde y)\n= \\int p(\\tilde y \\mid \\theta)\\, p(\\theta)\\, d\\theta.\n$$\n\nThis is called the **prior predictive distribution**.\n\nFor a Poisson model with a Gamma prior, $$\n\\theta \\sim \\text{Gamma}(a,b),\n$$ the prior predictive distribution of $\\tilde Y$ is $$\n\\tilde Y \\sim \\text{Negative Binomial}(a,b).\n$$\n\n### Posterior Predictive Distribution\n\nAfter observing data $Y_1 = y_1, \\ldots, Y_n = y_n$, the relevant predictive distribution for a new observation is $$\n\\Pr(\\tilde Y = \\tilde y \\mid Y_1=y_1,\\ldots,Y_n=y_n)\n= \\int p(\\tilde y \\mid \\theta)\\, p(\\theta \\mid y_1,\\ldots,y_n)\\, d\\theta.\n$$\n\nThis distribution is called the **posterior predictive distribution**.\n\nFor the Poisson–Gamma model, the posterior distribution is $$\n\\theta \\mid y_1,\\ldots,y_n \\sim \\text{Gamma}\\!\\left(a + \\sum_{i=1}^n y_i,\\; b+n\\right),\n$$ and the posterior predictive distribution is again Negative Binomial.\n\n------------------------------------------------------------------------\n\n### Monte Carlo Sampling from the Posterior Predictive Distribution\n\nIn many models, the posterior predictive distribution cannot be evaluated analytically. However, it can often be **sampled using Monte Carlo methods**.\n\nThe idea is simple:\n\n1.  Draw $\\theta^{(s)} \\sim p(\\theta \\mid y_1,\\ldots,y_n)$\\\n2.  Draw $\\tilde Y^{(s)} \\sim p(\\tilde y \\mid \\theta^{(s)})$\\\n3.  Repeat for $s = 1,\\ldots,S$\n\nThis produces samples $$\n\\tilde Y^{(1)}, \\ldots, \\tilde Y^{(S)}\n\\sim p(\\tilde y \\mid y_1,\\ldots,y_n),\n$$ which approximate the posterior predictive distribution.\n\n------------------------------------------------------------------------\n\n### Example: Comparing Two Groups (Poisson Model)\n\nSuppose we observe two independent groups with Poisson data:\n\n-   Group 1: $\\sum Y_{i,1} = 217$, $n_1 = 111$\\\n-   Group 2: $\\sum Y_{i,2} = 66$, $n_2 = 44$\n\nWith a common prior $$\n\\theta_k \\sim \\text{Gamma}(a,b), \\quad k=1,2,\n$$ the posterior distributions are $$\n\\theta_1 \\mid \\mathbf y_1 \\sim \\text{Gamma}(a+217,\\; b+111),\n$$ $$\n\\theta_2 \\mid \\mathbf y_2 \\sim \\text{Gamma}(a+66,\\; b+44).\n$$\n\nBecause $\\theta_1$ and $\\theta_2$ are **posterior independent**, posterior predictive sampling proceeds independently for each group:\n\n$$\n\\theta_1^{(s)} \\sim p(\\theta_1 \\mid \\mathbf y_1), \\quad\n\\tilde Y_1^{(s)} \\sim \\text{Poisson}(\\theta_1^{(s)}),\n$$ $$\n\\theta_2^{(s)} \\sim p(\\theta_2 \\mid \\mathbf y_2), \\quad\n\\tilde Y_2^{(s)} \\sim \\text{Poisson}(\\theta_2^{(s)}).\n$$\n\n------------------------------------------------------------------------\n\n### Monte Carlo Approximation of Predictive Quantities\n\nUsing Monte Carlo samples $\\{\\tilde Y_1^{(s)}, \\tilde Y_2^{(s)}\\}$, we can approximate quantities such as $$\n\\Pr(\\tilde Y_1 > \\tilde Y_2 \\mid \\text{data})\n\\approx\n\\frac{1}{S} \\sum_{s=1}^S\n\\mathbb{I}\\bigl(\\tilde Y_1^{(s)} > \\tilde Y_2^{(s)}\\bigr).\n$$\n\nMore generally, Monte Carlo samples from the posterior predictive distribution allow us to approximate:\n\n-   predictive probabilities,\n-   predictive expectations,\n-   quantiles and credible intervals,\n-   functions of future observations.\n\nThis flexibility is one of the main strengths of Monte Carlo methods in Bayesian analysis.\n\n### Checking\n\n4.4 Posterior Predictive Model Checking\n\nPosterior predictive model checking assesses whether a fitted Bayesian model can plausibly reproduce key features of the observed data.\n\nWe focus on women aged 40 without a college degree. The empirical distribution of the number of children for these women, together with the corresponding posterior predictive distribution, is shown in Figure 4.6.\n\nIn this sample of size $$\nn = 111,\n$$ the number of women with exactly two children is $$\ny_{\\text{obs}} = 38,\n$$ which is twice the number of women with exactly one child.\n\nIn contrast, the posterior predictive distribution (shown in gray) suggests that sampling a woman with two children is slightly less likely than sampling a woman with one child, with probabilities approximately $$\n0.27 \\quad \\text{vs.} \\quad 0.28.\n$$\n\nThese two distributions appear to be in conflict: if the observed data contain twice as many women with two children as with one child, why does the model predict otherwise?\n\nPossible Explanations\n\nOne explanation is sampling variability. The empirical distribution of a finite sample does not necessarily match the true population distribution, and with moderate sample sizes, random fluctuations can be substantial. A smooth population distribution can easily produce a bumpy empirical histogram.\n\nAn alternative explanation is model misspecification. In particular, the Poisson model cannot capture certain features of the data. There is no Poisson distribution with a sharp peak at $y = 2$, whereas the empirical distribution shows exactly such behavior.\n\nThese explanations can be investigated systematically using Monte Carlo simulation.\n\nA Posterior Predictive Discrepancy Statistic\n\nDefine the discrepancy statistic $$\nt(y) = \\frac{\\#{y_i = 2}}{\\#{y_i = 1}},\n$$ the ratio of the number of women with two children to the number with one child.\n\nFor the observed data, $$\nt(y_{\\text{obs}}) = 2.\n$$\n\nTo assess whether this value is surprising under the model, we examine the posterior predictive distribution of $t(\\tilde{Y})$.\n\nPosterior Predictive Monte Carlo Procedure\n\nFor each Monte Carlo iteration $s = 1, \\dots, S$: 1. Sample from the posterior $$\n\\theta^{(s)} \\sim p(\\theta \\mid y_{\\text{obs}})\n$$ 2. Generate a posterior predictive dataset $$\n\\tilde{Y}^{(s)} = (\\tilde{y}_1^{(s)}, \\dots, \\tilde{y}_n^{(s)}),\n\\quad\n\\tilde{y}_i^{(s)} \\stackrel{\\text{i.i.d.}}{\\sim} \\text{Poisson}(\\theta^{(s)})\n$$ 3. Compute the discrepancy $$\nt^{(s)} = t(\\tilde{Y}^{(s)})\n$$\n\nThis yields samples $$\n{ t^{(1)}, \\dots, t^{(S)} }\n$$ from the posterior predictive distribution of $t(\\tilde{Y})$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Prior parameters\na <- 2\nb <- 1\n\n## Data summary (no bachelor's degree group)\nn  <- 111\nsy <- 217   # sum(y_i)\n\n## Storage\nt_mc <- numeric(10000)\n\nfor (s in 1:10000) {\n  ## Draw from posterior\n  theta <- rgamma(1, shape = a + sy, rate = b + n)\n  \n  ## Posterior predictive sample\n  y_mc <- rpois(n, theta)\n  \n  ## Discrepancy statistic\n  n1 <- sum(y_mc == 1)\n  n2 <- sum(y_mc == 2)\n  \n  ## Avoid division by zero\n  t_mc[s] <- ifelse(n1 > 0, n2 / n1, NA)\n}\n\n## Remove undefined values\nt_mc <- t_mc[!is.na(t_mc)]\n```\n:::\n\n\nInterpretation\n\nFigure 4.6 shows the posterior predictive distribution of $t(\\tilde{Y})$, with the observed value $t(y_{\\text{obs}})$ indicated by a vertical line.\n\nOut of 10,000 Monte Carlo samples, only about 0.5% produce values of $$\nt(\\tilde{Y}) \\ge t(y_{\\text{obs}}).\n$$\n\nThis indicates that the observed discrepancy is extremely unlikely under the fitted Poisson model.\n\nConclusion\n\nThe posterior predictive check suggests that the Poisson model is inadequate for these data. Although it matches the posterior mean reasonably well, it fails to reproduce important distributional features.\n\nThis does not imply that the model is useless for all inferential goals. However, if our goal is to accurately describe the distribution of family sizes, a more flexible model is needed.\n\nPosterior predictive checks provide a principled, simulation-based tool for diagnosing such failures and guiding model refinement.\n\n## Gibbs sampler\n\nFor many multiparameter Bayesian models, the joint posterior distribution does not belong to a standard family and is therefore difficult to sample from directly. However, it is often the case that **sampling from the full conditional distribution of each parameter is straightforward**.\n\nIn such situations, posterior approximation can be carried out using the **Gibbs sampler**, an iterative Monte Carlo algorithm that constructs a dependent sequence of parameter values whose distribution converges to the target joint posterior distribution.\n\nIn this chapter, we introduce the Gibbs sampler in the context of the normal model with a **semiconjugate prior**, and study how well it approximates the posterior distribution.\n\n\n## 6.1 A Semiconjugate Prior Distribution\n\nFor normal distribution, it may be modeled our uncertainty about the population mean $\\theta$ as depending on the sampling variance $\\sigma^2$ via\n$$\n\\theta \\mid \\sigma^2 \\sim\n N\\!\\left(\\mu_0,\\; \\frac{\\sigma^2}{\\kappa_0}\\right).\n$$\n\nThis formulation ties the prior variance of $\\theta$ to the sampling variability of the data, and $\\mu_0$ can be interpreted as representing $\\kappa_0$ prior observations from the population.\n\nIn some settings this dependence is reasonable, but in others we may wish to specify prior uncertainty about $\\theta$ *independently* of $\\sigma^2$, so that\n$$\np(\\theta, \\sigma^2) = p(\\theta)\\,p(\\sigma^2).\n$$\n\nOne such specification is the following **semiconjugate prior distribution**:\n$$\n\\theta \\sim \\text{Normal}(\\mu_0, \\tau_0^2),\n\\qquad\n\\frac{1}{\\sigma^2} \\sim \\text{Gamma}\\!\\left(\\frac{\\nu_0}{2}, \\frac{\\nu_0 \\sigma_0^2}{2}\\right).\n$$\n\n**Posterior Distribution of $\\theta \\mid \\sigma^2$**\n\nSuppose\n$$\nY_1, \\dots, Y_n \\mid \\theta, \\sigma^2 \\;\\overset{i.i.d.}{\\sim}\\; (\\theta, \\sigma^2).\n$$\n\nThen, conditional on $\\sigma^2$ and the observed data $y_1, \\dots, y_n$, the posterior distribution of $\\theta$ is\n$$\n\\theta \\mid \\sigma^2, y_1, \\dots, y_n\n\\;\\sim\\;\n\\text{Normal}(\\mu_n, \\tau_n^2),\n$$\nwhere\n$$\n\\mu_n\n=\n\\frac{\\mu_0 / \\tau_0^2 + n \\bar{y} / \\sigma^2}\n     {1 / \\tau_0^2 + n / \\sigma^2},\n\\qquad\n\\tau_n^2\n=\n\\left( \\frac{1}{\\tau_0^2} + \\frac{n}{\\sigma^2} \\right)^{-1}.\n$$\n\nThis conditional posterior distribution will form one step of the Gibbs sampler.\n\n\n### Key Takeaway\n\n- The **joint posterior** of $(\\theta, \\sigma^2)$ is not available in closed form.\n- The **full conditional distributions** of $\\theta \\mid \\sigma^2, y$ and $\\sigma^2 \\mid \\theta, y$ *are* available in standard forms.\n- This structure makes the Gibbs sampler a natural and efficient tool for posterior approximation.\n\nIn the next section, we derive the full conditional distribution of $\\sigma^2$ and combine the two conditional updates into a complete Gibbs sampling algorithm.\n\n\nIn the conjugate case where $\\tau_0^2$ is proportional to $\\sigma^2$, we showed that the marginal posterior distribution\n$$\np(\\sigma^2 \\mid y_1,\\ldots,y_n)\n$$\nis an inverse-gamma distribution. In this setting, Monte Carlo samples of $(\\theta,\\sigma^2)$ from the joint posterior distribution can be obtained by the following two-step procedure:\n\n1. Sample\n$$\n\\sigma^{2(s)} \\sim p(\\sigma^2 \\mid y_1,\\ldots,y_n),\n$$\nwhich is an inverse-gamma distribution.\n\n2. Sample\n$$\n\\theta^{(s)} \\sim p(\\theta \\mid \\sigma^{2(s)}, y_1,\\ldots,y_n),\n$$\nwhich is a normal distribution.\n\nThis approach works because both full conditional distributions are standard and easy to sample from.\n\nHowever, when $\\tau_0^2$ is **not proportional** to $\\sigma^2$, the marginal posterior distribution of the precision\n$$\n\\frac{1}{\\sigma^2}\n$$\nis **not** a gamma distribution, nor any other standard distribution from which we can easily sample. As a result, direct Monte Carlo sampling from the marginal posterior is no longer straightforward, motivating the need for alternative approximation methods.\n\n## 6.2 Discrete Approximations\n\nIn the conjugate case where $\\tau_0^2$ was proportional to $\\sigma^2$, we showed that\n$$\np(\\sigma^2 \\mid y_1,\\dots,y_n)\n$$\nwas an inverse-gamma distribution, and Monte Carlo samples of $(\\theta, \\sigma^2)$ from the joint posterior distribution could be obtained by:\n\n1. sampling $\\sigma^{2(s)} \\sim p(\\sigma^2 \\mid y_1,\\dots,y_n)$, an inverse-gamma distribution;\n2. sampling $\\theta^{(s)} \\sim p(\\theta \\mid \\sigma^{2(s)}, y_1,\\dots,y_n)$, a normal distribution.\n\nHowever, in the semiconjugate case where $\\tau_0^2$ is **not proportional** to $\\sigma^2$, the marginal posterior distribution of $1/\\sigma^2$ is **not a gamma distribution**, nor any other standard distribution from which we can easily sample directly.\n\n### Posterior Density Ratios\n\nLet\n$$\n\\tilde{\\sigma}^2 = \\frac{1}{\\sigma^2}\n$$\ndenote the precision. Recall that the posterior distribution of $(\\theta, \\tilde{\\sigma}^2)$ is equal to the **joint distribution**\n$$\np(\\theta, \\tilde{\\sigma}^2, y_1,\\dots,y_n),\n$$\ndivided by $p(y_1,\\dots,y_n)$, which does not depend on the parameters. Therefore, the **relative posterior probabilities** of two parameter values $(\\theta_1, \\tilde{\\sigma}_1^2)$ and $(\\theta_2, \\tilde{\\sigma}_2^2)$ are directly computable:\n$$\n\\frac{p(\\theta_1, \\tilde{\\sigma}_1^2 \\mid y_1,\\dots,y_n)}\n     {p(\\theta_2, \\tilde{\\sigma}_2^2 \\mid y_1,\\dots,y_n)}\n=\n\\frac{p(\\theta_1, \\tilde{\\sigma}_1^2, y_1,\\dots,y_n)}\n     {p(\\theta_2, \\tilde{\\sigma}_2^2, y_1,\\dots,y_n)}.\n$$\n\n### Joint Distribution\n\nThe joint density can be written as\n$$\n\\begin{aligned}\np(\\theta, \\tilde{\\sigma}^2, y_1,\\dots,y_n)\n&= p(\\theta, \\tilde{\\sigma}^2)\\, p(y_1,\\dots,y_n \\mid \\theta, \\tilde{\\sigma}^2) \\\\\n&= \\text{Normal}(\\theta \\mid \\mu_0, \\tau_0^2)\n   \\times \\text{Gamma}\\!\\left(\\tilde{\\sigma}^2 \\mid \\frac{\\nu_0}{2}, \\frac{\\nu_0 \\sigma_0^2}{2}\\right)\n   \\times \\prod_{i=1}^n \\text{Normal}\\!\\left(y_i \\mid \\theta, \\frac{1}{\\tilde{\\sigma}^2}\\right).\n\\end{aligned}\n$$\n\nAll components of this joint density are standard distributions and therefore easy to evaluate numerically.\n\n### Discrete Posterior Approximation\n\nA **discrete approximation** to the posterior distribution is obtained by constructing a grid over the parameter space and evaluating relative posterior probabilities on that grid.\n\nSpecifically:\n\n- choose grids $\\{\\theta_1,\\dots,\\theta_G\\}$ and $\\{\\tilde{\\sigma}_1^2,\\dots,\\tilde{\\sigma}_H^2\\}$ consisting of evenly spaced parameter values;\n\n- evaluate $p(\\theta_g, \\tilde{\\sigma}_h^2, y_1,\\dots,y_n)$ for each grid point $(\\theta_g, \\tilde{\\sigma}_h^2)$;\n\n- assign posterior probabilities proportional to these values:\n\n$$\np(\\theta_g, \\tilde{\\sigma}_h^2 \\mid y_1,\\dots,y_n)\n\\;\\propto\\;\np(\\theta_g, \\tilde{\\sigma}_h^2, y_1,\\dots,y_n).\n$$\n\nThis discrete approximation can then be normalized and used to compute posterior summaries such as means, variances, and credible regions.\n\n\n### Remarks\n\n- Discrete approximations are conceptually simple and transparent.\n- They are feasible only in **low-dimensional parameter spaces**.\n- For higher-dimensional models, simulation-based methods such as the **Gibbs sampler** become essential.\n\n\n### Discrete approximations\n\nLet $\\{\\theta_1,\\ldots,\\theta_G\\}$ and $\\{\\tilde\\sigma_1^2,\\ldots,\\tilde\\sigma_H^2\\}$ be discrete grids for $\\theta$ and $\\sigma^2$, respectively.  \nA discrete approximation to the joint posterior distribution is defined by\n\n$$\np_D(\\theta_k,\\tilde\\sigma_l^2 \\mid y_1,\\ldots,y_n)\n=\n\\frac{p(\\theta_k,\\tilde\\sigma_l^2 \\mid y_1,\\ldots,y_n)}\n{\\sum_{g=1}^G \\sum_{h=1}^H p(\\theta_g,\\tilde\\sigma_h^2 \\mid y_1,\\ldots,y_n)}.\n$$\n\nUsing Bayes’ rule, this can be written equivalently as\n\n$$\np_D(\\theta_k,\\tilde\\sigma_l^2 \\mid y_1,\\ldots,y_n)\n=\n\\frac{p(\\theta_k,\\tilde\\sigma_l^2,y_1,\\ldots,y_n)}\n{\\sum_{g=1}^G \\sum_{h=1}^H p(\\theta_g,\\tilde\\sigma_h^2,y_1,\\ldots,y_n)}.\n$$\n\nThis defines a **valid joint probability distribution** over\n$$\n\\theta \\in \\{\\theta_1,\\ldots,\\theta_G\\}, \\qquad\n\\sigma^2 \\in \\{\\tilde\\sigma_1^2,\\ldots,\\tilde\\sigma_H^2\\},\n$$\nsince the probabilities sum to one. In fact, if the joint prior distribution is discrete on this grid, then $p_D$ is exactly the posterior distribution.\n\n#### Application: midge data\n\nWe now apply this approximation to the midge data from the previous chapter.  \nRecall that the data consist of $n=9$ observations with sample mean $\\bar y = 1.804$ and sample variance $s^2 = 0.017$.\n\nIn the conjugate model, the prior variance of $\\theta$ is proportional to the sampling variance $\\sigma^2 / \\kappa_0$. When the sampling variance is very small, this can undesirably force the prior uncertainty about $\\theta$ to be small as well. The semiconjugate prior removes this constraint.\n\nRecall that we previously suggested a prior mean and standard deviation of\n$$\n\\mu_0 = 1.9, \\qquad \\tau_0 = 0.95,\n$$\nplacing most of the prior mass on $\\theta > 0$.  \nFor $\\sigma^2$, we take prior parameters\n$$\n\\nu_0 = 1, \\qquad \\sigma_0^2 = 0.01.\n$$\n\n\n#### Grid-based approximation\n\nThe R implementation evaluates the joint posterior distribution\n$$\np(\\theta,\\sigma^2 \\mid y_1,\\ldots,y_n)\n$$\non a $100 \\times 100$ grid of evenly spaced parameter values, with\n$$\n\\theta \\in \\{1.505, 1.510, \\ldots, 1.995, 2.00\\}\n$$\nand\n$$\n\\sigma^2 \\in \\{1.75, 3.5, \\ldots, 173.25, 175.0\\}.\n$$\n\nThe first panel of Figure 6.1 shows the resulting discrete approximation to the **joint posterior distribution** of $(\\theta,\\sigma^2)$.\n\nMarginal and conditional posterior distributions can then be obtained by simple summation. For example, the marginal posterior distribution of $\\theta$ is\n\n$$\np_D(\\theta_k \\mid y_1,\\ldots,y_n)\n=\n\\sum_{h=1}^H p_D(\\theta_k,\\tilde\\sigma_h^2 \\mid y_1,\\ldots,y_n).\n$$\n\nThe resulting discrete approximations to the marginal posterior distributions of $\\theta$ and $\\sigma^2$ are shown in the second and third panels of Figure 6.1.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'dplyr'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(patchwork)\nlibrary(scales)\n\n## -----------------------------\n## Data and semiconjugate prior\n## -----------------------------\ny  <- c(1.64,1.70,1.72,1.74,1.82,1.82,1.82,1.90,2.08)\nn  <- length(y)\n\nmu0     <- 1.9\ntau0_sq <- 0.95^2\nnu0     <- 1\ns20     <- 0.01\n\n## -----------------------------\n## Grids: theta and precision  (tilde sigma^2 = 1/sigma^2)\n## -----------------------------\nG <- 100\nH <- 100\n\nmean.grid <- seq(1.505, 2.00, length.out = G)\nprec.grid <- seq(1.75, 175,  length.out = H)   # this IS \\tilde{\\sigma}^2\n\npost.grid <- matrix(NA_real_, nrow = G, ncol = H)\n\n## -----------------------------\n## Discrete joint posterior on the grid\n## p(theta, prec | y) ∝ p(theta) p(prec) ∏ N(y_i | theta, 1/prec)\n## -----------------------------\nfor (g in 1:G) {\n  for (h in 1:H) {\n    post.grid[g, h] <-\n      dnorm(mean.grid[g], mu0, sqrt(tau0_sq)) *\n      dgamma(prec.grid[h], shape = nu0/2, rate = s20*nu0/2) *\n      prod(dnorm(y, mean.grid[g], sd = 1 / sqrt(prec.grid[h])))\n  }\n}\npost.grid <- post.grid / sum(post.grid)\n\n## -----------------------------\n## Data frame + marginals\n## -----------------------------\npost_df <- expand.grid(theta = mean.grid, prec = prec.grid)\npost_df$prob <- as.vector(post.grid)\n\ntheta_marg <- post_df %>%\n  group_by(theta) %>%\n  summarise(prob = sum(prob), .groups = \"drop\")\n\nprec_marg <- post_df %>%\n  group_by(prec) %>%\n  summarise(prob = sum(prob), .groups = \"drop\")\n\n## -----------------------------\n## Plots (Figure 6.1 style)\n## -----------------------------\np_joint <- ggplot(post_df, aes(theta, prec, fill = prob)) +\n  geom_raster(interpolate = TRUE) +\n  scale_fill_gradient(low = \"white\", high = \"black\",\n                      trans = \"sqrt\", labels = label_number()) +\n  labs(x = expression(theta),\n       y = expression(tilde(sigma)^2)) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\np_theta <- ggplot(theta_marg, aes(theta, prob)) +\n  geom_line(linewidth = 1) +\n  labs(x = expression(theta),\n       y = expression(p(theta~\"|\"~y[1:n]))) +\n  theme_classic()\n\np_prec <- ggplot(prec_marg, aes(prec, prob)) +\n  geom_line(linewidth = 1) +\n  labs(x = expression(tilde(sigma)^2),\n       y = expression(p(tilde(sigma)^2~\"|\"~y[1:n]))) +\n  theme_classic()\n\np_joint + p_theta + p_prec\n```\n\n::: {.cell-output-display}\n![](04_MC_files/figure-pdf/unnamed-chunk-6-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n## Markov Chain Monte Carlo (MCMC)\n\n\n\n---\n\nThis Chapter borrows materials from Chapter 4 in Hoff (2009) and [Chapter 4 in Computational Methods in Statistics Course](https://chikuang.github.io/course/stat8670/04-monte-carlo.html#other-methods)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}