# Bayesian Inference for single parameter models

> Leading objectives:
>
> Understand how to perform Bayesian inference on a single parameter model.
>
> -   Binomial model with given n
> -   Poission model
> -   Exponential family

Recall the important ingredients of Bayesian inference:

1.  **Prior distribution:** $p(\theta)$
2.  **Likelihood function:** $p(y \mid \theta)$
3.  **Posterior distribution:** $p(\theta \mid y) \propto p(y \mid \theta) p(\theta)$

## Three basic ingredients of Bayesian inference

### Prior

The prior distribution encodes our beliefs about the parameter $\theta$ *before* conduct any experiments.


::: {.callout-note title = "Prior and Data are independent"}

Note that, the prior distribution is independent of the data. It represents our knowledge or beliefs about the parameter before seeing the data. 

:::

How do we choose a prior?

1.  **Informative priors:** Based on previous studies or expert knowledge
2.  **Weakly informative priors:** Provide some regularization without dominating the data
3.  **Non-informative priors:** Attempt to be "objective" (e.g., uniform, Jeffreys prior)

### Likelihood

The likelihood function represents the probability of observing the data given the parameter $\theta$. It can be derived from the assumed statistical model for the data or experiment, i.e., $y \sim p(y \mid \theta)$, or we can estimate this non-parametrically (i.e., without assuming the underlying distribution is the one we know.).


::: {.callout-note title = "Likelihood is NOT a probability distribution for $\theta$"}

Note that, the likelihood function is not a probability distribution for $\theta$ itself. It is a function of $\theta$ for fixed data $y$. 

:::

### Posterior

The posterior distribution combines the prior and likelihood to update our beliefs about $\theta$ after observing the data. It is given by Bayes' theorem: $$
p(\theta \mid y) = \frac{p(y \mid \theta) p(\theta)}{p(y)},
$$ where $p(y) = \int p(y \mid \theta) p(\theta) d\theta$ is the marginal likelihood or evidence.

### An simple example


**Examples:**

-   Beta prior + Binomial likelihood → Beta posterior
-   Normal prior + Normal likelihood (known variance) → Normal posterior
-   Gamma prior + Poisson likelihood → Gamma posterior

**Advantages:** - Analytical posteriors (no numerical integration needed) - Interpretable parameters - Computationally efficient

**Limitations:**

-   May not reflect true prior beliefs
-   Modern computing makes non-conjugate priors feasible

::: {.callout-example title = "Why Conjugate Priors?"}

Let's look a simple example to illustrate the convenience of conjugate priors. Consider a Binomial model with unknown success probability $\theta$ and known number of trials $n$. We can use a Beta prior for $\theta$.

Suppose we have a Binomial model with known number of trials $n$ and unknown success probability $\theta$. We can use a Beta prior for $\theta$.

-   **Prior:** $\theta \sim \text{Beta}(\alpha, \beta)$
-   **Likelihood:** $y \mid \theta \sim \text{Binomial}(n, \theta)$

The derivation of the posterior is as follows:

$$
\begin{aligned}
p(y \mid \theta) & = \binom{n}{y} \theta^y (1 - \theta)^{n - y}, \\
p(\theta) & = \frac{\theta^{\alpha - 1} (1 - \theta)^{\beta - 1}}{B(\alpha, \beta)},
\end{aligned}
$$ where $B(\alpha, \beta)$ is the Beta function. Then the posterior is proportional to: $$
p(\theta \mid y) \propto p(y \mid \theta) p(\theta) \propto \theta^{y + \alpha - 1} (1 -  \theta)^{n - y + \beta - 1}.
$$ This is the kernel of a Beta distribution with parameters $(\alpha + y, \beta + n - y)$. Thus, the posterior distribution is: $$
\theta \mid y \sim \text{Beta}(\alpha + y, \beta
 + n - y).
$$

Thus, the **Posterior** is $\theta \mid y \sim \text{Beta}(\alpha + y, \beta + n - y)$.

:::

## Happiness Data -- the first example of Bayesian inference procedure

We study Bayesian inference for a binomial proportion $\theta$ when the sample size $n$ is fixed. In this example, we want to see what is the procedure of doing Bayesian inference


::: {.callout-example title = "Happiness Data"}

In the 1998 General Social Survey, each female respondent aged 65 or over was asked whether she was generally happy.

Define the response variable
$$
Y_i =
\begin{cases}
1, & \text{if respondent } i \text{ reports being generally happy},\\
0, & \text{otherwise},
\end{cases}
\qquad i = 1,\ldots,n,
$$
where $n = 129$.

Because we lack information that distinguishes individuals, it is reasonable to treat the responses as **exchangeable**.  
That is, before observing the data, the labels or ordering of respondents carry no information.

Since the sample size $n$ is small relative to the population size $N$ of senior women, results from the previous chapter justify the following modeling approximation.

**Modeling Assumptions**:  Our beliefs about $(Y_1,\ldots,Y_{129})$ are described by:

- **An unknown population proportion**
  $$
  \theta = \frac{1}{N}\sum_{i=1}^N Y_i,
  $$
  where $\theta$ represents the proportion of generally happy individuals in the population.

- **A sampling model given $\theta$**

  Conditional on $\theta$, the responses $Y_1,\ldots,Y_{129}$ are independent and identically distributed Bernoulli random variables with
  $$
  \Pr(Y_i = 1 \mid \theta) = \theta.
  $$

> Given the population proportion $\theta$, each respondent independently reports being happy with probability $\theta$.

**Likelihood**:  Under this model, the probability of observing data $\{y_1,\ldots,y_{129}\}$ given $\theta$ is
$$
p(y_1,\ldots,y_{129} \mid \theta)
=
\theta^{\sum_{i=1}^{129} y_i}
(1-\theta)^{129-\sum_{i=1}^{129} y_i}.
$$

This expression depends on the data only through the sufficient statistic
$$
S = \sum_{i=1}^{129} Y_i,
$$
the total number of respondents who report being generally happy.

For the happiness data,
$$
S = 118,
$$
so the likelihood simplifies to
$$
p(y_1,\ldots,y_{129} \mid \theta)
=
\theta^{118}(1-\theta)^{11}.
$$

Q: Which prior to be used? 


A prior distribution is **conjugate** to a likelihood if the posterior distribution belongs to the same family as the prior. For the binomial likelihood, the **Beta distribution** is conjugate. But we have another choice of prior, to use *non-informative prior*.

**A Uniform Prior Distribution**:  Suppose our prior information about $\theta$ is very weak, in the sense that all subintervals of $[0,1]$ with equal length are equally plausible.  
Symbolically, for any $0 \le a < b < b+c \le 1$,
$$
\Pr(a \le \theta \le b)
=
\Pr(a+c \le \theta \le b+c).
$$

This implies a **uniform prior**:
$$
p(\theta) = 1, \qquad 0 \le \theta \le 1.
$$

**Posterior Distribution**:  Bayes’ rule gives
$$
p(\theta \mid y_1,\ldots,y_{129})
=
\frac{p(y_1,\ldots,y_{129} \mid \theta)\,p(\theta)}
     {p(y_1,\ldots,y_{129})}.
$$

With a uniform prior, this reduces to
$$
p(\theta \mid y_1,\ldots,y_{129})
\propto
\theta^{118}(1-\theta)^{11}.
$$

> **Key idea:** with a uniform prior, the posterior has the **same shape** as the likelihood.

To obtain a proper probability distribution, we must normalize.

**Normalizing Constant and the Beta Distribution**: Using the identity
$$
\int_0^1 \theta^{a-1}(1-\theta)^{b-1}\,d\theta
=
\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)},
$$
we find
$$
p(y_1,\ldots,y_{129})
=
\frac{\Gamma(119)\Gamma(12)}{\Gamma(131)}.
$$

Therefore, the posterior density is
$$
p(\theta \mid y_1,\ldots,y_{129})
=
\frac{\Gamma(131)}{\Gamma(119)\Gamma(12)}
\theta^{119-1}(1-\theta)^{12-1}.
$$

That is,
$$
\theta \mid y \sim \mathrm{Beta}(119,\,12).
$$


Recall that, a random variable $\theta \sim \mathrm{Beta}(a,b)$ distribution if
$$
p(\theta)
=
\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}
\theta^{a-1}(1-\theta)^{b-1}.
$$

For $\theta \sim \mathrm{Beta}(a,b)$, the expectation (i.e., mean or the first moment) is $\mathbb{E}(\theta) = \frac{a}{a+b}$, and the variance is $\mathrm{Var}(\theta)=\frac{ab}{(a+b)^2(a+b+1)}.$

In our example, the happiness data, the posterior distribution is
$$
\theta \mid y \sim \mathrm{Beta}(119,12).
$$

Thus, the posterior mean is $\mathbb{E}(\theta \mid y) = 0.915$, and the posterior standard deviation is $\mathrm{sd}(\theta \mid y) = 0.025$.

These summaries quantify both our **best estimate** of the population proportion and our **remaining uncertainty** after observing the data.

:::

### Inference about exchangeable binary data

**Posterior Inference under a Uniform Prior**

Suppose $Y_1, \ldots, Y_n \mid \theta \stackrel{\text{i.i.d.}}{\sim} \text{Bernoulli}(\theta)$, and we place a uniform prior on $\theta$. The posterior distribution of $\theta$ given the observed data
$y_1, \ldots, y_n$ is proportional to
$$
\begin{aligned}
p(\theta \mid y_1, \ldots, y_n) 
&= \frac{p(y_1, \ldots, y_n \mid \theta) p(\theta)}{p(y_1, \ldots, y_n)} \\
&= \theta^{\sum_i y_i}(1 - \theta)^{n - \sum_i y_i} \times \frac{p(\theta)}{p(y_1, \ldots, y_n)}\\
&\propto
\theta^{\sum_i y_i}(1 - \theta)^{n - \sum_i y_i}.
\end{aligned}
$$

Consider two parameter values $\theta_a$ and $\theta_b$. The ratio of their posterior densities is
$$
\begin{aligned}
\frac{p(\theta_a \mid y_1, \ldots, y_n)}
     {p(\theta_b \mid y_1, \ldots, y_n)}
&=\frac{\theta_a^{\sum y_i}\left(1-\theta_a\right)^{n-\sum y_i} \times p\left(\theta_a\right) / p\left(y_1, \ldots, y_n\right)}{\theta_b^{\sum y_i}\left(1-\theta_b\right)^{n-\sum y_i} \times p\left(\theta_b\right) / p\left(y_1, \ldots, y_n\right)} \\
&=
\left(\frac{\theta_a}{\theta_b}\right)^{\sum_i y_i}
\left(\frac{1 - \theta_a}{1 - \theta_b}\right)^{n - \sum_i y_i}
\frac{p(\theta_a)}{p(\theta_b)}.
\end{aligned}
$$

This expression shows that the data affect the posterior distribution
**only through the sum of the data** $\sum_{i=1}^n y_i$ based on the relative probability density at $\theta_a$ to $\theta_b$.

As a result, for any set $A$, one can show that
$$
\Pr(\theta \in A \mid Y_1 = y_1, \ldots, Y_n = y_n)
=
\Pr\left(\theta \in A \mid \sum_{i=1}^n Y_i = \sum_{i=1}^n y_i\right).
$$

This means that $\sum_{i=1}^n Y_i$ contains **all the information** in the data relevant for inference about $\theta$. We therefore say that $Y = \sum_{i=1}^n Y_i$ is a **sufficient statistic** for $\theta$.  The term *sufficient* is used because knowing $\sum_{i=1}^n Y_i$
is sufficient to carry out inference about $\theta$; no additional information
from the individual observations $Y_1, \ldots, Y_n$ is required.

In the case where $Y_1, \ldots, Y_n \mid \theta$ are i.i.d. Bernoulli$(\theta)$ random variables, the sufficient statistic $Y = \sum_{i=1}^n Y_i$ follows a **binomial distribution** with parameters $(n, \theta)$.


**The Binomial Model**

Because each $Y_i$ is Bernoulli$(\theta)$ and the observations are independent, the sufficient statistic $Y = \sum_{i=1}^n Y_i$
follows a **binomial distribution** with parameters $(n, \theta)$.

That is, $\Pr(Y = y \mid \theta)=\binom{n}{y} \theta^y (1 - \theta)^{n - y}, \qquad y = 0, 1, \ldots, n.$ For a binomial$(n, \theta)$ random variable $Y$,

+ $\mathbb{E}[Y \mid \theta] = n\theta$, 
+ $\mathrm{Var}(Y \mid \theta) = n\theta(1 - \theta).$

``` {r binomal, message=FALSE}
library(ggplot2)
library(dplyr)
library(tidyr)
library(cowplot)

# helper: make facet labels parse as plotmath
make_theta_labels <- function(df) {
  df %>%
    mutate(theta_lab = paste0("theta==", as.character(theta)))
}

# -------------------------
# Figure for n = 10
# -------------------------
n_val <- 10
thetas <- c(0.2, 0.8)

df10 <- expand_grid(theta = thetas, y = 0:n_val) %>%
  mutate(p = dbinom(y, size = n_val, prob = theta)) %>%
  make_theta_labels()

p10 <- ggplot(df10, aes(x = y, y = p)) +
  geom_col(width = 0.7, fill = "black") +
  facet_wrap(~ theta_lab, nrow = 1, labeller = label_parsed) +
  scale_x_continuous(breaks = 0:n_val) +
  labs(
    x = "y",
    y = expression(Pr(Y == y ~ "|" ~ theta ~ "," ~ n == 10))
  ) +
  theme_bw(base_size = 14) +
  theme(
    panel.grid = element_blank(),
    strip.background = element_blank()
  )

# -------------------------
# Figure for n = 100
# -------------------------
n_val <- 100

df100 <- expand_grid(theta = thetas, y = 0:n_val) %>%
  mutate(p = dbinom(y, size = n_val, prob = theta)) %>%
  make_theta_labels()

p100 <- ggplot(df100, aes(x = y, y = p)) +
  geom_col(width = 0.9, fill = "black") +
  facet_wrap(~ theta_lab, nrow = 1, labeller = label_parsed) +
  labs(
    x = "y",
    y = expression(Pr(Y == y ~ "|" ~ theta ~ "," ~ n == 100))
  ) +
  theme_bw(base_size = 14) +
  theme(
    panel.grid = element_blank(),
    strip.background = element_blank()
  )

# -------------------------
# Combine using cowplot
# -------------------------
final_plot <- plot_grid(p10, p100, ncol = 1, align = "v")
final_plot
```

---

This Chapter follows closely with Chapter 3 in Hoff (2009).
